{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FNFSIAM",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMLXZGTOymCnAkBgzdlg23x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Satyake/Triplet-Loss_Siamese_NN/blob/master/FNFSIAM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqkdCbDj-lnG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 769
        },
        "outputId": "a2993a15-d23a-4ec7-bb10-446490d3d2b8"
      },
      "source": [
        "!unzip /content/Test.zip\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/Test.zip\n",
            "   creating: Test/F/\n",
            "  inflating: Test/F/F07.JPG          \n",
            "  inflating: Test/F/F07S03R01.JPG    \n",
            "  inflating: Test/F/F07S04R01.JPG    \n",
            "  inflating: Test/F/F07S04R05.JPG    \n",
            "  inflating: Test/F/F08.JPG          \n",
            "  inflating: Test/F/F09.JPG          \n",
            "  inflating: Test/F/F10.JPG          \n",
            "  inflating: Test/F/F11.JPG          \n",
            "  inflating: Test/F/F12.JPG          \n",
            "  inflating: Test/F/F13.jpg          \n",
            "  inflating: Test/F/F14.jpg          \n",
            "  inflating: Test/F/F15.JPG          \n",
            "  inflating: Test/F/FF1.jpg          \n",
            "  inflating: Test/F/FF2.jpg          \n",
            "  inflating: Test/F/FF3.jpg          \n",
            "  inflating: Test/F/FF4.jpg          \n",
            "  inflating: Test/F/FF5.jpg          \n",
            "  inflating: Test/F/FF6.jpg          \n",
            "  inflating: Test/F/FF7.jpg          \n",
            "  inflating: Test/F/FF8.jpg          \n",
            "  inflating: Test/F/FF9.jpg          \n",
            "   creating: Test/NF/\n",
            "  inflating: Test/NF/D10125.JPG      \n",
            "  inflating: Test/NF/D1024.JPG       \n",
            "  inflating: Test/NF/D1033.JPG       \n",
            "  inflating: Test/NF/D1041.JPG       \n",
            "  inflating: Test/NF/D4111.JPG       \n",
            "  inflating: Test/NF/D4121.JPG       \n",
            "  inflating: Test/NF/D4131.JPG       \n",
            "  inflating: Test/NF/D4141.JPG       \n",
            "  inflating: Test/NF/D4171.JPG       \n",
            "  inflating: Test/NF/D4191.JPG       \n",
            "  inflating: Test/NF/D4201.JPG       \n",
            "  inflating: Test/NF/D4221.JPG       \n",
            "  inflating: Test/NF/D4231.JPG       \n",
            "  inflating: Test/NF/D555.JPG        \n",
            "  inflating: Test/NF/D562.JPG        \n",
            "  inflating: Test/NF/D585.JPG        \n",
            "  inflating: Test/NF/D752.JPG        \n",
            "  inflating: Test/NF/D753.JPG        \n",
            "  inflating: Test/NF/D815.JPG        \n",
            "  inflating: Test/NF/DD1.jpg         \n",
            "  inflating: Test/NF/DD2.jpg         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RD6C97cSCyYD"
      },
      "source": [
        "file_dir = '/content/Test/NF'\n",
        "files = glob.glob(file_dir)\n",
        "x3t = []\n",
        "for f1 in files:\n",
        "    img_dir = os.path.join(f1,'*')\n",
        "    images = glob.glob(img_dir)\n",
        "    for img in images:\n",
        "        img_temp = cv2.imread(img)\n",
        "        img_temp=cv2.cvtColor(img_temp,cv2.COLOR_BGR2GRAY)\n",
        "        img_temp = cv2.resize(img_temp,(512,512))\n",
        "        x3t.append(img_temp)\n",
        "\n",
        "file_dir = '/content/Test/F'\n",
        "files = glob.glob(file_dir)\n",
        "x3 = []\n",
        "for f1 in files:\n",
        "    img_dir = os.path.join(f1,'*')\n",
        "    images = glob.glob(img_dir)\n",
        "    for img in images:\n",
        "        img_temp = cv2.imread(img)\n",
        "        img_temp=cv2.cvtColor(img_temp,cv2.COLOR_BGR2GRAY)\n",
        "        img_temp = cv2.resize(img_temp,(512,512))\n",
        "        x3.append(img_temp)\n",
        "\n",
        "file_dir = '/content/Train/F'\n",
        "files = glob.glob(file_dir)\n",
        "x = []\n",
        "for f1 in files:\n",
        "    img_dir = os.path.join(f1,'*')\n",
        "    images = glob.glob(img_dir)\n",
        "    for img in images:\n",
        "        img_temp = cv2.imread(img)\n",
        "        img_temp=cv2.cvtColor(img_temp,cv2.COLOR_BGR2GRAY)\n",
        "        img_temp = cv2.resize(img_temp,(512,512))\n",
        "        x.append(img_temp)\n",
        "file_dir = '/content/Train/NF'\n",
        "files = glob.glob(file_dir)\n",
        "x2 = []\n",
        "for f1 in files:\n",
        "    img_dir = os.path.join(f1,'*')\n",
        "    images = glob.glob(img_dir)\n",
        "    for img in images:\n",
        "        img_temp = cv2.imread(img)\n",
        "        img_temp=cv2.cvtColor(img_temp,cv2.COLOR_BGR2GRAY)\n",
        "        img_temp = cv2.resize(img_temp,(512,512))\n",
        "        x2.append(img_temp)\n",
        "x_train1=np.array(x)\n",
        "y_train1=[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
        "y_train1=np.array(y_train1)\n",
        "x_train2=np.array(x2)\n",
        "y_train2=np.array([0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0])\n",
        "x_train=np.concatenate([x_train1,x_train2],axis=0)\n",
        "y_train=np.concatenate([y_train1,y_train2],axis=0)\n",
        "\n",
        "x_test1=np.array(x3)\n",
        "y_test1=np.array([1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1])\n",
        "y_test1.shape\n",
        "x_test2=np.array(x3t)\n",
        "y_test2=np.array([0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0])\n",
        "x_test2.shape\n",
        "x_test=np.concatenate([x_test1,x_test2],axis=0)\n",
        "y_test=np.concatenate([y_test1,y_test2],axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5ETkMHHlin7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "c30c8c61-4803-409b-b8ea-4a7a6aa9bf64"
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    def recall(y_true, y_pred):\n",
        "        \"\"\"Recall metric.\n",
        "\n",
        "        Only computes a batch-wise average of recall.\n",
        "\n",
        "        Computes the recall, a metric for multi-label classification of\n",
        "        how many relevant items are selected.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        \"\"\"Precision metric.\n",
        "\n",
        "        Only computes a batch-wise average of precision.\n",
        "\n",
        "        Computes the precision, a metric for multi-label classification of\n",
        "        how many selected items are relevant.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return precision\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sWML7iPEPbK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "e2215dbc-49e8-4e81-e7c6-25444f7eed98"
      },
      "source": [
        "y_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(42,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4CL6OX8-uw8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "7278b21b-7077-47d4-c7f0-4d96c959dcd8"
      },
      "source": [
        "import numpy as np\n",
        "import glob\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import cv2\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import random\n",
        "import matplotlib.patheffects as PathEffects\n",
        "from keras.layers import Input, Conv2D, Lambda, Dense, Flatten,MaxPooling2D, concatenate,BatchNormalization\n",
        "from keras.models import Model, Sequential\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.optimizers import SGD,Adam\n",
        "from keras.losses import binary_crossentropy\n",
        "import os\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import permutations\n",
        "import seaborn as sns\n",
        "from keras.datasets import mnist\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.svm import SVC\n",
        "# Define our own plot function\n",
        "def scatter(x, labels, subtitle=None):\n",
        "    # We choose a color palette with seaborn.\n",
        "    palette = np.array(sns.color_palette(\"hls\", 10))\n",
        "\n",
        "    # We create a scatter plot.\n",
        "    f = plt.figure(figsize=(8, 8))\n",
        "    ax = plt.subplot(aspect='equal')\n",
        "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40,\n",
        "                    c=palette[labels.astype(np.int)])\n",
        "    plt.xlim(-25, 25)\n",
        "    plt.ylim(-25, 25)\n",
        "    ax.axis('off')\n",
        "    ax.axis('tight')\n",
        "\n",
        "    # We add the labels for each digit.\n",
        "    txts = []\n",
        "    for i in range(10):\n",
        "        # Position of each label.\n",
        "        xtext, ytext = np.median(x[labels == i, :], axis=0)\n",
        "        txt = ax.text(xtext, ytext, str(i), fontsize=24)\n",
        "        txt.set_path_effects([\n",
        "            PathEffects.Stroke(linewidth=5, foreground=\"w\"),\n",
        "            PathEffects.Normal()])\n",
        "        txts.append(txt)\n",
        "        \n",
        "    if subtitle != None:\n",
        "        plt.suptitle(subtitle)\n",
        "        \n",
        "    plt.savefig(subtitle)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y58W_PGLCr0F"
      },
      "source": [
        "x_train_flat = x_train.reshape(-1,262144)\n",
        "x_test_flat = x_test.reshape(-1,262144)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3C07FjHCvPT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "97af9425-0a5e-481a-da27-d504f6f2f7b8"
      },
      "source": [
        "tsne = TSNE()\n",
        "train_tsne_embeds = tsne.fit_transform(x_train_flat[:512])\n",
        "scatter(train_tsne_embeds, y_train[:108], \"Samples from Training Data\")\n",
        "\n",
        "eval_tsne_embeds = tsne.fit_transform(x_test_flat[:512])\n",
        "scatter(eval_tsne_embeds, y_test[:42], \"Samples from Validation Data\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:154: RuntimeWarning: invalid value encountered in true_divide\n",
            "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:154: RuntimeWarning: invalid value encountered in true_divide\n",
            "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsDplG98-0Hm"
      },
      "source": [
        "Classifier_input = Input((262144,))\n",
        "Classifier_output = Dense(1, activation='sigmoid')(Classifier_input)\n",
        "Classifier_model = Model(Classifier_input, Classifier_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tShEYaw8-9M5"
      },
      "source": [
        "le = LabelBinarizer()\n",
        "y_train_onehot = le.fit_transform(y_train)\n",
        "y_test_onehot = le.transform(y_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAboZz9qFBVU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a6c05bb9-eaa1-406c-86f7-545b30c4a55c"
      },
      "source": [
        "Classifier_model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "Classifier_model.fit(x_train_flat,y_train_onehot, validation_data=(x_test_flat,y_test_onehot),epochs=500)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 108 samples, validate on 42 samples\n",
            "Epoch 1/500\n",
            "108/108 [==============================] - 0s 3ms/step - loss: 1439.9693 - accuracy: 0.6296 - val_loss: 1958.7319 - val_accuracy: 0.5000\n",
            "Epoch 2/500\n",
            "108/108 [==============================] - 0s 585us/step - loss: 538.2642 - accuracy: 0.4630 - val_loss: 1218.1898 - val_accuracy: 0.5000\n",
            "Epoch 3/500\n",
            "108/108 [==============================] - 0s 581us/step - loss: 573.3965 - accuracy: 0.6667 - val_loss: 1041.5354 - val_accuracy: 0.5000\n",
            "Epoch 4/500\n",
            "108/108 [==============================] - 0s 583us/step - loss: 955.5132 - accuracy: 0.4259 - val_loss: 2291.5076 - val_accuracy: 0.5000\n",
            "Epoch 5/500\n",
            "108/108 [==============================] - 0s 602us/step - loss: 1300.6486 - accuracy: 0.6667 - val_loss: 59.3060 - val_accuracy: 0.6190\n",
            "Epoch 6/500\n",
            "108/108 [==============================] - 0s 606us/step - loss: 491.4144 - accuracy: 0.5278 - val_loss: 417.2073 - val_accuracy: 0.5952\n",
            "Epoch 7/500\n",
            "108/108 [==============================] - 0s 595us/step - loss: 404.1972 - accuracy: 0.7037 - val_loss: 60.7170 - val_accuracy: 0.6429\n",
            "Epoch 8/500\n",
            "108/108 [==============================] - 0s 597us/step - loss: 557.3623 - accuracy: 0.4167 - val_loss: 797.2291 - val_accuracy: 0.5000\n",
            "Epoch 9/500\n",
            "108/108 [==============================] - 0s 570us/step - loss: 1136.5444 - accuracy: 0.6667 - val_loss: 2221.2442 - val_accuracy: 0.5000\n",
            "Epoch 10/500\n",
            "108/108 [==============================] - 0s 576us/step - loss: 1032.4081 - accuracy: 0.6852 - val_loss: 412.5802 - val_accuracy: 0.5476\n",
            "Epoch 11/500\n",
            "108/108 [==============================] - 0s 584us/step - loss: 535.0874 - accuracy: 0.4259 - val_loss: 689.9156 - val_accuracy: 0.5952\n",
            "Epoch 12/500\n",
            "108/108 [==============================] - 0s 595us/step - loss: 790.9407 - accuracy: 0.6944 - val_loss: 613.9901 - val_accuracy: 0.6905\n",
            "Epoch 13/500\n",
            "108/108 [==============================] - 0s 605us/step - loss: 410.6637 - accuracy: 0.5833 - val_loss: 123.0348 - val_accuracy: 0.6667\n",
            "Epoch 14/500\n",
            "108/108 [==============================] - 0s 591us/step - loss: 225.1159 - accuracy: 0.7407 - val_loss: 543.7825 - val_accuracy: 0.6905\n",
            "Epoch 15/500\n",
            "108/108 [==============================] - 0s 581us/step - loss: 226.7471 - accuracy: 0.7407 - val_loss: 396.5652 - val_accuracy: 0.5952\n",
            "Epoch 16/500\n",
            "108/108 [==============================] - 0s 580us/step - loss: 172.9999 - accuracy: 0.7037 - val_loss: 342.5179 - val_accuracy: 0.7381\n",
            "Epoch 17/500\n",
            "108/108 [==============================] - 0s 615us/step - loss: 181.6379 - accuracy: 0.7685 - val_loss: 236.4609 - val_accuracy: 0.6190\n",
            "Epoch 18/500\n",
            "108/108 [==============================] - 0s 600us/step - loss: 124.0708 - accuracy: 0.7407 - val_loss: 272.3045 - val_accuracy: 0.7619\n",
            "Epoch 19/500\n",
            "108/108 [==============================] - 0s 612us/step - loss: 137.2796 - accuracy: 0.7685 - val_loss: 61.5584 - val_accuracy: 0.7381\n",
            "Epoch 20/500\n",
            "108/108 [==============================] - 0s 604us/step - loss: 161.0516 - accuracy: 0.8148 - val_loss: 64.7491 - val_accuracy: 0.7381\n",
            "Epoch 21/500\n",
            "108/108 [==============================] - 0s 597us/step - loss: 109.1426 - accuracy: 0.6852 - val_loss: 147.4158 - val_accuracy: 0.7619\n",
            "Epoch 22/500\n",
            "108/108 [==============================] - 0s 576us/step - loss: 67.1378 - accuracy: 0.8519 - val_loss: 50.0804 - val_accuracy: 0.7381\n",
            "Epoch 23/500\n",
            "108/108 [==============================] - 0s 579us/step - loss: 38.0167 - accuracy: 0.8796 - val_loss: 45.1624 - val_accuracy: 0.7381\n",
            "Epoch 24/500\n",
            "108/108 [==============================] - 0s 601us/step - loss: 21.5277 - accuracy: 0.9167 - val_loss: 27.3461 - val_accuracy: 0.8095\n",
            "Epoch 25/500\n",
            "108/108 [==============================] - 0s 596us/step - loss: 18.1364 - accuracy: 0.9167 - val_loss: 24.8490 - val_accuracy: 0.7857\n",
            "Epoch 26/500\n",
            "108/108 [==============================] - 0s 593us/step - loss: 48.4495 - accuracy: 0.8796 - val_loss: 127.1990 - val_accuracy: 0.6905\n",
            "Epoch 27/500\n",
            "108/108 [==============================] - 0s 568us/step - loss: 46.3489 - accuracy: 0.8519 - val_loss: 54.1523 - val_accuracy: 0.7619\n",
            "Epoch 28/500\n",
            "108/108 [==============================] - 0s 578us/step - loss: 19.9858 - accuracy: 0.9074 - val_loss: 29.4546 - val_accuracy: 0.8095\n",
            "Epoch 29/500\n",
            "108/108 [==============================] - 0s 579us/step - loss: 34.2316 - accuracy: 0.8704 - val_loss: 79.2977 - val_accuracy: 0.7619\n",
            "Epoch 30/500\n",
            "108/108 [==============================] - 0s 602us/step - loss: 15.1866 - accuracy: 0.8981 - val_loss: 157.6687 - val_accuracy: 0.7619\n",
            "Epoch 31/500\n",
            "108/108 [==============================] - 0s 592us/step - loss: 62.6354 - accuracy: 0.8704 - val_loss: 32.2576 - val_accuracy: 0.8333\n",
            "Epoch 32/500\n",
            "108/108 [==============================] - 0s 593us/step - loss: 15.9559 - accuracy: 0.9167 - val_loss: 20.0074 - val_accuracy: 0.8571\n",
            "Epoch 33/500\n",
            "108/108 [==============================] - 0s 583us/step - loss: 13.0908 - accuracy: 0.9352 - val_loss: 14.5383 - val_accuracy: 0.8810\n",
            "Epoch 34/500\n",
            "108/108 [==============================] - 0s 588us/step - loss: 10.1200 - accuracy: 0.9444 - val_loss: 64.5686 - val_accuracy: 0.7381\n",
            "Epoch 35/500\n",
            "108/108 [==============================] - 0s 588us/step - loss: 24.6104 - accuracy: 0.9074 - val_loss: 22.7422 - val_accuracy: 0.8571\n",
            "Epoch 36/500\n",
            "108/108 [==============================] - 0s 603us/step - loss: 31.2046 - accuracy: 0.8889 - val_loss: 60.4041 - val_accuracy: 0.7857\n",
            "Epoch 37/500\n",
            "108/108 [==============================] - 0s 600us/step - loss: 24.2921 - accuracy: 0.8889 - val_loss: 13.8232 - val_accuracy: 0.9286\n",
            "Epoch 38/500\n",
            "108/108 [==============================] - 0s 579us/step - loss: 7.5169 - accuracy: 0.9444 - val_loss: 45.9549 - val_accuracy: 0.7857\n",
            "Epoch 39/500\n",
            "108/108 [==============================] - 0s 584us/step - loss: 14.7459 - accuracy: 0.9259 - val_loss: 109.5023 - val_accuracy: 0.6905\n",
            "Epoch 40/500\n",
            "108/108 [==============================] - 0s 591us/step - loss: 72.2899 - accuracy: 0.8148 - val_loss: 69.7739 - val_accuracy: 0.7619\n",
            "Epoch 41/500\n",
            "108/108 [==============================] - 0s 602us/step - loss: 142.4762 - accuracy: 0.6852 - val_loss: 261.6320 - val_accuracy: 0.7619\n",
            "Epoch 42/500\n",
            "108/108 [==============================] - 0s 598us/step - loss: 130.2260 - accuracy: 0.8148 - val_loss: 465.7743 - val_accuracy: 0.5476\n",
            "Epoch 43/500\n",
            "108/108 [==============================] - 0s 575us/step - loss: 184.0015 - accuracy: 0.6944 - val_loss: 206.2767 - val_accuracy: 0.7619\n",
            "Epoch 44/500\n",
            "108/108 [==============================] - 0s 574us/step - loss: 135.0599 - accuracy: 0.7407 - val_loss: 80.7154 - val_accuracy: 0.7619\n",
            "Epoch 45/500\n",
            "108/108 [==============================] - 0s 572us/step - loss: 80.6422 - accuracy: 0.8056 - val_loss: 247.7825 - val_accuracy: 0.5952\n",
            "Epoch 46/500\n",
            "108/108 [==============================] - 0s 598us/step - loss: 48.6682 - accuracy: 0.8148 - val_loss: 122.7286 - val_accuracy: 0.7619\n",
            "Epoch 47/500\n",
            "108/108 [==============================] - 0s 588us/step - loss: 29.1139 - accuracy: 0.9167 - val_loss: 99.1136 - val_accuracy: 0.7381\n",
            "Epoch 48/500\n",
            "108/108 [==============================] - 0s 606us/step - loss: 15.6379 - accuracy: 0.9444 - val_loss: 68.1667 - val_accuracy: 0.7619\n",
            "Epoch 49/500\n",
            "108/108 [==============================] - 0s 592us/step - loss: 9.7974 - accuracy: 0.9444 - val_loss: 39.8059 - val_accuracy: 0.8095\n",
            "Epoch 50/500\n",
            "108/108 [==============================] - 0s 584us/step - loss: 14.6390 - accuracy: 0.9352 - val_loss: 18.5967 - val_accuracy: 0.9286\n",
            "Epoch 51/500\n",
            "108/108 [==============================] - 0s 570us/step - loss: 15.6215 - accuracy: 0.9259 - val_loss: 17.4752 - val_accuracy: 0.9286\n",
            "Epoch 52/500\n",
            "108/108 [==============================] - 0s 616us/step - loss: 23.9728 - accuracy: 0.9167 - val_loss: 67.7806 - val_accuracy: 0.7619\n",
            "Epoch 53/500\n",
            "108/108 [==============================] - 0s 590us/step - loss: 16.7932 - accuracy: 0.9352 - val_loss: 18.2100 - val_accuracy: 0.9524\n",
            "Epoch 54/500\n",
            "108/108 [==============================] - 0s 596us/step - loss: 6.7597 - accuracy: 0.9722 - val_loss: 23.4677 - val_accuracy: 0.9286\n",
            "Epoch 55/500\n",
            "108/108 [==============================] - 0s 575us/step - loss: 9.9966 - accuracy: 0.9537 - val_loss: 19.1460 - val_accuracy: 0.9048\n",
            "Epoch 56/500\n",
            "108/108 [==============================] - 0s 578us/step - loss: 5.9059 - accuracy: 0.9722 - val_loss: 28.7574 - val_accuracy: 0.8810\n",
            "Epoch 57/500\n",
            "108/108 [==============================] - 0s 599us/step - loss: 4.6872 - accuracy: 0.9630 - val_loss: 20.7041 - val_accuracy: 0.9286\n",
            "Epoch 58/500\n",
            "108/108 [==============================] - 0s 608us/step - loss: 4.3226 - accuracy: 0.9815 - val_loss: 27.0542 - val_accuracy: 0.9048\n",
            "Epoch 59/500\n",
            "108/108 [==============================] - 0s 597us/step - loss: 16.7040 - accuracy: 0.9167 - val_loss: 21.1143 - val_accuracy: 0.9286\n",
            "Epoch 60/500\n",
            "108/108 [==============================] - 0s 589us/step - loss: 5.2550 - accuracy: 0.9537 - val_loss: 35.6375 - val_accuracy: 0.8571\n",
            "Epoch 61/500\n",
            "108/108 [==============================] - 0s 562us/step - loss: 2.8638 - accuracy: 0.9722 - val_loss: 23.8670 - val_accuracy: 0.9286\n",
            "Epoch 62/500\n",
            "108/108 [==============================] - 0s 595us/step - loss: 4.4255 - accuracy: 0.9630 - val_loss: 25.2487 - val_accuracy: 0.9286\n",
            "Epoch 63/500\n",
            "108/108 [==============================] - 0s 614us/step - loss: 3.4319 - accuracy: 0.9815 - val_loss: 32.2473 - val_accuracy: 0.8810\n",
            "Epoch 64/500\n",
            "108/108 [==============================] - 0s 589us/step - loss: 3.9605 - accuracy: 0.9630 - val_loss: 66.2157 - val_accuracy: 0.7619\n",
            "Epoch 65/500\n",
            "108/108 [==============================] - 0s 597us/step - loss: 42.9030 - accuracy: 0.8519 - val_loss: 26.0578 - val_accuracy: 0.9286\n",
            "Epoch 66/500\n",
            "108/108 [==============================] - 0s 580us/step - loss: 32.3238 - accuracy: 0.8889 - val_loss: 66.8965 - val_accuracy: 0.7619\n",
            "Epoch 67/500\n",
            "108/108 [==============================] - 0s 578us/step - loss: 12.4857 - accuracy: 0.9352 - val_loss: 29.6361 - val_accuracy: 0.8810\n",
            "Epoch 68/500\n",
            "108/108 [==============================] - 0s 582us/step - loss: 10.7423 - accuracy: 0.9444 - val_loss: 99.6403 - val_accuracy: 0.7143\n",
            "Epoch 69/500\n",
            "108/108 [==============================] - 0s 605us/step - loss: 9.4597 - accuracy: 0.9074 - val_loss: 64.3217 - val_accuracy: 0.7619\n",
            "Epoch 70/500\n",
            "108/108 [==============================] - 0s 591us/step - loss: 9.2470 - accuracy: 0.9630 - val_loss: 181.7040 - val_accuracy: 0.5952\n",
            "Epoch 71/500\n",
            "108/108 [==============================] - 0s 593us/step - loss: 13.7041 - accuracy: 0.9259 - val_loss: 27.9842 - val_accuracy: 0.8810\n",
            "Epoch 72/500\n",
            "108/108 [==============================] - 0s 604us/step - loss: 1.2524 - accuracy: 0.9907 - val_loss: 72.7630 - val_accuracy: 0.7381\n",
            "Epoch 73/500\n",
            "108/108 [==============================] - 0s 576us/step - loss: 4.5951 - accuracy: 0.9722 - val_loss: 28.2542 - val_accuracy: 0.8810\n",
            "Epoch 74/500\n",
            "108/108 [==============================] - 0s 586us/step - loss: 3.8958 - accuracy: 0.9722 - val_loss: 37.4127 - val_accuracy: 0.8810\n",
            "Epoch 75/500\n",
            "108/108 [==============================] - 0s 580us/step - loss: 4.1846 - accuracy: 0.9722 - val_loss: 25.7103 - val_accuracy: 0.9286\n",
            "Epoch 76/500\n",
            "108/108 [==============================] - 0s 594us/step - loss: 2.8999 - accuracy: 0.9815 - val_loss: 29.6357 - val_accuracy: 0.9286\n",
            "Epoch 77/500\n",
            "108/108 [==============================] - 0s 591us/step - loss: 0.7326 - accuracy: 0.9815 - val_loss: 47.2389 - val_accuracy: 0.8571\n",
            "Epoch 78/500\n",
            "108/108 [==============================] - 0s 591us/step - loss: 6.0589 - accuracy: 0.9537 - val_loss: 51.3745 - val_accuracy: 0.7857\n",
            "Epoch 79/500\n",
            "108/108 [==============================] - 0s 597us/step - loss: 11.4925 - accuracy: 0.9537 - val_loss: 39.5847 - val_accuracy: 0.8571\n",
            "Epoch 80/500\n",
            "108/108 [==============================] - 0s 579us/step - loss: 6.5786 - accuracy: 0.9630 - val_loss: 29.4018 - val_accuracy: 0.8810\n",
            "Epoch 81/500\n",
            "108/108 [==============================] - 0s 578us/step - loss: 10.1023 - accuracy: 0.9630 - val_loss: 35.1127 - val_accuracy: 0.8810\n",
            "Epoch 82/500\n",
            "108/108 [==============================] - 0s 601us/step - loss: 7.0678 - accuracy: 0.9352 - val_loss: 114.6360 - val_accuracy: 0.7619\n",
            "Epoch 83/500\n",
            "108/108 [==============================] - 0s 601us/step - loss: 21.2445 - accuracy: 0.9167 - val_loss: 137.2223 - val_accuracy: 0.6667\n",
            "Epoch 84/500\n",
            "108/108 [==============================] - 0s 646us/step - loss: 28.6348 - accuracy: 0.8796 - val_loss: 150.4062 - val_accuracy: 0.6429\n",
            "Epoch 85/500\n",
            "108/108 [==============================] - 0s 608us/step - loss: 38.9801 - accuracy: 0.8611 - val_loss: 143.6800 - val_accuracy: 0.7619\n",
            "Epoch 86/500\n",
            "108/108 [==============================] - 0s 597us/step - loss: 35.4216 - accuracy: 0.8796 - val_loss: 178.1766 - val_accuracy: 0.6190\n",
            "Epoch 87/500\n",
            "108/108 [==============================] - 0s 593us/step - loss: 129.8356 - accuracy: 0.7222 - val_loss: 84.1807 - val_accuracy: 0.7143\n",
            "Epoch 88/500\n",
            "108/108 [==============================] - 0s 578us/step - loss: 220.0889 - accuracy: 0.7500 - val_loss: 378.1770 - val_accuracy: 0.5952\n",
            "Epoch 89/500\n",
            "108/108 [==============================] - 0s 562us/step - loss: 125.8147 - accuracy: 0.7500 - val_loss: 170.9801 - val_accuracy: 0.7619\n",
            "Epoch 90/500\n",
            "108/108 [==============================] - 0s 596us/step - loss: 129.8095 - accuracy: 0.7500 - val_loss: 70.3779 - val_accuracy: 0.7143\n",
            "Epoch 91/500\n",
            "108/108 [==============================] - 0s 587us/step - loss: 6.1041 - accuracy: 0.9815 - val_loss: 66.4138 - val_accuracy: 0.7857\n",
            "Epoch 92/500\n",
            "108/108 [==============================] - 0s 586us/step - loss: 7.0061 - accuracy: 0.9815 - val_loss: 43.4470 - val_accuracy: 0.8571\n",
            "Epoch 93/500\n",
            "108/108 [==============================] - 0s 581us/step - loss: 4.2580 - accuracy: 0.9722 - val_loss: 25.3363 - val_accuracy: 0.9286\n",
            "Epoch 94/500\n",
            "108/108 [==============================] - 0s 585us/step - loss: 2.6802 - accuracy: 0.9815 - val_loss: 34.1774 - val_accuracy: 0.9048\n",
            "Epoch 95/500\n",
            "108/108 [==============================] - 0s 595us/step - loss: 0.3328 - accuracy: 0.9907 - val_loss: 28.7229 - val_accuracy: 0.9286\n",
            "Epoch 96/500\n",
            "108/108 [==============================] - 0s 611us/step - loss: 1.7279 - accuracy: 0.9907 - val_loss: 29.8294 - val_accuracy: 0.9286\n",
            "Epoch 97/500\n",
            "108/108 [==============================] - 0s 604us/step - loss: 0.9232 - accuracy: 0.9815 - val_loss: 51.3029 - val_accuracy: 0.7857\n",
            "Epoch 98/500\n",
            "108/108 [==============================] - 0s 606us/step - loss: 2.4466 - accuracy: 0.9815 - val_loss: 25.4973 - val_accuracy: 0.9286\n",
            "Epoch 99/500\n",
            "108/108 [==============================] - 0s 582us/step - loss: 1.8177 - accuracy: 0.9907 - val_loss: 30.5605 - val_accuracy: 0.9048\n",
            "Epoch 100/500\n",
            "108/108 [==============================] - 0s 615us/step - loss: 4.6650 - accuracy: 0.9815 - val_loss: 73.1846 - val_accuracy: 0.7381\n",
            "Epoch 101/500\n",
            "108/108 [==============================] - 0s 594us/step - loss: 8.3163 - accuracy: 0.9630 - val_loss: 44.1356 - val_accuracy: 0.8810\n",
            "Epoch 102/500\n",
            "108/108 [==============================] - 0s 602us/step - loss: 10.2386 - accuracy: 0.9444 - val_loss: 35.1005 - val_accuracy: 0.8810\n",
            "Epoch 103/500\n",
            "108/108 [==============================] - 0s 600us/step - loss: 2.6137e-24 - accuracy: 1.0000 - val_loss: 29.8698 - val_accuracy: 0.9286\n",
            "Epoch 104/500\n",
            "108/108 [==============================] - 0s 590us/step - loss: 0.6263 - accuracy: 0.9815 - val_loss: 29.8432 - val_accuracy: 0.9286\n",
            "Epoch 105/500\n",
            "108/108 [==============================] - 0s 582us/step - loss: 2.6809 - accuracy: 0.9907 - val_loss: 63.9680 - val_accuracy: 0.8095\n",
            "Epoch 106/500\n",
            "108/108 [==============================] - 0s 591us/step - loss: 4.5306 - accuracy: 0.9630 - val_loss: 115.4652 - val_accuracy: 0.6667\n",
            "Epoch 107/500\n",
            "108/108 [==============================] - 0s 603us/step - loss: 13.4003 - accuracy: 0.9444 - val_loss: 27.5212 - val_accuracy: 0.9286\n",
            "Epoch 108/500\n",
            "108/108 [==============================] - 0s 601us/step - loss: 7.5522 - accuracy: 0.9537 - val_loss: 104.8914 - val_accuracy: 0.6905\n",
            "Epoch 109/500\n",
            "108/108 [==============================] - 0s 597us/step - loss: 11.5137 - accuracy: 0.9444 - val_loss: 28.5472 - val_accuracy: 0.9048\n",
            "Epoch 110/500\n",
            "108/108 [==============================] - 0s 579us/step - loss: 12.4918 - accuracy: 0.9537 - val_loss: 44.7809 - val_accuracy: 0.8810\n",
            "Epoch 111/500\n",
            "108/108 [==============================] - 0s 586us/step - loss: 13.2155 - accuracy: 0.9352 - val_loss: 71.8502 - val_accuracy: 0.7857\n",
            "Epoch 112/500\n",
            "108/108 [==============================] - 0s 593us/step - loss: 33.5737 - accuracy: 0.9074 - val_loss: 254.1158 - val_accuracy: 0.6190\n",
            "Epoch 113/500\n",
            "108/108 [==============================] - 0s 598us/step - loss: 65.9749 - accuracy: 0.7963 - val_loss: 42.7386 - val_accuracy: 0.8810\n",
            "Epoch 114/500\n",
            "108/108 [==============================] - 0s 590us/step - loss: 7.4931 - accuracy: 0.9444 - val_loss: 35.5818 - val_accuracy: 0.8810\n",
            "Epoch 115/500\n",
            "108/108 [==============================] - 0s 584us/step - loss: 3.4888 - accuracy: 0.9630 - val_loss: 49.7598 - val_accuracy: 0.8810\n",
            "Epoch 116/500\n",
            "108/108 [==============================] - 0s 589us/step - loss: 8.5277 - accuracy: 0.9444 - val_loss: 113.6634 - val_accuracy: 0.7857\n",
            "Epoch 117/500\n",
            "108/108 [==============================] - 0s 593us/step - loss: 47.5759 - accuracy: 0.8981 - val_loss: 508.4801 - val_accuracy: 0.5476\n",
            "Epoch 118/500\n",
            "108/108 [==============================] - 0s 598us/step - loss: 251.4906 - accuracy: 0.7037 - val_loss: 41.3011 - val_accuracy: 0.9048\n",
            "Epoch 119/500\n",
            "108/108 [==============================] - 0s 595us/step - loss: 38.3629 - accuracy: 0.8333 - val_loss: 270.9154 - val_accuracy: 0.7381\n",
            "Epoch 120/500\n",
            "108/108 [==============================] - 0s 600us/step - loss: 128.0745 - accuracy: 0.7778 - val_loss: 35.1722 - val_accuracy: 0.9286\n",
            "Epoch 121/500\n",
            "108/108 [==============================] - 0s 593us/step - loss: 102.4329 - accuracy: 0.8333 - val_loss: 205.3753 - val_accuracy: 0.6429\n",
            "Epoch 122/500\n",
            "108/108 [==============================] - 0s 581us/step - loss: 28.6273 - accuracy: 0.8889 - val_loss: 152.5689 - val_accuracy: 0.7619\n",
            "Epoch 123/500\n",
            "108/108 [==============================] - 0s 594us/step - loss: 32.3798 - accuracy: 0.9352 - val_loss: 132.2124 - val_accuracy: 0.7381\n",
            "Epoch 124/500\n",
            "108/108 [==============================] - 0s 614us/step - loss: 14.1547 - accuracy: 0.9259 - val_loss: 26.1823 - val_accuracy: 0.9286\n",
            "Epoch 125/500\n",
            "108/108 [==============================] - 0s 600us/step - loss: 1.0798 - accuracy: 0.9907 - val_loss: 98.8347 - val_accuracy: 0.7381\n",
            "Epoch 126/500\n",
            "108/108 [==============================] - 0s 607us/step - loss: 6.2370 - accuracy: 0.9722 - val_loss: 35.4026 - val_accuracy: 0.9048\n",
            "Epoch 127/500\n",
            "108/108 [==============================] - 0s 608us/step - loss: 7.4672 - accuracy: 0.9630 - val_loss: 44.4119 - val_accuracy: 0.8810\n",
            "Epoch 128/500\n",
            "108/108 [==============================] - 0s 590us/step - loss: 2.2093 - accuracy: 0.9815 - val_loss: 62.3473 - val_accuracy: 0.7857\n",
            "Epoch 129/500\n",
            "108/108 [==============================] - 0s 581us/step - loss: 1.1959 - accuracy: 0.9907 - val_loss: 28.1280 - val_accuracy: 0.9286\n",
            "Epoch 130/500\n",
            "108/108 [==============================] - 0s 573us/step - loss: 2.7750 - accuracy: 0.9815 - val_loss: 37.2409 - val_accuracy: 0.9048\n",
            "Epoch 131/500\n",
            "108/108 [==============================] - 0s 620us/step - loss: 0.2541 - accuracy: 0.9907 - val_loss: 44.9546 - val_accuracy: 0.8810\n",
            "Epoch 132/500\n",
            "108/108 [==============================] - 0s 606us/step - loss: 8.3991e-16 - accuracy: 1.0000 - val_loss: 42.7946 - val_accuracy: 0.8810\n",
            "Epoch 133/500\n",
            "108/108 [==============================] - 0s 611us/step - loss: 1.3221e-25 - accuracy: 1.0000 - val_loss: 41.3699 - val_accuracy: 0.8810\n",
            "Epoch 134/500\n",
            "108/108 [==============================] - 0s 601us/step - loss: 1.0293e-34 - accuracy: 1.0000 - val_loss: 40.4308 - val_accuracy: 0.8810\n",
            "Epoch 135/500\n",
            "108/108 [==============================] - 0s 589us/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 39.8116 - val_accuracy: 0.8810\n",
            "Epoch 136/500\n",
            "108/108 [==============================] - 0s 573us/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 39.4034 - val_accuracy: 0.8810\n",
            "Epoch 137/500\n",
            "108/108 [==============================] - 0s 588us/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 39.1343 - val_accuracy: 0.8810\n",
            "Epoch 138/500\n",
            "108/108 [==============================] - 0s 606us/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 38.9570 - val_accuracy: 0.8810\n",
            "Epoch 139/500\n",
            "108/108 [==============================] - 0s 598us/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 38.8401 - val_accuracy: 0.8810\n",
            "Epoch 140/500\n",
            "108/108 [==============================] - 0s 598us/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 38.7630 - val_accuracy: 0.8810\n",
            "Epoch 141/500\n",
            "108/108 [==============================] - 0s 598us/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 38.7122 - val_accuracy: 0.8810\n",
            "Epoch 142/500\n",
            "108/108 [==============================] - 0s 583us/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 38.6788 - val_accuracy: 0.8810\n",
            "Epoch 143/500\n",
            "108/108 [==============================] - 0s 595us/step - loss: 5.2634e-39 - accuracy: 1.0000 - val_loss: 38.6567 - val_accuracy: 0.8810\n",
            "Epoch 144/500\n",
            "108/108 [==============================] - 0s 592us/step - loss: 7.0071e-39 - accuracy: 1.0000 - val_loss: 38.6422 - val_accuracy: 0.8810\n",
            "Epoch 145/500\n",
            "108/108 [==============================] - 0s 610us/step - loss: 9.0579e-39 - accuracy: 1.0000 - val_loss: 38.6326 - val_accuracy: 0.8810\n",
            "Epoch 146/500\n",
            "108/108 [==============================] - 0s 605us/step - loss: 9.5775e-39 - accuracy: 1.0000 - val_loss: 38.6263 - val_accuracy: 0.8810\n",
            "Epoch 147/500\n",
            "108/108 [==============================] - 0s 638us/step - loss: 1.0395e-38 - accuracy: 1.0000 - val_loss: 38.6221 - val_accuracy: 0.8810\n",
            "Epoch 148/500\n",
            "108/108 [==============================] - 0s 633us/step - loss: 1.0969e-38 - accuracy: 1.0000 - val_loss: 38.6194 - val_accuracy: 0.8810\n",
            "Epoch 149/500\n",
            "108/108 [==============================] - 0s 636us/step - loss: 1.1516e-38 - accuracy: 1.0000 - val_loss: 38.6176 - val_accuracy: 0.8810\n",
            "Epoch 150/500\n",
            "108/108 [==============================] - 0s 610us/step - loss: 1.1782e-38 - accuracy: 1.0000 - val_loss: 38.6164 - val_accuracy: 0.8810\n",
            "Epoch 151/500\n",
            "108/108 [==============================] - 0s 642us/step - loss: 1.1857e-38 - accuracy: 1.0000 - val_loss: 38.6156 - val_accuracy: 0.8810\n",
            "Epoch 152/500\n",
            "108/108 [==============================] - 0s 702us/step - loss: 1.1984e-38 - accuracy: 1.0000 - val_loss: 38.6151 - val_accuracy: 0.8810\n",
            "Epoch 153/500\n",
            "108/108 [==============================] - 0s 715us/step - loss: 1.2044e-38 - accuracy: 1.0000 - val_loss: 38.6148 - val_accuracy: 0.8810\n",
            "Epoch 154/500\n",
            "108/108 [==============================] - 0s 667us/step - loss: 1.2076e-38 - accuracy: 1.0000 - val_loss: 38.6146 - val_accuracy: 0.8810\n",
            "Epoch 155/500\n",
            "108/108 [==============================] - 0s 694us/step - loss: 1.2100e-38 - accuracy: 1.0000 - val_loss: 38.6145 - val_accuracy: 0.8810\n",
            "Epoch 156/500\n",
            "108/108 [==============================] - 0s 723us/step - loss: 1.2106e-38 - accuracy: 1.0000 - val_loss: 38.6145 - val_accuracy: 0.8810\n",
            "Epoch 157/500\n",
            "108/108 [==============================] - 0s 647us/step - loss: 1.2119e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 158/500\n",
            "108/108 [==============================] - 0s 586us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 159/500\n",
            "108/108 [==============================] - 0s 573us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 160/500\n",
            "108/108 [==============================] - 0s 583us/step - loss: 1.2119e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 161/500\n",
            "108/108 [==============================] - 0s 600us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 162/500\n",
            "108/108 [==============================] - 0s 595us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 163/500\n",
            "108/108 [==============================] - 0s 605us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 164/500\n",
            "108/108 [==============================] - 0s 583us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 165/500\n",
            "108/108 [==============================] - 0s 600us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 166/500\n",
            "108/108 [==============================] - 0s 588us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 167/500\n",
            "108/108 [==============================] - 0s 601us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 168/500\n",
            "108/108 [==============================] - 0s 601us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 169/500\n",
            "108/108 [==============================] - 0s 589us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 170/500\n",
            "108/108 [==============================] - 0s 585us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 171/500\n",
            "108/108 [==============================] - 0s 595us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 172/500\n",
            "108/108 [==============================] - 0s 598us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 173/500\n",
            "108/108 [==============================] - 0s 590us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 174/500\n",
            "108/108 [==============================] - 0s 658us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 175/500\n",
            "108/108 [==============================] - 0s 603us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 176/500\n",
            "108/108 [==============================] - 0s 611us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 177/500\n",
            "108/108 [==============================] - 0s 590us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 178/500\n",
            "108/108 [==============================] - 0s 582us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 179/500\n",
            "108/108 [==============================] - 0s 593us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 180/500\n",
            "108/108 [==============================] - 0s 615us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 181/500\n",
            "108/108 [==============================] - 0s 594us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 182/500\n",
            "108/108 [==============================] - 0s 608us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 183/500\n",
            "108/108 [==============================] - 0s 603us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 184/500\n",
            "108/108 [==============================] - 0s 600us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 185/500\n",
            "108/108 [==============================] - 0s 628us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 186/500\n",
            "108/108 [==============================] - 0s 595us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 187/500\n",
            "108/108 [==============================] - 0s 607us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 188/500\n",
            "108/108 [==============================] - 0s 612us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 189/500\n",
            "108/108 [==============================] - 0s 602us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 190/500\n",
            "108/108 [==============================] - 0s 601us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 191/500\n",
            "108/108 [==============================] - 0s 573us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 192/500\n",
            "108/108 [==============================] - 0s 578us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 193/500\n",
            "108/108 [==============================] - 0s 592us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 194/500\n",
            "108/108 [==============================] - 0s 579us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 195/500\n",
            "108/108 [==============================] - 0s 599us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 196/500\n",
            "108/108 [==============================] - 0s 591us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 197/500\n",
            "108/108 [==============================] - 0s 576us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 198/500\n",
            "108/108 [==============================] - 0s 586us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 199/500\n",
            "108/108 [==============================] - 0s 603us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 200/500\n",
            "108/108 [==============================] - 0s 629us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 201/500\n",
            "108/108 [==============================] - 0s 605us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 202/500\n",
            "108/108 [==============================] - 0s 583us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 203/500\n",
            "108/108 [==============================] - 0s 600us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 204/500\n",
            "108/108 [==============================] - 0s 607us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 205/500\n",
            "108/108 [==============================] - 0s 593us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 206/500\n",
            "108/108 [==============================] - 0s 620us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 207/500\n",
            "108/108 [==============================] - 0s 591us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 208/500\n",
            "108/108 [==============================] - 0s 579us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 209/500\n",
            "108/108 [==============================] - 0s 574us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 210/500\n",
            "108/108 [==============================] - 0s 583us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 211/500\n",
            "108/108 [==============================] - 0s 611us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 212/500\n",
            "108/108 [==============================] - 0s 588us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 213/500\n",
            "108/108 [==============================] - 0s 605us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 214/500\n",
            "108/108 [==============================] - 0s 589us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 215/500\n",
            "108/108 [==============================] - 0s 579us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 216/500\n",
            "108/108 [==============================] - 0s 593us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 217/500\n",
            "108/108 [==============================] - 0s 595us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 218/500\n",
            "108/108 [==============================] - 0s 597us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 219/500\n",
            "108/108 [==============================] - 0s 604us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 220/500\n",
            "108/108 [==============================] - 0s 605us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 221/500\n",
            "108/108 [==============================] - 0s 582us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 222/500\n",
            "108/108 [==============================] - 0s 599us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 223/500\n",
            "108/108 [==============================] - 0s 613us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 224/500\n",
            "108/108 [==============================] - 0s 605us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 225/500\n",
            "108/108 [==============================] - 0s 596us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 226/500\n",
            "108/108 [==============================] - 0s 587us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 227/500\n",
            "108/108 [==============================] - 0s 588us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 228/500\n",
            "108/108 [==============================] - 0s 600us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 229/500\n",
            "108/108 [==============================] - 0s 607us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 230/500\n",
            "108/108 [==============================] - 0s 609us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 231/500\n",
            "108/108 [==============================] - 0s 596us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 232/500\n",
            "108/108 [==============================] - 0s 586us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 233/500\n",
            "108/108 [==============================] - 0s 594us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 234/500\n",
            "108/108 [==============================] - 0s 591us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 235/500\n",
            "108/108 [==============================] - 0s 614us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 236/500\n",
            "108/108 [==============================] - 0s 611us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 237/500\n",
            "108/108 [==============================] - 0s 598us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 238/500\n",
            "108/108 [==============================] - 0s 598us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 239/500\n",
            "108/108 [==============================] - 0s 582us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 240/500\n",
            "108/108 [==============================] - 0s 611us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 241/500\n",
            "108/108 [==============================] - 0s 595us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 242/500\n",
            "108/108 [==============================] - 0s 600us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 243/500\n",
            "108/108 [==============================] - 0s 606us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 244/500\n",
            "108/108 [==============================] - 0s 636us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 245/500\n",
            "108/108 [==============================] - 0s 619us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 246/500\n",
            "108/108 [==============================] - 0s 619us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 247/500\n",
            "108/108 [==============================] - 0s 661us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 248/500\n",
            "108/108 [==============================] - 0s 623us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 249/500\n",
            "108/108 [==============================] - 0s 625us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 250/500\n",
            "108/108 [==============================] - 0s 598us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 251/500\n",
            "108/108 [==============================] - 0s 609us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 252/500\n",
            "108/108 [==============================] - 0s 608us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 253/500\n",
            "108/108 [==============================] - 0s 625us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 254/500\n",
            "108/108 [==============================] - 0s 624us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 255/500\n",
            "108/108 [==============================] - 0s 622us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 256/500\n",
            "108/108 [==============================] - 0s 608us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 257/500\n",
            "108/108 [==============================] - 0s 601us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 258/500\n",
            "108/108 [==============================] - 0s 609us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 259/500\n",
            "108/108 [==============================] - 0s 626us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 260/500\n",
            "108/108 [==============================] - 0s 633us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 261/500\n",
            "108/108 [==============================] - 0s 603us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 262/500\n",
            "108/108 [==============================] - 0s 594us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 263/500\n",
            "108/108 [==============================] - 0s 595us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 264/500\n",
            "108/108 [==============================] - 0s 618us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 265/500\n",
            "108/108 [==============================] - 0s 611us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 266/500\n",
            "108/108 [==============================] - 0s 584us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 267/500\n",
            "108/108 [==============================] - 0s 589us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 268/500\n",
            "108/108 [==============================] - 0s 615us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 269/500\n",
            "108/108 [==============================] - 0s 608us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 270/500\n",
            "108/108 [==============================] - 0s 608us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 271/500\n",
            "108/108 [==============================] - 0s 595us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 272/500\n",
            "108/108 [==============================] - 0s 592us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 273/500\n",
            "108/108 [==============================] - 0s 618us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 274/500\n",
            "108/108 [==============================] - 0s 621us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 275/500\n",
            "108/108 [==============================] - 0s 637us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 276/500\n",
            "108/108 [==============================] - 0s 614us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 277/500\n",
            "108/108 [==============================] - 0s 602us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 278/500\n",
            "108/108 [==============================] - 0s 599us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 279/500\n",
            "108/108 [==============================] - 0s 616us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 280/500\n",
            "108/108 [==============================] - 0s 618us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 281/500\n",
            "108/108 [==============================] - 0s 602us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 282/500\n",
            "108/108 [==============================] - 0s 585us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 283/500\n",
            "108/108 [==============================] - 0s 604us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 284/500\n",
            "108/108 [==============================] - 0s 589us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 285/500\n",
            "108/108 [==============================] - 0s 612us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 286/500\n",
            "108/108 [==============================] - 0s 604us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 287/500\n",
            "108/108 [==============================] - 0s 618us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 288/500\n",
            "108/108 [==============================] - 0s 606us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 289/500\n",
            "108/108 [==============================] - 0s 599us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 290/500\n",
            "108/108 [==============================] - 0s 596us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 291/500\n",
            "108/108 [==============================] - 0s 621us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 292/500\n",
            "108/108 [==============================] - 0s 601us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 293/500\n",
            "108/108 [==============================] - 0s 602us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 294/500\n",
            "108/108 [==============================] - 0s 582us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 295/500\n",
            "108/108 [==============================] - 0s 593us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 296/500\n",
            "108/108 [==============================] - 0s 601us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 297/500\n",
            "108/108 [==============================] - 0s 608us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 298/500\n",
            "108/108 [==============================] - 0s 610us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 299/500\n",
            "108/108 [==============================] - 0s 600us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 300/500\n",
            "108/108 [==============================] - 0s 587us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 301/500\n",
            "108/108 [==============================] - 0s 583us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 302/500\n",
            "108/108 [==============================] - 0s 579us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 303/500\n",
            "108/108 [==============================] - 0s 618us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 304/500\n",
            "108/108 [==============================] - 0s 604us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 305/500\n",
            "108/108 [==============================] - 0s 615us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 306/500\n",
            "108/108 [==============================] - 0s 596us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 307/500\n",
            "108/108 [==============================] - 0s 583us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 308/500\n",
            "108/108 [==============================] - 0s 594us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 309/500\n",
            "108/108 [==============================] - 0s 617us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 310/500\n",
            "108/108 [==============================] - 0s 601us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 311/500\n",
            "108/108 [==============================] - 0s 657us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 312/500\n",
            "108/108 [==============================] - 0s 608us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 313/500\n",
            "108/108 [==============================] - 0s 585us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 314/500\n",
            "108/108 [==============================] - 0s 596us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 315/500\n",
            "108/108 [==============================] - 0s 610us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 316/500\n",
            "108/108 [==============================] - 0s 590us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 317/500\n",
            "108/108 [==============================] - 0s 590us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 318/500\n",
            "108/108 [==============================] - 0s 610us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 319/500\n",
            "108/108 [==============================] - 0s 617us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 320/500\n",
            "108/108 [==============================] - 0s 605us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 321/500\n",
            "108/108 [==============================] - 0s 585us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 322/500\n",
            "108/108 [==============================] - 0s 577us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 323/500\n",
            "108/108 [==============================] - 0s 587us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 324/500\n",
            "108/108 [==============================] - 0s 602us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 325/500\n",
            "108/108 [==============================] - 0s 594us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 326/500\n",
            "108/108 [==============================] - 0s 584us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 327/500\n",
            "108/108 [==============================] - 0s 582us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 328/500\n",
            "108/108 [==============================] - 0s 589us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 329/500\n",
            "108/108 [==============================] - 0s 597us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 330/500\n",
            "108/108 [==============================] - 0s 619us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 331/500\n",
            "108/108 [==============================] - 0s 596us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 332/500\n",
            "108/108 [==============================] - 0s 600us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 333/500\n",
            "108/108 [==============================] - 0s 605us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 334/500\n",
            "108/108 [==============================] - 0s 598us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 335/500\n",
            "108/108 [==============================] - 0s 600us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 336/500\n",
            "108/108 [==============================] - 0s 617us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 337/500\n",
            "108/108 [==============================] - 0s 598us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 338/500\n",
            "108/108 [==============================] - 0s 593us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 339/500\n",
            "108/108 [==============================] - 0s 604us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 340/500\n",
            "108/108 [==============================] - 0s 596us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 341/500\n",
            "108/108 [==============================] - 0s 610us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 342/500\n",
            "108/108 [==============================] - 0s 612us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 343/500\n",
            "108/108 [==============================] - 0s 608us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 344/500\n",
            "108/108 [==============================] - 0s 586us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 345/500\n",
            "108/108 [==============================] - 0s 611us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 346/500\n",
            "108/108 [==============================] - 0s 595us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 347/500\n",
            "108/108 [==============================] - 0s 602us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 348/500\n",
            "108/108 [==============================] - 0s 619us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 349/500\n",
            "108/108 [==============================] - 0s 604us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 350/500\n",
            "108/108 [==============================] - 0s 605us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 351/500\n",
            "108/108 [==============================] - 0s 601us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 352/500\n",
            "108/108 [==============================] - 0s 596us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 353/500\n",
            "108/108 [==============================] - 0s 609us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 354/500\n",
            "108/108 [==============================] - 0s 616us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 355/500\n",
            "108/108 [==============================] - 0s 609us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 356/500\n",
            "108/108 [==============================] - 0s 591us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 357/500\n",
            "108/108 [==============================] - 0s 607us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 358/500\n",
            "108/108 [==============================] - 0s 609us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 359/500\n",
            "108/108 [==============================] - 0s 616us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 360/500\n",
            "108/108 [==============================] - 0s 608us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 361/500\n",
            "108/108 [==============================] - 0s 603us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 362/500\n",
            "108/108 [==============================] - 0s 626us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 363/500\n",
            "108/108 [==============================] - 0s 608us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 364/500\n",
            "108/108 [==============================] - 0s 627us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 365/500\n",
            "108/108 [==============================] - 0s 626us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 366/500\n",
            "108/108 [==============================] - 0s 654us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 367/500\n",
            "108/108 [==============================] - 0s 596us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 368/500\n",
            "108/108 [==============================] - 0s 596us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 369/500\n",
            "108/108 [==============================] - 0s 593us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 370/500\n",
            "108/108 [==============================] - 0s 617us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 371/500\n",
            "108/108 [==============================] - 0s 609us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 372/500\n",
            "108/108 [==============================] - 0s 596us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 373/500\n",
            "108/108 [==============================] - 0s 598us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 374/500\n",
            "108/108 [==============================] - 0s 599us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 375/500\n",
            "108/108 [==============================] - 0s 626us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 376/500\n",
            "108/108 [==============================] - 0s 601us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 377/500\n",
            "108/108 [==============================] - 0s 611us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 378/500\n",
            "108/108 [==============================] - 0s 608us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 379/500\n",
            "108/108 [==============================] - 0s 591us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 380/500\n",
            "108/108 [==============================] - 0s 594us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 381/500\n",
            "108/108 [==============================] - 0s 605us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 382/500\n",
            "108/108 [==============================] - 0s 606us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 383/500\n",
            "108/108 [==============================] - 0s 622us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 384/500\n",
            "108/108 [==============================] - 0s 604us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 385/500\n",
            "108/108 [==============================] - 0s 597us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 386/500\n",
            "108/108 [==============================] - 0s 582us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 387/500\n",
            "108/108 [==============================] - 0s 605us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 388/500\n",
            "108/108 [==============================] - 0s 591us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 389/500\n",
            "108/108 [==============================] - 0s 587us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 390/500\n",
            "108/108 [==============================] - 0s 579us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 391/500\n",
            "108/108 [==============================] - 0s 583us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 392/500\n",
            "108/108 [==============================] - 0s 590us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 393/500\n",
            "108/108 [==============================] - 0s 620us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 394/500\n",
            "108/108 [==============================] - 0s 627us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 395/500\n",
            "108/108 [==============================] - 0s 627us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 396/500\n",
            "108/108 [==============================] - 0s 611us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 397/500\n",
            "108/108 [==============================] - 0s 590us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 398/500\n",
            "108/108 [==============================] - 0s 596us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 399/500\n",
            "108/108 [==============================] - 0s 612us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 400/500\n",
            "108/108 [==============================] - 0s 611us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 401/500\n",
            "108/108 [==============================] - 0s 606us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 402/500\n",
            "108/108 [==============================] - 0s 605us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 403/500\n",
            "108/108 [==============================] - 0s 621us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 404/500\n",
            "108/108 [==============================] - 0s 601us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 405/500\n",
            "108/108 [==============================] - 0s 601us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 406/500\n",
            "108/108 [==============================] - 0s 584us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 407/500\n",
            "108/108 [==============================] - 0s 599us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 408/500\n",
            "108/108 [==============================] - 0s 604us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 409/500\n",
            "108/108 [==============================] - 0s 642us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 410/500\n",
            "108/108 [==============================] - 0s 604us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 411/500\n",
            "108/108 [==============================] - 0s 594us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 412/500\n",
            "108/108 [==============================] - 0s 587us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 413/500\n",
            "108/108 [==============================] - 0s 602us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 414/500\n",
            "108/108 [==============================] - 0s 616us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 415/500\n",
            "108/108 [==============================] - 0s 609us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 416/500\n",
            "108/108 [==============================] - 0s 610us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 417/500\n",
            "108/108 [==============================] - 0s 592us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 418/500\n",
            "108/108 [==============================] - 0s 593us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 419/500\n",
            "108/108 [==============================] - 0s 593us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 420/500\n",
            "108/108 [==============================] - 0s 613us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 421/500\n",
            "108/108 [==============================] - 0s 614us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 422/500\n",
            "108/108 [==============================] - 0s 593us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 423/500\n",
            "108/108 [==============================] - 0s 616us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 424/500\n",
            "108/108 [==============================] - 0s 609us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 425/500\n",
            "108/108 [==============================] - 0s 644us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 426/500\n",
            "108/108 [==============================] - 0s 628us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 427/500\n",
            "108/108 [==============================] - 0s 591us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 428/500\n",
            "108/108 [==============================] - 0s 583us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 429/500\n",
            "108/108 [==============================] - 0s 585us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 430/500\n",
            "108/108 [==============================] - 0s 613us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 431/500\n",
            "108/108 [==============================] - 0s 605us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 432/500\n",
            "108/108 [==============================] - 0s 603us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 433/500\n",
            "108/108 [==============================] - 0s 592us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 434/500\n",
            "108/108 [==============================] - 0s 588us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 435/500\n",
            "108/108 [==============================] - 0s 607us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 436/500\n",
            "108/108 [==============================] - 0s 628us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 437/500\n",
            "108/108 [==============================] - 0s 607us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 438/500\n",
            "108/108 [==============================] - 0s 608us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 439/500\n",
            "108/108 [==============================] - 0s 599us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 440/500\n",
            "108/108 [==============================] - 0s 585us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 441/500\n",
            "108/108 [==============================] - 0s 605us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 442/500\n",
            "108/108 [==============================] - 0s 619us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 443/500\n",
            "108/108 [==============================] - 0s 608us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 444/500\n",
            "108/108 [==============================] - 0s 592us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 445/500\n",
            "108/108 [==============================] - 0s 595us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 446/500\n",
            "108/108 [==============================] - 0s 603us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 447/500\n",
            "108/108 [==============================] - 0s 605us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 448/500\n",
            "108/108 [==============================] - 0s 610us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 449/500\n",
            "108/108 [==============================] - 0s 603us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 450/500\n",
            "108/108 [==============================] - 0s 588us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 451/500\n",
            "108/108 [==============================] - 0s 602us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 452/500\n",
            "108/108 [==============================] - 0s 612us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 453/500\n",
            "108/108 [==============================] - 0s 599us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 454/500\n",
            "108/108 [==============================] - 0s 599us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 455/500\n",
            "108/108 [==============================] - 0s 603us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 456/500\n",
            "108/108 [==============================] - 0s 591us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 457/500\n",
            "108/108 [==============================] - 0s 596us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 458/500\n",
            "108/108 [==============================] - 0s 613us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 459/500\n",
            "108/108 [==============================] - 0s 619us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 460/500\n",
            "108/108 [==============================] - 0s 602us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 461/500\n",
            "108/108 [==============================] - 0s 586us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 462/500\n",
            "108/108 [==============================] - 0s 590us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 463/500\n",
            "108/108 [==============================] - 0s 614us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 464/500\n",
            "108/108 [==============================] - 0s 609us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 465/500\n",
            "108/108 [==============================] - 0s 593us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 466/500\n",
            "108/108 [==============================] - 0s 584us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 467/500\n",
            "108/108 [==============================] - 0s 590us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 468/500\n",
            "108/108 [==============================] - 0s 592us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 469/500\n",
            "108/108 [==============================] - 0s 614us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 470/500\n",
            "108/108 [==============================] - 0s 604us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 471/500\n",
            "108/108 [==============================] - 0s 603us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 472/500\n",
            "108/108 [==============================] - 0s 597us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 473/500\n",
            "108/108 [==============================] - 0s 591us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 474/500\n",
            "108/108 [==============================] - 0s 590us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 475/500\n",
            "108/108 [==============================] - 0s 611us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 476/500\n",
            "108/108 [==============================] - 0s 608us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 477/500\n",
            "108/108 [==============================] - 0s 611us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 478/500\n",
            "108/108 [==============================] - 0s 584us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 479/500\n",
            "108/108 [==============================] - 0s 592us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 480/500\n",
            "108/108 [==============================] - 0s 594us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 481/500\n",
            "108/108 [==============================] - 0s 615us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 482/500\n",
            "108/108 [==============================] - 0s 585us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 483/500\n",
            "108/108 [==============================] - 0s 583us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 484/500\n",
            "108/108 [==============================] - 0s 589us/step - loss: 1.2122e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 485/500\n",
            "108/108 [==============================] - 0s 612us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 486/500\n",
            "108/108 [==============================] - 0s 604us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 487/500\n",
            "108/108 [==============================] - 0s 600us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 488/500\n",
            "108/108 [==============================] - 0s 591us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 489/500\n",
            "108/108 [==============================] - 0s 598us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 490/500\n",
            "108/108 [==============================] - 0s 602us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 491/500\n",
            "108/108 [==============================] - 0s 619us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 492/500\n",
            "108/108 [==============================] - 0s 613us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 493/500\n",
            "108/108 [==============================] - 0s 592us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 494/500\n",
            "108/108 [==============================] - 0s 580us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 495/500\n",
            "108/108 [==============================] - 0s 596us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 496/500\n",
            "108/108 [==============================] - 0s 581us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 497/500\n",
            "108/108 [==============================] - 0s 608us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 498/500\n",
            "108/108 [==============================] - 0s 604us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 499/500\n",
            "108/108 [==============================] - 0s 593us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n",
            "Epoch 500/500\n",
            "108/108 [==============================] - 0s 593us/step - loss: 1.2120e-38 - accuracy: 1.0000 - val_loss: 38.6144 - val_accuracy: 0.8810\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fdff954fe48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Dq7IG1p_CrJ"
      },
      "source": [
        "def generate_triplet(x,y,testsize=0.3,ap_pairs=10,an_pairs=10):\n",
        "    data_xy = tuple([x,y])\n",
        "\n",
        "    trainsize = 1-testsize\n",
        "\n",
        "    triplet_train_pairs = []\n",
        "    triplet_test_pairs = []\n",
        "    for data_class in sorted(set(data_xy[1])):\n",
        "\n",
        "        same_class_idx = np.where((data_xy[1] == data_class))[0]\n",
        "        diff_class_idx = np.where(data_xy[1] != data_class)[0]\n",
        "        A_P_pairs = random.sample(list(permutations(same_class_idx,2)),k=ap_pairs) #Generating Anchor-Positive pairs\n",
        "        Neg_idx = random.sample(list(diff_class_idx),k=an_pairs)\n",
        "        \n",
        "\n",
        "        #train\n",
        "        A_P_len = len(A_P_pairs)\n",
        "        Neg_len = len(Neg_idx)\n",
        "        for ap in A_P_pairs[:int(A_P_len*trainsize)]:\n",
        "            Anchor = data_xy[0][ap[0]]\n",
        "            Positive = data_xy[0][ap[1]]\n",
        "            for n in Neg_idx:\n",
        "                Negative = data_xy[0][n]\n",
        "                triplet_train_pairs.append([Anchor,Positive,Negative])               \n",
        "        #test\n",
        "        for ap in A_P_pairs[int(A_P_len*trainsize):]:\n",
        "            Anchor = data_xy[0][ap[0]]\n",
        "            Positive = data_xy[0][ap[1]]\n",
        "            for n in Neg_idx:\n",
        "                Negative = data_xy[0][n]\n",
        "                triplet_test_pairs.append([Anchor,Positive,Negative])    \n",
        "                \n",
        "    return np.array(triplet_train_pairs), np.array(triplet_test_pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaAw6qVn_Jr4"
      },
      "source": [
        "X_train, X_test = generate_triplet(x_train_flat,y_train, ap_pairs=20, an_pairs=20,testsize=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fM-zYr2H_J8Z"
      },
      "source": [
        "def triplet_loss(y_true, y_pred, alpha = 0.4):\n",
        "    \"\"\"\n",
        "    Implementation of the triplet loss function\n",
        "    Arguments:\n",
        "    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
        "    y_pred -- python list containing three objects:\n",
        "            anchor -- the encodings for the anchor data\n",
        "            positive -- the encodings for the positive data (similar to anchor)\n",
        "            negative -- the encodings for the negative data (different from anchor)\n",
        "    Returns:\n",
        "    loss -- real number, value of the loss\n",
        "    \"\"\"\n",
        "    print('y_pred.shape = ',y_pred)\n",
        "    \n",
        "    total_lenght = y_pred.shape.as_list()[-1]\n",
        "#     print('total_lenght=',  total_lenght)\n",
        "#     total_lenght =12\n",
        "    \n",
        "    anchor = y_pred[:,0:int(total_lenght*1/3)]\n",
        "    positive = y_pred[:,int(total_lenght*1/3):int(total_lenght*2/3)]\n",
        "    negative = y_pred[:,int(total_lenght*2/3):int(total_lenght*3/3)]\n",
        "\n",
        "    # distance between the anchor and the positive\n",
        "    pos_dist = K.sum(K.square(anchor-positive),axis=1)\n",
        "\n",
        "    # distance between the anchor and the negative\n",
        "    neg_dist = K.sum(K.square(anchor-negative),axis=1)\n",
        "\n",
        "    # compute loss\n",
        "    basic_loss = pos_dist-neg_dist+alpha\n",
        "    loss = K.maximum(basic_loss,0.0)\n",
        " \n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ctyt-1KvGGRc"
      },
      "source": [
        "def create_base_network(in_dims):\n",
        "    \"\"\"\n",
        "    Base network to be shared.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(5,(1,1),padding='same',input_shape=(in_dims[0],in_dims[1],in_dims[2],),activation='relu',name='conv1'))\n",
        "    #model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D((2,2),(2,2),padding='same',name='pool1'))\n",
        "    model.add(Conv2D(10,(1,1),padding='same',activation='relu',name='conv2'))\n",
        "    model.add(MaxPooling2D((2,2),(2,2),padding='same',name='pool2'))\n",
        "    model.add(Flatten(name='flatten'))\n",
        "    model.add(Dense(500,activation='relu'))\n",
        "    model.add(Dense(500,activation='relu'))\n",
        "\n",
        "    model.add(Dense(4,name='embeddings'))\n",
        "    # model.add(Dense(600))\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBOsP1P5GIEk"
      },
      "source": [
        "adam_optim = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFg5IIXiGKf_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "84ca2c5c-6d4a-4031-ab0a-fcbc18495ca0"
      },
      "source": [
        "anchor_input = Input((512,512,1, ), name='anchor_input')\n",
        "positive_input = Input((512,512,1, ), name='positive_input')\n",
        "negative_input = Input((512,512,1, ), name='negative_input')\n",
        "\n",
        "# Shared embedding layer for positive and negative items\n",
        "Shared_DNN = create_base_network([512,512,1,])\n",
        "\n",
        "\n",
        "encoded_anchor = Shared_DNN(anchor_input)\n",
        "encoded_positive = Shared_DNN(positive_input)\n",
        "encoded_negative = Shared_DNN(negative_input)\n",
        "\n",
        "\n",
        "merged_vector = concatenate([encoded_anchor, encoded_positive, encoded_negative], axis=-1, name='merged_layer')\n",
        "\n",
        "model = Model(inputs=[anchor_input,positive_input, negative_input], outputs=merged_vector)\n",
        "model.compile(loss=triplet_loss, optimizer=adam_optim)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_pred.shape =  Tensor(\"merged_layer_1/concat:0\", shape=(None, 12), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHM2xlosGQt1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "bc8d2fe1-27bf-4377-cb07-4ee832d534c0"
      },
      "source": [
        "Anchor = X_train[:,0,:].reshape(-1,512,512,1)\n",
        "Positive = X_train[:,1,:].reshape(-1,512,512,1)\n",
        "Negative = X_train[:,2,:].reshape(-1,512,512,1)\n",
        "Anchor_test = X_test[:,0,:].reshape(-1,512,512,1)\n",
        "Positive_test = X_test[:,1,:].reshape(-1,512,512,1)\n",
        "Negative_test = X_test[:,2,:].reshape(-1,512,512,1)\n",
        "\n",
        "Y_dummy = np.empty((Anchor.shape[0],40))\n",
        "Y_dummy2 = np.empty((Anchor_test.shape[0],1))\n",
        "\n",
        "model.fit([Anchor,Positive,Negative],y=Y_dummy,validation_data=([Anchor_test,Positive_test,Negative_test],Y_dummy2), batch_size=12, epochs=10)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 640 samples, validate on 160 samples\n",
            "Epoch 1/10\n",
            "640/640 [==============================] - 25s 39ms/step - loss: 144.9852 - val_loss: 8295.6074\n",
            "Epoch 2/10\n",
            "640/640 [==============================] - 25s 39ms/step - loss: 118.9563 - val_loss: 24777.4201\n",
            "Epoch 3/10\n",
            "640/640 [==============================] - 25s 39ms/step - loss: 27.7140 - val_loss: 28123.7209\n",
            "Epoch 4/10\n",
            "640/640 [==============================] - 26s 40ms/step - loss: 10.6498 - val_loss: 30618.6740\n",
            "Epoch 5/10\n",
            "640/640 [==============================] - 25s 40ms/step - loss: 0.0954 - val_loss: 30813.3431\n",
            "Epoch 6/10\n",
            "640/640 [==============================] - 25s 38ms/step - loss: 0.0000e+00 - val_loss: 30934.3063\n",
            "Epoch 7/10\n",
            "640/640 [==============================] - 24s 38ms/step - loss: 0.0000e+00 - val_loss: 30934.6778\n",
            "Epoch 8/10\n",
            "640/640 [==============================] - 24s 38ms/step - loss: 0.0000e+00 - val_loss: 30934.6891\n",
            "Epoch 9/10\n",
            "640/640 [==============================] - 24s 38ms/step - loss: 0.0000e+00 - val_loss: 30934.6891\n",
            "Epoch 10/10\n",
            "640/640 [==============================] - 25s 39ms/step - loss: 0.0000e+00 - val_loss: 30934.6891\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f08f0d0bfd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnRJJVDL_KdN"
      },
      "source": [
        "trained_model = Model(inputs=anchor_input, outputs=encoded_anchor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2W_Dv2u_Kpx"
      },
      "source": [
        "tsne = TSNE()\n",
        "X_train_trm = trained_model.predict(x_train[:108].reshape(-1,512,512,1))\n",
        "X_test_trm = trained_model.predict(x_test[:42].reshape(-1,512,512,1))\n",
        "train_tsne_embeds = tsne.fit_transform(X_train_trm)\n",
        "eval_tsne_embeds = tsne.fit_transform(X_test_trm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLLmOcZH_wk3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "590e67e1-e58a-40b5-b17d-9ffe36c2ebb8"
      },
      "source": [
        "scatter(eval_tsne_embeds, y_test[:512], \"Validation Data After TNN\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:154: RuntimeWarning: invalid value encountered in true_divide\n",
            "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n",
            "posx and posy should be finite values\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "940P9wQUrKXZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d0b8f616-4746-4124-9472-ebfb3ef5ba7d"
      },
      "source": [
        "X_train_trm = trained_model.predict(x_train.reshape(-1,512,512,1))\n",
        "X_test_trm = trained_model.predict(x_test.reshape(-1,512,512,1))\n",
        "\n",
        "Classifier_input = Input((4,))\n",
        "Classifier_output = Dense(1, activation='sigmoid')(Classifier_input)\n",
        "Classifier_model = Model(Classifier_input, Classifier_output)\n",
        "\n",
        "\n",
        "Classifier_model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "Classifier_model.fit(X_train_trm,y_train_onehot, validation_data=(X_test_trm,y_test_onehot),epochs=1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 108 samples, validate on 42 samples\n",
            "Epoch 1/1000\n",
            "108/108 [==============================] - 0s 792us/step - loss: 41.0588 - f1: 0.5000 - val_loss: 41.0370 - val_f1: 0.5000\n",
            "Epoch 2/1000\n",
            "108/108 [==============================] - 0s 62us/step - loss: 39.0751 - f1: 1.0000 - val_loss: 39.3856 - val_f1: 0.5000\n",
            "Epoch 3/1000\n",
            "108/108 [==============================] - 0s 64us/step - loss: 36.9167 - f1: 1.0000 - val_loss: 37.7353 - val_f1: 0.5000\n",
            "Epoch 4/1000\n",
            "108/108 [==============================] - 0s 63us/step - loss: 34.8769 - f1: 1.0000 - val_loss: 36.0823 - val_f1: 0.5000\n",
            "Epoch 5/1000\n",
            "108/108 [==============================] - 0s 62us/step - loss: 32.8154 - f1: 1.0000 - val_loss: 34.4418 - val_f1: 0.5000\n",
            "Epoch 6/1000\n",
            "108/108 [==============================] - 0s 61us/step - loss: 30.7967 - f1: 1.0000 - val_loss: 32.7986 - val_f1: 0.5000\n",
            "Epoch 7/1000\n",
            "108/108 [==============================] - 0s 60us/step - loss: 28.9176 - f1: 0.7500 - val_loss: 31.1581 - val_f1: 0.5000\n",
            "Epoch 8/1000\n",
            "108/108 [==============================] - 0s 62us/step - loss: 27.0582 - f1: 1.0000 - val_loss: 29.5532 - val_f1: 0.5000\n",
            "Epoch 9/1000\n",
            "108/108 [==============================] - 0s 63us/step - loss: 25.2693 - f1: 1.0000 - val_loss: 28.0640 - val_f1: 0.5000\n",
            "Epoch 10/1000\n",
            "108/108 [==============================] - 0s 61us/step - loss: 23.6708 - f1: 1.0000 - val_loss: 26.6092 - val_f1: 0.5000\n",
            "Epoch 11/1000\n",
            "108/108 [==============================] - 0s 61us/step - loss: 22.1199 - f1: 1.0000 - val_loss: 25.2048 - val_f1: 0.5000\n",
            "Epoch 12/1000\n",
            "108/108 [==============================] - 0s 60us/step - loss: 20.6365 - f1: 1.0000 - val_loss: 23.8624 - val_f1: 0.5000\n",
            "Epoch 13/1000\n",
            "108/108 [==============================] - 0s 62us/step - loss: 19.3959 - f1: 1.0000 - val_loss: 22.5910 - val_f1: 0.5000\n",
            "Epoch 14/1000\n",
            "108/108 [==============================] - 0s 59us/step - loss: 18.3147 - f1: 1.0000 - val_loss: 21.4204 - val_f1: 0.5000\n",
            "Epoch 15/1000\n",
            "108/108 [==============================] - 0s 61us/step - loss: 17.3301 - f1: 1.0000 - val_loss: 20.4084 - val_f1: 0.5000\n",
            "Epoch 16/1000\n",
            "108/108 [==============================] - 0s 59us/step - loss: 16.4204 - f1: 1.0000 - val_loss: 19.5237 - val_f1: 0.5000\n",
            "Epoch 17/1000\n",
            "108/108 [==============================] - 0s 60us/step - loss: 15.5560 - f1: 1.0000 - val_loss: 18.6780 - val_f1: 0.5000\n",
            "Epoch 18/1000\n",
            "108/108 [==============================] - 0s 63us/step - loss: 14.8223 - f1: 1.0000 - val_loss: 17.8269 - val_f1: 0.5000\n",
            "Epoch 19/1000\n",
            "108/108 [==============================] - 0s 61us/step - loss: 14.0573 - f1: 1.0000 - val_loss: 16.9979 - val_f1: 0.5000\n",
            "Epoch 20/1000\n",
            "108/108 [==============================] - 0s 59us/step - loss: 13.2807 - f1: 1.0000 - val_loss: 16.1850 - val_f1: 0.5000\n",
            "Epoch 21/1000\n",
            "108/108 [==============================] - 0s 60us/step - loss: 12.6329 - f1: 1.0000 - val_loss: 15.3826 - val_f1: 0.5000\n",
            "Epoch 22/1000\n",
            "108/108 [==============================] - 0s 60us/step - loss: 11.9559 - f1: 0.9750 - val_loss: 14.6605 - val_f1: 0.5000\n",
            "Epoch 23/1000\n",
            "108/108 [==============================] - 0s 62us/step - loss: 11.4842 - f1: 0.9833 - val_loss: 13.9673 - val_f1: 0.5000\n",
            "Epoch 24/1000\n",
            "108/108 [==============================] - 0s 64us/step - loss: 10.9769 - f1: 0.9868 - val_loss: 13.3455 - val_f1: 0.4375\n",
            "Epoch 25/1000\n",
            "108/108 [==============================] - 0s 59us/step - loss: 10.5680 - f1: 0.9853 - val_loss: 12.7717 - val_f1: 0.4375\n",
            "Epoch 26/1000\n",
            "108/108 [==============================] - 0s 61us/step - loss: 10.2175 - f1: 0.9853 - val_loss: 12.2474 - val_f1: 0.4500\n",
            "Epoch 27/1000\n",
            "108/108 [==============================] - 0s 58us/step - loss: 9.8915 - f1: 0.9875 - val_loss: 11.7832 - val_f1: 0.4500\n",
            "Epoch 28/1000\n",
            "108/108 [==============================] - 0s 61us/step - loss: 9.6202 - f1: 0.9833 - val_loss: 11.3372 - val_f1: 0.4500\n",
            "Epoch 29/1000\n",
            "108/108 [==============================] - 0s 59us/step - loss: 9.2887 - f1: 0.9375 - val_loss: 10.9415 - val_f1: 0.4500\n",
            "Epoch 30/1000\n",
            "108/108 [==============================] - 0s 61us/step - loss: 9.0862 - f1: 0.9722 - val_loss: 10.5516 - val_f1: 0.4545\n",
            "Epoch 31/1000\n",
            "108/108 [==============================] - 0s 59us/step - loss: 8.8521 - f1: 0.9393 - val_loss: 10.1970 - val_f1: 0.4545\n",
            "Epoch 32/1000\n",
            "108/108 [==============================] - 0s 59us/step - loss: 8.6183 - f1: 0.9591 - val_loss: 9.8702 - val_f1: 0.4545\n",
            "Epoch 33/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 8.4533 - f1: 0.9556 - val_loss: 9.5540 - val_f1: 0.4583\n",
            "Epoch 34/1000\n",
            "108/108 [==============================] - 0s 59us/step - loss: 8.2778 - f1: 0.9518 - val_loss: 9.2781 - val_f1: 0.4615\n",
            "Epoch 35/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 8.1266 - f1: 0.9630 - val_loss: 9.0345 - val_f1: 0.4615\n",
            "Epoch 36/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 8.0009 - f1: 0.9606 - val_loss: 8.7929 - val_f1: 0.4615\n",
            "Epoch 37/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 7.8333 - f1: 0.9470 - val_loss: 8.5765 - val_f1: 0.4615\n",
            "Epoch 38/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 7.7140 - f1: 0.9700 - val_loss: 8.3447 - val_f1: 0.4615\n",
            "Epoch 39/1000\n",
            "108/108 [==============================] - 0s 60us/step - loss: 7.5698 - f1: 0.9660 - val_loss: 8.1128 - val_f1: 0.4615\n",
            "Epoch 40/1000\n",
            "108/108 [==============================] - 0s 59us/step - loss: 7.4815 - f1: 0.9466 - val_loss: 7.8687 - val_f1: 0.4615\n",
            "Epoch 41/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 7.3546 - f1: 0.9395 - val_loss: 7.6659 - val_f1: 0.4643\n",
            "Epoch 42/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 7.2719 - f1: 0.9306 - val_loss: 7.4914 - val_f1: 0.4643\n",
            "Epoch 43/1000\n",
            "108/108 [==============================] - 0s 58us/step - loss: 7.2118 - f1: 0.9022 - val_loss: 7.3519 - val_f1: 0.4643\n",
            "Epoch 44/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 7.1779 - f1: 0.9105 - val_loss: 7.2325 - val_f1: 0.4643\n",
            "Epoch 45/1000\n",
            "108/108 [==============================] - 0s 60us/step - loss: 7.1343 - f1: 0.9376 - val_loss: 7.1468 - val_f1: 0.4643\n",
            "Epoch 46/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 7.1006 - f1: 0.9365 - val_loss: 7.0720 - val_f1: 0.4643\n",
            "Epoch 47/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 7.0722 - f1: 0.9402 - val_loss: 7.0054 - val_f1: 0.4643\n",
            "Epoch 48/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 7.0593 - f1: 0.9125 - val_loss: 6.9383 - val_f1: 0.4643\n",
            "Epoch 49/1000\n",
            "108/108 [==============================] - 0s 60us/step - loss: 7.0306 - f1: 0.9287 - val_loss: 6.8889 - val_f1: 0.4643\n",
            "Epoch 50/1000\n",
            "108/108 [==============================] - 0s 58us/step - loss: 7.0124 - f1: 0.8925 - val_loss: 6.8394 - val_f1: 0.4667\n",
            "Epoch 51/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 7.0033 - f1: 0.9064 - val_loss: 6.7875 - val_f1: 0.4667\n",
            "Epoch 52/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 6.9799 - f1: 0.9295 - val_loss: 6.7490 - val_f1: 0.4667\n",
            "Epoch 53/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 6.9636 - f1: 0.9017 - val_loss: 6.7097 - val_f1: 0.4667\n",
            "Epoch 54/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 6.9455 - f1: 0.8988 - val_loss: 6.6697 - val_f1: 0.4667\n",
            "Epoch 55/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 6.9284 - f1: 0.9287 - val_loss: 6.6252 - val_f1: 0.4667\n",
            "Epoch 56/1000\n",
            "108/108 [==============================] - 0s 58us/step - loss: 6.9178 - f1: 0.9228 - val_loss: 6.5835 - val_f1: 0.4667\n",
            "Epoch 57/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 6.9042 - f1: 0.9268 - val_loss: 6.5490 - val_f1: 0.4667\n",
            "Epoch 58/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 6.8960 - f1: 0.9115 - val_loss: 6.5197 - val_f1: 0.4688\n",
            "Epoch 59/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 6.8818 - f1: 0.8563 - val_loss: 6.5012 - val_f1: 0.4688\n",
            "Epoch 60/1000\n",
            "108/108 [==============================] - 0s 58us/step - loss: 6.8735 - f1: 0.9162 - val_loss: 6.4891 - val_f1: 0.4688\n",
            "Epoch 61/1000\n",
            "108/108 [==============================] - 0s 59us/step - loss: 6.8651 - f1: 0.8664 - val_loss: 6.4764 - val_f1: 0.4688\n",
            "Epoch 62/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 6.8613 - f1: 0.8775 - val_loss: 6.4632 - val_f1: 0.4688\n",
            "Epoch 63/1000\n",
            "108/108 [==============================] - 0s 60us/step - loss: 6.8503 - f1: 0.9167 - val_loss: 6.4584 - val_f1: 0.4688\n",
            "Epoch 64/1000\n",
            "108/108 [==============================] - 0s 60us/step - loss: 6.8408 - f1: 0.8839 - val_loss: 6.4404 - val_f1: 0.4688\n",
            "Epoch 65/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 6.8312 - f1: 0.9013 - val_loss: 6.4236 - val_f1: 0.4688\n",
            "Epoch 66/1000\n",
            "108/108 [==============================] - 0s 60us/step - loss: 6.8241 - f1: 0.8930 - val_loss: 6.4052 - val_f1: 0.4688\n",
            "Epoch 67/1000\n",
            "108/108 [==============================] - 0s 58us/step - loss: 6.8151 - f1: 0.8969 - val_loss: 6.3907 - val_f1: 0.4688\n",
            "Epoch 68/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 6.8075 - f1: 0.8961 - val_loss: 6.3789 - val_f1: 0.4688\n",
            "Epoch 69/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 6.8026 - f1: 0.8910 - val_loss: 6.3657 - val_f1: 0.4688\n",
            "Epoch 70/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 6.7932 - f1: 0.8989 - val_loss: 6.3562 - val_f1: 0.4688\n",
            "Epoch 71/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 6.7862 - f1: 0.8575 - val_loss: 6.3478 - val_f1: 0.4688\n",
            "Epoch 72/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 6.7810 - f1: 0.8949 - val_loss: 6.3356 - val_f1: 0.4688\n",
            "Epoch 73/1000\n",
            "108/108 [==============================] - 0s 59us/step - loss: 6.7724 - f1: 0.8801 - val_loss: 6.3273 - val_f1: 0.4688\n",
            "Epoch 74/1000\n",
            "108/108 [==============================] - 0s 58us/step - loss: 6.7649 - f1: 0.9154 - val_loss: 6.3236 - val_f1: 0.4688\n",
            "Epoch 75/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 6.7574 - f1: 0.8984 - val_loss: 6.3211 - val_f1: 0.4688\n",
            "Epoch 76/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 6.7521 - f1: 0.8885 - val_loss: 6.3139 - val_f1: 0.4688\n",
            "Epoch 77/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 6.7452 - f1: 0.8993 - val_loss: 6.3091 - val_f1: 0.4688\n",
            "Epoch 78/1000\n",
            "108/108 [==============================] - 0s 58us/step - loss: 6.7379 - f1: 0.9178 - val_loss: 6.3014 - val_f1: 0.4688\n",
            "Epoch 79/1000\n",
            "108/108 [==============================] - 0s 64us/step - loss: 6.7298 - f1: 0.9030 - val_loss: 6.2925 - val_f1: 0.4688\n",
            "Epoch 80/1000\n",
            "108/108 [==============================] - 0s 58us/step - loss: 6.7233 - f1: 0.8841 - val_loss: 6.2800 - val_f1: 0.4688\n",
            "Epoch 81/1000\n",
            "108/108 [==============================] - 0s 65us/step - loss: 6.7159 - f1: 0.9052 - val_loss: 6.2740 - val_f1: 0.4688\n",
            "Epoch 82/1000\n",
            "108/108 [==============================] - 0s 60us/step - loss: 6.7085 - f1: 0.9155 - val_loss: 6.2724 - val_f1: 0.4688\n",
            "Epoch 83/1000\n",
            "108/108 [==============================] - 0s 58us/step - loss: 6.7006 - f1: 0.9151 - val_loss: 6.2582 - val_f1: 0.4688\n",
            "Epoch 84/1000\n",
            "108/108 [==============================] - 0s 61us/step - loss: 6.6958 - f1: 0.8828 - val_loss: 6.2390 - val_f1: 0.4688\n",
            "Epoch 85/1000\n",
            "108/108 [==============================] - 0s 59us/step - loss: 6.6851 - f1: 0.9229 - val_loss: 6.2331 - val_f1: 0.4688\n",
            "Epoch 86/1000\n",
            "108/108 [==============================] - 0s 60us/step - loss: 6.6774 - f1: 0.8791 - val_loss: 6.2301 - val_f1: 0.4688\n",
            "Epoch 87/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 6.6710 - f1: 0.9005 - val_loss: 6.2348 - val_f1: 0.4688\n",
            "Epoch 88/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 6.6627 - f1: 0.8694 - val_loss: 6.2367 - val_f1: 0.4688\n",
            "Epoch 89/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 6.6561 - f1: 0.8867 - val_loss: 6.2429 - val_f1: 0.4688\n",
            "Epoch 90/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 6.6506 - f1: 0.9009 - val_loss: 6.2537 - val_f1: 0.4688\n",
            "Epoch 91/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 6.6468 - f1: 0.9193 - val_loss: 6.2606 - val_f1: 0.4688\n",
            "Epoch 92/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 6.6405 - f1: 0.8927 - val_loss: 6.2600 - val_f1: 0.4688\n",
            "Epoch 93/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 6.6399 - f1: 0.9042 - val_loss: 6.2429 - val_f1: 0.4688\n",
            "Epoch 94/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 6.6270 - f1: 0.9103 - val_loss: 6.2409 - val_f1: 0.4688\n",
            "Epoch 95/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 6.6189 - f1: 0.9117 - val_loss: 6.2316 - val_f1: 0.4688\n",
            "Epoch 96/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 6.6128 - f1: 0.9122 - val_loss: 6.2242 - val_f1: 0.4688\n",
            "Epoch 97/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 6.6064 - f1: 0.8812 - val_loss: 6.2129 - val_f1: 0.4688\n",
            "Epoch 98/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 6.5987 - f1: 0.8979 - val_loss: 6.2104 - val_f1: 0.4688\n",
            "Epoch 99/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 6.5929 - f1: 0.8805 - val_loss: 6.2046 - val_f1: 0.4688\n",
            "Epoch 100/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 6.5861 - f1: 0.9148 - val_loss: 6.2091 - val_f1: 0.4688\n",
            "Epoch 101/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 6.5801 - f1: 0.8990 - val_loss: 6.2073 - val_f1: 0.4688\n",
            "Epoch 102/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 6.5719 - f1: 0.8954 - val_loss: 6.1975 - val_f1: 0.4688\n",
            "Epoch 103/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 6.5649 - f1: 0.9211 - val_loss: 6.1808 - val_f1: 0.4688\n",
            "Epoch 104/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 6.5567 - f1: 0.8996 - val_loss: 6.1571 - val_f1: 0.4688\n",
            "Epoch 105/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 6.5495 - f1: 0.9218 - val_loss: 6.1396 - val_f1: 0.4688\n",
            "Epoch 106/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 6.5388 - f1: 0.9198 - val_loss: 6.1265 - val_f1: 0.4688\n",
            "Epoch 107/1000\n",
            "108/108 [==============================] - 0s 59us/step - loss: 6.5331 - f1: 0.9054 - val_loss: 6.1058 - val_f1: 0.4688\n",
            "Epoch 108/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 6.5236 - f1: 0.8983 - val_loss: 6.0935 - val_f1: 0.4688\n",
            "Epoch 109/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 6.5168 - f1: 0.9202 - val_loss: 6.0811 - val_f1: 0.4688\n",
            "Epoch 110/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 6.5111 - f1: 0.8694 - val_loss: 6.0634 - val_f1: 0.4688\n",
            "Epoch 111/1000\n",
            "108/108 [==============================] - 0s 58us/step - loss: 6.5021 - f1: 0.9030 - val_loss: 6.0599 - val_f1: 0.4688\n",
            "Epoch 112/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 6.4979 - f1: 0.9201 - val_loss: 6.0603 - val_f1: 0.4688\n",
            "Epoch 113/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 6.4852 - f1: 0.8988 - val_loss: 6.0430 - val_f1: 0.4688\n",
            "Epoch 114/1000\n",
            "108/108 [==============================] - 0s 58us/step - loss: 6.4805 - f1: 0.9018 - val_loss: 6.0295 - val_f1: 0.4688\n",
            "Epoch 115/1000\n",
            "108/108 [==============================] - 0s 60us/step - loss: 6.4713 - f1: 0.9030 - val_loss: 6.0258 - val_f1: 0.4688\n",
            "Epoch 116/1000\n",
            "108/108 [==============================] - 0s 60us/step - loss: 6.4661 - f1: 0.8969 - val_loss: 6.0201 - val_f1: 0.4688\n",
            "Epoch 117/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 6.4589 - f1: 0.9165 - val_loss: 6.0210 - val_f1: 0.4688\n",
            "Epoch 118/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 6.4496 - f1: 0.9001 - val_loss: 6.0119 - val_f1: 0.4688\n",
            "Epoch 119/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 6.4428 - f1: 0.9190 - val_loss: 6.0053 - val_f1: 0.4688\n",
            "Epoch 120/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 6.4343 - f1: 0.9200 - val_loss: 6.0016 - val_f1: 0.4688\n",
            "Epoch 121/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 6.4271 - f1: 0.9174 - val_loss: 5.9827 - val_f1: 0.4688\n",
            "Epoch 122/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 6.4170 - f1: 0.9128 - val_loss: 5.9708 - val_f1: 0.4688\n",
            "Epoch 123/1000\n",
            "108/108 [==============================] - 0s 63us/step - loss: 6.4071 - f1: 0.9049 - val_loss: 5.9618 - val_f1: 0.4688\n",
            "Epoch 124/1000\n",
            "108/108 [==============================] - 0s 66us/step - loss: 6.3995 - f1: 0.8826 - val_loss: 5.9524 - val_f1: 0.4688\n",
            "Epoch 125/1000\n",
            "108/108 [==============================] - 0s 64us/step - loss: 6.3921 - f1: 0.9202 - val_loss: 5.9457 - val_f1: 0.4688\n",
            "Epoch 126/1000\n",
            "108/108 [==============================] - 0s 58us/step - loss: 6.3856 - f1: 0.9172 - val_loss: 5.9413 - val_f1: 0.4688\n",
            "Epoch 127/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 6.3777 - f1: 0.9157 - val_loss: 5.9350 - val_f1: 0.4688\n",
            "Epoch 128/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 6.3679 - f1: 0.8880 - val_loss: 5.9173 - val_f1: 0.4688\n",
            "Epoch 129/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 6.3601 - f1: 0.8824 - val_loss: 5.9036 - val_f1: 0.4688\n",
            "Epoch 130/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 6.3519 - f1: 0.9165 - val_loss: 5.8903 - val_f1: 0.4688\n",
            "Epoch 131/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 6.3438 - f1: 0.9140 - val_loss: 5.8818 - val_f1: 0.4688\n",
            "Epoch 132/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 6.3361 - f1: 0.8848 - val_loss: 5.8749 - val_f1: 0.4688\n",
            "Epoch 133/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 6.3278 - f1: 0.9001 - val_loss: 5.8702 - val_f1: 0.4688\n",
            "Epoch 134/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 6.3188 - f1: 0.8875 - val_loss: 5.8676 - val_f1: 0.4688\n",
            "Epoch 135/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 6.3132 - f1: 0.9139 - val_loss: 5.8709 - val_f1: 0.4688\n",
            "Epoch 136/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 6.3030 - f1: 0.9194 - val_loss: 5.8674 - val_f1: 0.4688\n",
            "Epoch 137/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 6.2960 - f1: 0.9205 - val_loss: 5.8608 - val_f1: 0.4688\n",
            "Epoch 138/1000\n",
            "108/108 [==============================] - 0s 59us/step - loss: 6.2888 - f1: 0.9022 - val_loss: 5.8559 - val_f1: 0.4688\n",
            "Epoch 139/1000\n",
            "108/108 [==============================] - 0s 60us/step - loss: 6.2812 - f1: 0.8864 - val_loss: 5.8585 - val_f1: 0.4688\n",
            "Epoch 140/1000\n",
            "108/108 [==============================] - 0s 61us/step - loss: 6.2747 - f1: 0.9129 - val_loss: 5.8597 - val_f1: 0.4688\n",
            "Epoch 141/1000\n",
            "108/108 [==============================] - 0s 59us/step - loss: 6.2654 - f1: 0.9006 - val_loss: 5.8540 - val_f1: 0.4688\n",
            "Epoch 142/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 6.2590 - f1: 0.8849 - val_loss: 5.8504 - val_f1: 0.4688\n",
            "Epoch 143/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 6.2506 - f1: 0.9173 - val_loss: 5.8486 - val_f1: 0.4688\n",
            "Epoch 144/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 6.2440 - f1: 0.9207 - val_loss: 5.8446 - val_f1: 0.4688\n",
            "Epoch 145/1000\n",
            "108/108 [==============================] - 0s 61us/step - loss: 6.2366 - f1: 0.8961 - val_loss: 5.8344 - val_f1: 0.4688\n",
            "Epoch 146/1000\n",
            "108/108 [==============================] - 0s 61us/step - loss: 6.2291 - f1: 0.9108 - val_loss: 5.8320 - val_f1: 0.4688\n",
            "Epoch 147/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 6.2233 - f1: 0.8971 - val_loss: 5.8259 - val_f1: 0.4688\n",
            "Epoch 148/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 6.2127 - f1: 0.9012 - val_loss: 5.8257 - val_f1: 0.4688\n",
            "Epoch 149/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 6.2059 - f1: 0.8613 - val_loss: 5.8249 - val_f1: 0.4688\n",
            "Epoch 150/1000\n",
            "108/108 [==============================] - 0s 59us/step - loss: 6.2002 - f1: 0.9100 - val_loss: 5.8306 - val_f1: 0.4688\n",
            "Epoch 151/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 6.1921 - f1: 0.9149 - val_loss: 5.8218 - val_f1: 0.4688\n",
            "Epoch 152/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 6.1817 - f1: 0.9122 - val_loss: 5.8096 - val_f1: 0.4688\n",
            "Epoch 153/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 6.1736 - f1: 0.9156 - val_loss: 5.8016 - val_f1: 0.4688\n",
            "Epoch 154/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 6.1674 - f1: 0.9052 - val_loss: 5.7847 - val_f1: 0.4688\n",
            "Epoch 155/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 6.1590 - f1: 0.8999 - val_loss: 5.7724 - val_f1: 0.4688\n",
            "Epoch 156/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 6.1514 - f1: 0.9034 - val_loss: 5.7621 - val_f1: 0.4688\n",
            "Epoch 157/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 6.1430 - f1: 0.8981 - val_loss: 5.7528 - val_f1: 0.4688\n",
            "Epoch 158/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 6.1345 - f1: 0.9212 - val_loss: 5.7426 - val_f1: 0.4688\n",
            "Epoch 159/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 6.1277 - f1: 0.9180 - val_loss: 5.7314 - val_f1: 0.4688\n",
            "Epoch 160/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 6.1199 - f1: 0.8790 - val_loss: 5.7219 - val_f1: 0.4688\n",
            "Epoch 161/1000\n",
            "108/108 [==============================] - 0s 50us/step - loss: 6.1113 - f1: 0.8911 - val_loss: 5.7278 - val_f1: 0.4688\n",
            "Epoch 162/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 6.1035 - f1: 0.8879 - val_loss: 5.7351 - val_f1: 0.4688\n",
            "Epoch 163/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 6.0922 - f1: 0.9037 - val_loss: 5.7460 - val_f1: 0.4688\n",
            "Epoch 164/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 6.0881 - f1: 0.9206 - val_loss: 5.7516 - val_f1: 0.4688\n",
            "Epoch 165/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 6.0800 - f1: 0.9220 - val_loss: 5.7466 - val_f1: 0.4688\n",
            "Epoch 166/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 6.0761 - f1: 0.9079 - val_loss: 5.7321 - val_f1: 0.4688\n",
            "Epoch 167/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 6.0662 - f1: 0.9068 - val_loss: 5.7329 - val_f1: 0.4688\n",
            "Epoch 168/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 6.0573 - f1: 0.9072 - val_loss: 5.7325 - val_f1: 0.4688\n",
            "Epoch 169/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 6.0515 - f1: 0.9247 - val_loss: 5.7263 - val_f1: 0.4688\n",
            "Epoch 170/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 6.0431 - f1: 0.9311 - val_loss: 5.7154 - val_f1: 0.4688\n",
            "Epoch 171/1000\n",
            "108/108 [==============================] - 0s 50us/step - loss: 6.0366 - f1: 0.8797 - val_loss: 5.6910 - val_f1: 0.4688\n",
            "Epoch 172/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 6.0271 - f1: 0.9214 - val_loss: 5.6797 - val_f1: 0.4688\n",
            "Epoch 173/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 6.0191 - f1: 0.9023 - val_loss: 5.6664 - val_f1: 0.4688\n",
            "Epoch 174/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 6.0109 - f1: 0.9006 - val_loss: 5.6585 - val_f1: 0.4688\n",
            "Epoch 175/1000\n",
            "108/108 [==============================] - 0s 50us/step - loss: 6.0022 - f1: 0.9161 - val_loss: 5.6522 - val_f1: 0.4688\n",
            "Epoch 176/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 5.9955 - f1: 0.9012 - val_loss: 5.6430 - val_f1: 0.4688\n",
            "Epoch 177/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 5.9878 - f1: 0.8870 - val_loss: 5.6361 - val_f1: 0.4688\n",
            "Epoch 178/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 5.9802 - f1: 0.8851 - val_loss: 5.6322 - val_f1: 0.4688\n",
            "Epoch 179/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 5.9734 - f1: 0.9035 - val_loss: 5.6280 - val_f1: 0.4688\n",
            "Epoch 180/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 5.9682 - f1: 0.9320 - val_loss: 5.6284 - val_f1: 0.4688\n",
            "Epoch 181/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 5.9601 - f1: 0.9274 - val_loss: 5.6258 - val_f1: 0.4688\n",
            "Epoch 182/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 5.9525 - f1: 0.9068 - val_loss: 5.6114 - val_f1: 0.4688\n",
            "Epoch 183/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 5.9461 - f1: 0.9194 - val_loss: 5.6023 - val_f1: 0.4688\n",
            "Epoch 184/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 5.9382 - f1: 0.9271 - val_loss: 5.5987 - val_f1: 0.4688\n",
            "Epoch 185/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 5.9321 - f1: 0.9130 - val_loss: 5.5908 - val_f1: 0.4688\n",
            "Epoch 186/1000\n",
            "108/108 [==============================] - 0s 50us/step - loss: 5.9255 - f1: 0.9073 - val_loss: 5.5802 - val_f1: 0.4688\n",
            "Epoch 187/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 5.9169 - f1: 0.9051 - val_loss: 5.5760 - val_f1: 0.4688\n",
            "Epoch 188/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 5.9109 - f1: 0.9058 - val_loss: 5.5790 - val_f1: 0.4688\n",
            "Epoch 189/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 5.9051 - f1: 0.9179 - val_loss: 5.5666 - val_f1: 0.4688\n",
            "Epoch 190/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 5.8955 - f1: 0.9100 - val_loss: 5.5697 - val_f1: 0.4688\n",
            "Epoch 191/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 5.8875 - f1: 0.9109 - val_loss: 5.5730 - val_f1: 0.4688\n",
            "Epoch 192/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 5.8830 - f1: 0.9211 - val_loss: 5.5762 - val_f1: 0.4688\n",
            "Epoch 193/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 5.8737 - f1: 0.9103 - val_loss: 5.5733 - val_f1: 0.4688\n",
            "Epoch 194/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 5.8687 - f1: 0.9084 - val_loss: 5.5655 - val_f1: 0.4688\n",
            "Epoch 195/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 5.8604 - f1: 0.8899 - val_loss: 5.5578 - val_f1: 0.4688\n",
            "Epoch 196/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 5.8546 - f1: 0.9015 - val_loss: 5.5553 - val_f1: 0.4688\n",
            "Epoch 197/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 5.8470 - f1: 0.9289 - val_loss: 5.5520 - val_f1: 0.4688\n",
            "Epoch 198/1000\n",
            "108/108 [==============================] - 0s 58us/step - loss: 5.8408 - f1: 0.9307 - val_loss: 5.5474 - val_f1: 0.4688\n",
            "Epoch 199/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 5.8373 - f1: 0.8839 - val_loss: 5.5273 - val_f1: 0.4688\n",
            "Epoch 200/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 5.8262 - f1: 0.9126 - val_loss: 5.5192 - val_f1: 0.4688\n",
            "Epoch 201/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 5.8195 - f1: 0.9111 - val_loss: 5.5115 - val_f1: 0.4688\n",
            "Epoch 202/1000\n",
            "108/108 [==============================] - 0s 59us/step - loss: 5.8087 - f1: 0.8679 - val_loss: 5.5140 - val_f1: 0.4688\n",
            "Epoch 203/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 5.8028 - f1: 0.9093 - val_loss: 5.5218 - val_f1: 0.4688\n",
            "Epoch 204/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 5.7961 - f1: 0.9278 - val_loss: 5.5270 - val_f1: 0.4688\n",
            "Epoch 205/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 5.7919 - f1: 0.9408 - val_loss: 5.5232 - val_f1: 0.4688\n",
            "Epoch 206/1000\n",
            "108/108 [==============================] - 0s 58us/step - loss: 5.7857 - f1: 0.9370 - val_loss: 5.5197 - val_f1: 0.4688\n",
            "Epoch 207/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 5.7797 - f1: 0.9230 - val_loss: 5.4966 - val_f1: 0.4688\n",
            "Epoch 208/1000\n",
            "108/108 [==============================] - 0s 58us/step - loss: 5.7707 - f1: 0.9088 - val_loss: 5.4755 - val_f1: 0.4688\n",
            "Epoch 209/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 5.7611 - f1: 0.9287 - val_loss: 5.4721 - val_f1: 0.4688\n",
            "Epoch 210/1000\n",
            "108/108 [==============================] - 0s 58us/step - loss: 5.7527 - f1: 0.9191 - val_loss: 5.4594 - val_f1: 0.4688\n",
            "Epoch 211/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 5.7456 - f1: 0.9026 - val_loss: 5.4509 - val_f1: 0.4688\n",
            "Epoch 212/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 5.7407 - f1: 0.9375 - val_loss: 5.4586 - val_f1: 0.4688\n",
            "Epoch 213/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 5.7329 - f1: 0.9249 - val_loss: 5.4514 - val_f1: 0.4688\n",
            "Epoch 214/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 5.7266 - f1: 0.9239 - val_loss: 5.4484 - val_f1: 0.4688\n",
            "Epoch 215/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 5.7197 - f1: 0.9165 - val_loss: 5.4432 - val_f1: 0.4688\n",
            "Epoch 216/1000\n",
            "108/108 [==============================] - 0s 58us/step - loss: 5.7139 - f1: 0.9355 - val_loss: 5.4402 - val_f1: 0.4688\n",
            "Epoch 217/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 5.7044 - f1: 0.9381 - val_loss: 5.4161 - val_f1: 0.4688\n",
            "Epoch 218/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 5.6970 - f1: 0.9287 - val_loss: 5.3981 - val_f1: 0.4688\n",
            "Epoch 219/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 5.6943 - f1: 0.9338 - val_loss: 5.3814 - val_f1: 0.4688\n",
            "Epoch 220/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 5.6822 - f1: 0.9317 - val_loss: 5.3746 - val_f1: 0.4688\n",
            "Epoch 221/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 5.6757 - f1: 0.9282 - val_loss: 5.3686 - val_f1: 0.4688\n",
            "Epoch 222/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 5.6702 - f1: 0.9049 - val_loss: 5.3589 - val_f1: 0.4688\n",
            "Epoch 223/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 5.6630 - f1: 0.9434 - val_loss: 5.3644 - val_f1: 0.4688\n",
            "Epoch 224/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 5.6572 - f1: 0.9356 - val_loss: 5.3568 - val_f1: 0.4688\n",
            "Epoch 225/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 5.6486 - f1: 0.9386 - val_loss: 5.3450 - val_f1: 0.4688\n",
            "Epoch 226/1000\n",
            "108/108 [==============================] - 0s 59us/step - loss: 5.6444 - f1: 0.8975 - val_loss: 5.3199 - val_f1: 0.4688\n",
            "Epoch 227/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 5.6375 - f1: 0.8749 - val_loss: 5.3109 - val_f1: 0.4688\n",
            "Epoch 228/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 5.6264 - f1: 0.9263 - val_loss: 5.3238 - val_f1: 0.4688\n",
            "Epoch 229/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 5.6195 - f1: 0.9226 - val_loss: 5.3242 - val_f1: 0.4688\n",
            "Epoch 230/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 5.6130 - f1: 0.9240 - val_loss: 5.3322 - val_f1: 0.4688\n",
            "Epoch 231/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 5.6065 - f1: 0.8542 - val_loss: 5.3385 - val_f1: 0.4688\n",
            "Epoch 232/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 5.6056 - f1: 0.9233 - val_loss: 5.3657 - val_f1: 0.4688\n",
            "Epoch 233/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 5.5996 - f1: 0.9420 - val_loss: 5.3808 - val_f1: 0.4688\n",
            "Epoch 234/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 5.5965 - f1: 0.9420 - val_loss: 5.3734 - val_f1: 0.4688\n",
            "Epoch 235/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 5.5908 - f1: 0.9432 - val_loss: 5.3466 - val_f1: 0.4688\n",
            "Epoch 236/1000\n",
            "108/108 [==============================] - 0s 59us/step - loss: 5.5766 - f1: 0.9187 - val_loss: 5.3236 - val_f1: 0.4688\n",
            "Epoch 237/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 5.5689 - f1: 0.9005 - val_loss: 5.2990 - val_f1: 0.4688\n",
            "Epoch 238/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 5.5596 - f1: 0.9182 - val_loss: 5.2827 - val_f1: 0.4688\n",
            "Epoch 239/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 5.5536 - f1: 0.9229 - val_loss: 5.2677 - val_f1: 0.4688\n",
            "Epoch 240/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 5.5449 - f1: 0.9201 - val_loss: 5.2592 - val_f1: 0.4688\n",
            "Epoch 241/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 5.5373 - f1: 0.9013 - val_loss: 5.2480 - val_f1: 0.4688\n",
            "Epoch 242/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 5.5294 - f1: 0.9095 - val_loss: 5.2456 - val_f1: 0.4688\n",
            "Epoch 243/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 5.5238 - f1: 0.9256 - val_loss: 5.2553 - val_f1: 0.4688\n",
            "Epoch 244/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 5.5233 - f1: 0.9451 - val_loss: 5.2650 - val_f1: 0.4688\n",
            "Epoch 245/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 5.5124 - f1: 0.9426 - val_loss: 5.2528 - val_f1: 0.4688\n",
            "Epoch 246/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 5.5027 - f1: 0.9274 - val_loss: 5.2323 - val_f1: 0.4688\n",
            "Epoch 247/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 5.4937 - f1: 0.9405 - val_loss: 5.2162 - val_f1: 0.4688\n",
            "Epoch 248/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 5.4845 - f1: 0.9230 - val_loss: 5.2032 - val_f1: 0.4688\n",
            "Epoch 249/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 5.4804 - f1: 0.9133 - val_loss: 5.1918 - val_f1: 0.4688\n",
            "Epoch 250/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 5.4718 - f1: 0.9249 - val_loss: 5.1971 - val_f1: 0.4688\n",
            "Epoch 251/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 5.4664 - f1: 0.9416 - val_loss: 5.1887 - val_f1: 0.4688\n",
            "Epoch 252/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 5.4569 - f1: 0.9421 - val_loss: 5.1720 - val_f1: 0.4688\n",
            "Epoch 253/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 5.4483 - f1: 0.9256 - val_loss: 5.1585 - val_f1: 0.4688\n",
            "Epoch 254/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 5.4401 - f1: 0.9383 - val_loss: 5.1478 - val_f1: 0.4688\n",
            "Epoch 255/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 5.4365 - f1: 0.9242 - val_loss: 5.1332 - val_f1: 0.4688\n",
            "Epoch 256/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 5.4271 - f1: 0.9225 - val_loss: 5.1295 - val_f1: 0.4688\n",
            "Epoch 257/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 5.4200 - f1: 0.9228 - val_loss: 5.1295 - val_f1: 0.4688\n",
            "Epoch 258/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 5.4148 - f1: 0.9396 - val_loss: 5.1258 - val_f1: 0.4688\n",
            "Epoch 259/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 5.4065 - f1: 0.9239 - val_loss: 5.1043 - val_f1: 0.4688\n",
            "Epoch 260/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 5.3987 - f1: 0.9056 - val_loss: 5.0818 - val_f1: 0.4688\n",
            "Epoch 261/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 5.3905 - f1: 0.9447 - val_loss: 5.0667 - val_f1: 0.4688\n",
            "Epoch 262/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 5.3876 - f1: 0.8945 - val_loss: 5.0434 - val_f1: 0.4688\n",
            "Epoch 263/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 5.3775 - f1: 0.9380 - val_loss: 5.0469 - val_f1: 0.4688\n",
            "Epoch 264/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 5.3715 - f1: 0.9460 - val_loss: 5.0442 - val_f1: 0.4688\n",
            "Epoch 265/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 5.3623 - f1: 0.9414 - val_loss: 5.0400 - val_f1: 0.4688\n",
            "Epoch 266/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 5.3564 - f1: 0.9419 - val_loss: 5.0308 - val_f1: 0.4688\n",
            "Epoch 267/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 5.3488 - f1: 0.9336 - val_loss: 5.0123 - val_f1: 0.4688\n",
            "Epoch 268/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 5.3433 - f1: 0.9262 - val_loss: 5.0005 - val_f1: 0.4688\n",
            "Epoch 269/1000\n",
            "108/108 [==============================] - 0s 64us/step - loss: 5.3363 - f1: 0.9206 - val_loss: 4.9987 - val_f1: 0.4688\n",
            "Epoch 270/1000\n",
            "108/108 [==============================] - 0s 63us/step - loss: 5.3307 - f1: 0.9423 - val_loss: 4.9979 - val_f1: 0.4688\n",
            "Epoch 271/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 5.3208 - f1: 0.9019 - val_loss: 4.9955 - val_f1: 0.4688\n",
            "Epoch 272/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 5.3156 - f1: 0.9211 - val_loss: 5.0012 - val_f1: 0.4688\n",
            "Epoch 273/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 5.3077 - f1: 0.9422 - val_loss: 4.9930 - val_f1: 0.4688\n",
            "Epoch 274/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 5.2980 - f1: 0.9123 - val_loss: 4.9716 - val_f1: 0.4688\n",
            "Epoch 275/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 5.2936 - f1: 0.9211 - val_loss: 4.9513 - val_f1: 0.4688\n",
            "Epoch 276/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 5.2828 - f1: 0.9205 - val_loss: 4.9465 - val_f1: 0.4688\n",
            "Epoch 277/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 5.2778 - f1: 0.9381 - val_loss: 4.9513 - val_f1: 0.4688\n",
            "Epoch 278/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 5.2688 - f1: 0.9462 - val_loss: 4.9451 - val_f1: 0.4688\n",
            "Epoch 279/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 5.2619 - f1: 0.9341 - val_loss: 4.9267 - val_f1: 0.4688\n",
            "Epoch 280/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 5.2523 - f1: 0.9374 - val_loss: 4.9036 - val_f1: 0.4688\n",
            "Epoch 281/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 5.2528 - f1: 0.9125 - val_loss: 4.8790 - val_f1: 0.4688\n",
            "Epoch 282/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 5.2393 - f1: 0.9451 - val_loss: 4.8843 - val_f1: 0.4688\n",
            "Epoch 283/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 5.2309 - f1: 0.9246 - val_loss: 4.8891 - val_f1: 0.4688\n",
            "Epoch 284/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 5.2231 - f1: 0.9431 - val_loss: 4.8922 - val_f1: 0.4688\n",
            "Epoch 285/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 5.2142 - f1: 0.9142 - val_loss: 4.8871 - val_f1: 0.4688\n",
            "Epoch 286/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 5.2072 - f1: 0.8659 - val_loss: 4.8816 - val_f1: 0.4688\n",
            "Epoch 287/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 5.2009 - f1: 0.9187 - val_loss: 4.8744 - val_f1: 0.4688\n",
            "Epoch 288/1000\n",
            "108/108 [==============================] - 0s 59us/step - loss: 5.1911 - f1: 0.9167 - val_loss: 4.8732 - val_f1: 0.4688\n",
            "Epoch 289/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 5.1852 - f1: 0.9443 - val_loss: 4.8791 - val_f1: 0.4688\n",
            "Epoch 290/1000\n",
            "108/108 [==============================] - 0s 58us/step - loss: 5.1789 - f1: 0.9422 - val_loss: 4.8658 - val_f1: 0.4688\n",
            "Epoch 291/1000\n",
            "108/108 [==============================] - 0s 60us/step - loss: 5.1705 - f1: 0.9400 - val_loss: 4.8561 - val_f1: 0.4688\n",
            "Epoch 292/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 5.1629 - f1: 0.9418 - val_loss: 4.8437 - val_f1: 0.4688\n",
            "Epoch 293/1000\n",
            "108/108 [==============================] - 0s 58us/step - loss: 5.1607 - f1: 0.9239 - val_loss: 4.8206 - val_f1: 0.4688\n",
            "Epoch 294/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 5.1480 - f1: 0.9395 - val_loss: 4.8153 - val_f1: 0.4688\n",
            "Epoch 295/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 5.1419 - f1: 0.9414 - val_loss: 4.7996 - val_f1: 0.4688\n",
            "Epoch 296/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 5.1359 - f1: 0.9155 - val_loss: 4.7779 - val_f1: 0.4688\n",
            "Epoch 297/1000\n",
            "108/108 [==============================] - 0s 60us/step - loss: 5.1279 - f1: 0.9044 - val_loss: 4.7677 - val_f1: 0.4688\n",
            "Epoch 298/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 5.1216 - f1: 0.9411 - val_loss: 4.7730 - val_f1: 0.4688\n",
            "Epoch 299/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 5.1136 - f1: 0.9409 - val_loss: 4.7561 - val_f1: 0.4688\n",
            "Epoch 300/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 5.1043 - f1: 0.9229 - val_loss: 4.7409 - val_f1: 0.4688\n",
            "Epoch 301/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 5.0977 - f1: 0.8849 - val_loss: 4.7371 - val_f1: 0.4688\n",
            "Epoch 302/1000\n",
            "108/108 [==============================] - 0s 58us/step - loss: 5.0906 - f1: 0.9449 - val_loss: 4.7463 - val_f1: 0.4688\n",
            "Epoch 303/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 5.0819 - f1: 0.9287 - val_loss: 4.7298 - val_f1: 0.4688\n",
            "Epoch 304/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 5.0716 - f1: 0.9425 - val_loss: 4.7269 - val_f1: 0.4688\n",
            "Epoch 305/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 5.0647 - f1: 0.9414 - val_loss: 4.7196 - val_f1: 0.4688\n",
            "Epoch 306/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 5.0567 - f1: 0.9374 - val_loss: 4.7144 - val_f1: 0.4688\n",
            "Epoch 307/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 5.0491 - f1: 0.9053 - val_loss: 4.6962 - val_f1: 0.4688\n",
            "Epoch 308/1000\n",
            "108/108 [==============================] - 0s 58us/step - loss: 5.0418 - f1: 0.9172 - val_loss: 4.6780 - val_f1: 0.4688\n",
            "Epoch 309/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 5.0342 - f1: 0.8939 - val_loss: 4.6710 - val_f1: 0.4688\n",
            "Epoch 310/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 5.0237 - f1: 0.9372 - val_loss: 4.6771 - val_f1: 0.4688\n",
            "Epoch 311/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 5.0152 - f1: 0.9395 - val_loss: 4.6810 - val_f1: 0.4688\n",
            "Epoch 312/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 5.0094 - f1: 0.9287 - val_loss: 4.6784 - val_f1: 0.4688\n",
            "Epoch 313/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 5.0047 - f1: 0.9345 - val_loss: 4.6695 - val_f1: 0.4688\n",
            "Epoch 314/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 4.9925 - f1: 0.9416 - val_loss: 4.6456 - val_f1: 0.4688\n",
            "Epoch 315/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 4.9892 - f1: 0.9003 - val_loss: 4.6192 - val_f1: 0.4688\n",
            "Epoch 316/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 4.9808 - f1: 0.9393 - val_loss: 4.6099 - val_f1: 0.4688\n",
            "Epoch 317/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 4.9751 - f1: 0.9436 - val_loss: 4.6093 - val_f1: 0.4688\n",
            "Epoch 318/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 4.9656 - f1: 0.9128 - val_loss: 4.6032 - val_f1: 0.4688\n",
            "Epoch 319/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 4.9589 - f1: 0.9400 - val_loss: 4.6020 - val_f1: 0.4688\n",
            "Epoch 320/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 4.9491 - f1: 0.9419 - val_loss: 4.5875 - val_f1: 0.4688\n",
            "Epoch 321/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 4.9404 - f1: 0.9437 - val_loss: 4.5701 - val_f1: 0.4688\n",
            "Epoch 322/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 4.9355 - f1: 0.9244 - val_loss: 4.5506 - val_f1: 0.4688\n",
            "Epoch 323/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 4.9293 - f1: 0.9417 - val_loss: 4.5382 - val_f1: 0.4688\n",
            "Epoch 324/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 4.9203 - f1: 0.9273 - val_loss: 4.5283 - val_f1: 0.4688\n",
            "Epoch 325/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 4.9141 - f1: 0.9397 - val_loss: 4.5241 - val_f1: 0.4688\n",
            "Epoch 326/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 4.9042 - f1: 0.9415 - val_loss: 4.5143 - val_f1: 0.4688\n",
            "Epoch 327/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 4.8978 - f1: 0.9424 - val_loss: 4.4955 - val_f1: 0.4688\n",
            "Epoch 328/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 4.8903 - f1: 0.9399 - val_loss: 4.4845 - val_f1: 0.4706\n",
            "Epoch 329/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 4.8810 - f1: 0.9062 - val_loss: 4.4793 - val_f1: 0.4688\n",
            "Epoch 330/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 4.8736 - f1: 0.9431 - val_loss: 4.4754 - val_f1: 0.4688\n",
            "Epoch 331/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 4.8637 - f1: 0.9386 - val_loss: 4.4702 - val_f1: 0.4688\n",
            "Epoch 332/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 4.8575 - f1: 0.9370 - val_loss: 4.4689 - val_f1: 0.4688\n",
            "Epoch 333/1000\n",
            "108/108 [==============================] - 0s 58us/step - loss: 4.8474 - f1: 0.9375 - val_loss: 4.4619 - val_f1: 0.4688\n",
            "Epoch 334/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 4.8412 - f1: 0.9182 - val_loss: 4.4593 - val_f1: 0.4688\n",
            "Epoch 335/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 4.8328 - f1: 0.9180 - val_loss: 4.4606 - val_f1: 0.4688\n",
            "Epoch 336/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 4.8249 - f1: 0.9356 - val_loss: 4.4511 - val_f1: 0.4688\n",
            "Epoch 337/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 4.8188 - f1: 0.9388 - val_loss: 4.4393 - val_f1: 0.4688\n",
            "Epoch 338/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 4.8101 - f1: 0.9249 - val_loss: 4.4160 - val_f1: 0.4706\n",
            "Epoch 339/1000\n",
            "108/108 [==============================] - 0s 58us/step - loss: 4.8017 - f1: 0.9389 - val_loss: 4.4055 - val_f1: 0.4706\n",
            "Epoch 340/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 4.7942 - f1: 0.9442 - val_loss: 4.3951 - val_f1: 0.4706\n",
            "Epoch 341/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 4.7838 - f1: 0.9436 - val_loss: 4.3812 - val_f1: 0.4706\n",
            "Epoch 342/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 4.7763 - f1: 0.9141 - val_loss: 4.3656 - val_f1: 0.4706\n",
            "Epoch 343/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 4.7712 - f1: 0.9110 - val_loss: 4.3507 - val_f1: 0.4444\n",
            "Epoch 344/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 4.7613 - f1: 0.9327 - val_loss: 4.3456 - val_f1: 0.4706\n",
            "Epoch 345/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 4.7534 - f1: 0.9149 - val_loss: 4.3376 - val_f1: 0.4706\n",
            "Epoch 346/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 4.7432 - f1: 0.9195 - val_loss: 4.3419 - val_f1: 0.4706\n",
            "Epoch 347/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 4.7398 - f1: 0.9449 - val_loss: 4.3529 - val_f1: 0.4706\n",
            "Epoch 348/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 4.7219 - f1: 0.9400 - val_loss: 4.3418 - val_f1: 0.4706\n",
            "Epoch 349/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 4.7130 - f1: 0.9436 - val_loss: 4.3332 - val_f1: 0.4706\n",
            "Epoch 350/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 4.7027 - f1: 0.9460 - val_loss: 4.3118 - val_f1: 0.4706\n",
            "Epoch 351/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 4.7002 - f1: 0.9175 - val_loss: 4.2879 - val_f1: 0.4706\n",
            "Epoch 352/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 4.6886 - f1: 0.9227 - val_loss: 4.2801 - val_f1: 0.4706\n",
            "Epoch 353/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 4.6789 - f1: 0.8947 - val_loss: 4.2800 - val_f1: 0.4706\n",
            "Epoch 354/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 4.6695 - f1: 0.9423 - val_loss: 4.2911 - val_f1: 0.4706\n",
            "Epoch 355/1000\n",
            "108/108 [==============================] - 0s 50us/step - loss: 4.6576 - f1: 0.9022 - val_loss: 4.2943 - val_f1: 0.4688\n",
            "Epoch 356/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 4.6564 - f1: 0.9392 - val_loss: 4.3172 - val_f1: 0.4688\n",
            "Epoch 357/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 4.6466 - f1: 0.8929 - val_loss: 4.3259 - val_f1: 0.4688\n",
            "Epoch 358/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 4.6393 - f1: 0.9415 - val_loss: 4.3242 - val_f1: 0.4688\n",
            "Epoch 359/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 4.6324 - f1: 0.9373 - val_loss: 4.3116 - val_f1: 0.4688\n",
            "Epoch 360/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 4.6256 - f1: 0.9427 - val_loss: 4.2953 - val_f1: 0.4688\n",
            "Epoch 361/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 4.6155 - f1: 0.9404 - val_loss: 4.2606 - val_f1: 0.4688\n",
            "Epoch 362/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 4.6084 - f1: 0.9186 - val_loss: 4.2350 - val_f1: 0.4706\n",
            "Epoch 363/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 4.5989 - f1: 0.9400 - val_loss: 4.2233 - val_f1: 0.4706\n",
            "Epoch 364/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 4.5914 - f1: 0.9423 - val_loss: 4.2068 - val_f1: 0.4706\n",
            "Epoch 365/1000\n",
            "108/108 [==============================] - 0s 60us/step - loss: 4.5856 - f1: 0.9362 - val_loss: 4.1982 - val_f1: 0.4706\n",
            "Epoch 366/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 4.5787 - f1: 0.9398 - val_loss: 4.1895 - val_f1: 0.4706\n",
            "Epoch 367/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 4.5731 - f1: 0.9430 - val_loss: 4.1811 - val_f1: 0.4706\n",
            "Epoch 368/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 4.5668 - f1: 0.9338 - val_loss: 4.1782 - val_f1: 0.4706\n",
            "Epoch 369/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 4.5596 - f1: 0.9137 - val_loss: 4.1812 - val_f1: 0.4706\n",
            "Epoch 370/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 4.5513 - f1: 0.9247 - val_loss: 4.1872 - val_f1: 0.4706\n",
            "Epoch 371/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 4.5441 - f1: 0.9452 - val_loss: 4.1938 - val_f1: 0.4706\n",
            "Epoch 372/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 4.5394 - f1: 0.9199 - val_loss: 4.1940 - val_f1: 0.4706\n",
            "Epoch 373/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 4.5358 - f1: 0.9386 - val_loss: 4.2002 - val_f1: 0.4688\n",
            "Epoch 374/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 4.5298 - f1: 0.9383 - val_loss: 4.1942 - val_f1: 0.4688\n",
            "Epoch 375/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 4.5272 - f1: 0.9250 - val_loss: 4.1738 - val_f1: 0.4706\n",
            "Epoch 376/1000\n",
            "108/108 [==============================] - 0s 58us/step - loss: 4.5153 - f1: 0.9239 - val_loss: 4.1670 - val_f1: 0.4706\n",
            "Epoch 377/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 4.5088 - f1: 0.9382 - val_loss: 4.1598 - val_f1: 0.4706\n",
            "Epoch 378/1000\n",
            "108/108 [==============================] - 0s 59us/step - loss: 4.5021 - f1: 0.8998 - val_loss: 4.1436 - val_f1: 0.4706\n",
            "Epoch 379/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 4.4948 - f1: 0.8923 - val_loss: 4.1377 - val_f1: 0.4706\n",
            "Epoch 380/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 4.4896 - f1: 0.9055 - val_loss: 4.1499 - val_f1: 0.4706\n",
            "Epoch 381/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 4.4832 - f1: 0.9420 - val_loss: 4.1488 - val_f1: 0.4688\n",
            "Epoch 382/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 4.4779 - f1: 0.9011 - val_loss: 4.1363 - val_f1: 0.4706\n",
            "Epoch 383/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 4.4669 - f1: 0.9005 - val_loss: 4.1357 - val_f1: 0.4688\n",
            "Epoch 384/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 4.4635 - f1: 0.9349 - val_loss: 4.1298 - val_f1: 0.4688\n",
            "Epoch 385/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 4.4554 - f1: 0.9435 - val_loss: 4.1208 - val_f1: 0.4706\n",
            "Epoch 386/1000\n",
            "108/108 [==============================] - 0s 59us/step - loss: 4.4490 - f1: 0.8980 - val_loss: 4.1008 - val_f1: 0.4706\n",
            "Epoch 387/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 4.4405 - f1: 0.9356 - val_loss: 4.0932 - val_f1: 0.4706\n",
            "Epoch 388/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 4.4297 - f1: 0.9057 - val_loss: 4.0816 - val_f1: 0.4706\n",
            "Epoch 389/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 4.4229 - f1: 0.9256 - val_loss: 4.0601 - val_f1: 0.4706\n",
            "Epoch 390/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 4.4154 - f1: 0.9479 - val_loss: 4.0507 - val_f1: 0.4706\n",
            "Epoch 391/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 4.4037 - f1: 0.9423 - val_loss: 4.0359 - val_f1: 0.4706\n",
            "Epoch 392/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 4.3952 - f1: 0.9418 - val_loss: 4.0212 - val_f1: 0.4706\n",
            "Epoch 393/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 4.3889 - f1: 0.9252 - val_loss: 4.0102 - val_f1: 0.4706\n",
            "Epoch 394/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 4.3820 - f1: 0.9473 - val_loss: 4.0035 - val_f1: 0.4706\n",
            "Epoch 395/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 4.3790 - f1: 0.9183 - val_loss: 3.9869 - val_f1: 0.4706\n",
            "Epoch 396/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 4.3668 - f1: 0.9294 - val_loss: 3.9813 - val_f1: 0.4706\n",
            "Epoch 397/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 4.3606 - f1: 0.9227 - val_loss: 3.9814 - val_f1: 0.4706\n",
            "Epoch 398/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 4.3534 - f1: 0.9403 - val_loss: 3.9822 - val_f1: 0.4706\n",
            "Epoch 399/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 4.3465 - f1: 0.9146 - val_loss: 3.9735 - val_f1: 0.4706\n",
            "Epoch 400/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 4.3361 - f1: 0.9460 - val_loss: 3.9764 - val_f1: 0.4706\n",
            "Epoch 401/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 4.3313 - f1: 0.9118 - val_loss: 3.9779 - val_f1: 0.4706\n",
            "Epoch 402/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 4.3283 - f1: 0.9432 - val_loss: 3.9829 - val_f1: 0.4706\n",
            "Epoch 403/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 4.3207 - f1: 0.9408 - val_loss: 3.9779 - val_f1: 0.4706\n",
            "Epoch 404/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 4.3134 - f1: 0.9410 - val_loss: 3.9582 - val_f1: 0.4706\n",
            "Epoch 405/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 4.3059 - f1: 0.9437 - val_loss: 3.9383 - val_f1: 0.4706\n",
            "Epoch 406/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 4.2943 - f1: 0.9370 - val_loss: 3.9266 - val_f1: 0.4706\n",
            "Epoch 407/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 4.2877 - f1: 0.9210 - val_loss: 3.9138 - val_f1: 0.4706\n",
            "Epoch 408/1000\n",
            "108/108 [==============================] - 0s 59us/step - loss: 4.2809 - f1: 0.9399 - val_loss: 3.9047 - val_f1: 0.4706\n",
            "Epoch 409/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 4.2796 - f1: 0.9425 - val_loss: 3.8811 - val_f1: 0.4706\n",
            "Epoch 410/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 4.2671 - f1: 0.9246 - val_loss: 3.8682 - val_f1: 0.4706\n",
            "Epoch 411/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 4.2563 - f1: 0.9277 - val_loss: 3.8653 - val_f1: 0.4706\n",
            "Epoch 412/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 4.2460 - f1: 0.9370 - val_loss: 3.8621 - val_f1: 0.4706\n",
            "Epoch 413/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 4.2372 - f1: 0.9405 - val_loss: 3.8557 - val_f1: 0.4706\n",
            "Epoch 414/1000\n",
            "108/108 [==============================] - 0s 62us/step - loss: 4.2309 - f1: 0.9452 - val_loss: 3.8477 - val_f1: 0.4706\n",
            "Epoch 415/1000\n",
            "108/108 [==============================] - 0s 61us/step - loss: 4.2216 - f1: 0.9424 - val_loss: 3.8407 - val_f1: 0.4706\n",
            "Epoch 416/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 4.2133 - f1: 0.9397 - val_loss: 3.8287 - val_f1: 0.4706\n",
            "Epoch 417/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 4.2077 - f1: 0.9475 - val_loss: 3.8191 - val_f1: 0.4706\n",
            "Epoch 418/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 4.1992 - f1: 0.9350 - val_loss: 3.8135 - val_f1: 0.4706\n",
            "Epoch 419/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 4.1930 - f1: 0.9436 - val_loss: 3.8094 - val_f1: 0.4706\n",
            "Epoch 420/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 4.1860 - f1: 0.8667 - val_loss: 3.8061 - val_f1: 0.4706\n",
            "Epoch 421/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 4.1821 - f1: 0.9436 - val_loss: 3.8141 - val_f1: 0.4706\n",
            "Epoch 422/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 4.1749 - f1: 0.9388 - val_loss: 3.8169 - val_f1: 0.4706\n",
            "Epoch 423/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 4.1685 - f1: 0.9278 - val_loss: 3.8097 - val_f1: 0.4706\n",
            "Epoch 424/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 4.1621 - f1: 0.9425 - val_loss: 3.8092 - val_f1: 0.4706\n",
            "Epoch 425/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 4.1558 - f1: 0.9201 - val_loss: 3.8031 - val_f1: 0.4706\n",
            "Epoch 426/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 4.1508 - f1: 0.9432 - val_loss: 3.7941 - val_f1: 0.4706\n",
            "Epoch 427/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 4.1438 - f1: 0.8950 - val_loss: 3.7767 - val_f1: 0.4706\n",
            "Epoch 428/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 4.1367 - f1: 0.9393 - val_loss: 3.7726 - val_f1: 0.4706\n",
            "Epoch 429/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 4.1284 - f1: 0.9406 - val_loss: 3.7602 - val_f1: 0.4706\n",
            "Epoch 430/1000\n",
            "108/108 [==============================] - 0s 62us/step - loss: 4.1213 - f1: 0.9461 - val_loss: 3.7491 - val_f1: 0.4706\n",
            "Epoch 431/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 4.1138 - f1: 0.9199 - val_loss: 3.7374 - val_f1: 0.4706\n",
            "Epoch 432/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 4.1112 - f1: 0.9198 - val_loss: 3.7292 - val_f1: 0.4706\n",
            "Epoch 433/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 4.1040 - f1: 0.9449 - val_loss: 3.7284 - val_f1: 0.4706\n",
            "Epoch 434/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 4.0981 - f1: 0.9404 - val_loss: 3.7172 - val_f1: 0.4706\n",
            "Epoch 435/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 4.0874 - f1: 0.9419 - val_loss: 3.7111 - val_f1: 0.4706\n",
            "Epoch 436/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 4.0797 - f1: 0.9448 - val_loss: 3.6981 - val_f1: 0.4706\n",
            "Epoch 437/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 4.0726 - f1: 0.9201 - val_loss: 3.6898 - val_f1: 0.4706\n",
            "Epoch 438/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 4.0647 - f1: 0.9269 - val_loss: 3.6861 - val_f1: 0.4706\n",
            "Epoch 439/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 4.0563 - f1: 0.9301 - val_loss: 3.6848 - val_f1: 0.4706\n",
            "Epoch 440/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 4.0546 - f1: 0.9158 - val_loss: 3.6915 - val_f1: 0.4706\n",
            "Epoch 441/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 4.0444 - f1: 0.9210 - val_loss: 3.6937 - val_f1: 0.4706\n",
            "Epoch 442/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 4.0404 - f1: 0.9560 - val_loss: 3.6913 - val_f1: 0.4706\n",
            "Epoch 443/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 4.0346 - f1: 0.9491 - val_loss: 3.6746 - val_f1: 0.4706\n",
            "Epoch 444/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 4.0230 - f1: 0.9165 - val_loss: 3.6609 - val_f1: 0.4706\n",
            "Epoch 445/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 4.0135 - f1: 0.9423 - val_loss: 3.6485 - val_f1: 0.4706\n",
            "Epoch 446/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 4.0073 - f1: 0.9393 - val_loss: 3.6312 - val_f1: 0.4706\n",
            "Epoch 447/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 4.0016 - f1: 0.9447 - val_loss: 3.6216 - val_f1: 0.4474\n",
            "Epoch 448/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 3.9994 - f1: 0.9117 - val_loss: 3.6151 - val_f1: 0.4500\n",
            "Epoch 449/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 3.9941 - f1: 0.9432 - val_loss: 3.6098 - val_f1: 0.4474\n",
            "Epoch 450/1000\n",
            "108/108 [==============================] - 0s 58us/step - loss: 3.9847 - f1: 0.9391 - val_loss: 3.6050 - val_f1: 0.4474\n",
            "Epoch 451/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 3.9764 - f1: 0.9240 - val_loss: 3.6000 - val_f1: 0.4722\n",
            "Epoch 452/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 3.9772 - f1: 0.9424 - val_loss: 3.5996 - val_f1: 0.4706\n",
            "Epoch 453/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 3.9631 - f1: 0.9430 - val_loss: 3.5918 - val_f1: 0.4722\n",
            "Epoch 454/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 3.9528 - f1: 0.9387 - val_loss: 3.5858 - val_f1: 0.4722\n",
            "Epoch 455/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 3.9467 - f1: 0.9232 - val_loss: 3.5795 - val_f1: 0.4722\n",
            "Epoch 456/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 3.9391 - f1: 0.9413 - val_loss: 3.5727 - val_f1: 0.4722\n",
            "Epoch 457/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 3.9308 - f1: 0.9118 - val_loss: 3.5677 - val_f1: 0.4722\n",
            "Epoch 458/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 3.9241 - f1: 0.9220 - val_loss: 3.5637 - val_f1: 0.4706\n",
            "Epoch 459/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 3.9178 - f1: 0.9435 - val_loss: 3.5561 - val_f1: 0.4722\n",
            "Epoch 460/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 3.9102 - f1: 0.9529 - val_loss: 3.5491 - val_f1: 0.4722\n",
            "Epoch 461/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 3.9030 - f1: 0.9457 - val_loss: 3.5411 - val_f1: 0.4722\n",
            "Epoch 462/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 3.8926 - f1: 0.8929 - val_loss: 3.5352 - val_f1: 0.4722\n",
            "Epoch 463/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 3.8883 - f1: 0.9528 - val_loss: 3.5275 - val_f1: 0.4722\n",
            "Epoch 464/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 3.8808 - f1: 0.9258 - val_loss: 3.5186 - val_f1: 0.4722\n",
            "Epoch 465/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 3.8705 - f1: 0.9258 - val_loss: 3.5130 - val_f1: 0.4722\n",
            "Epoch 466/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 3.8658 - f1: 0.9529 - val_loss: 3.5053 - val_f1: 0.4722\n",
            "Epoch 467/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 3.8539 - f1: 0.9426 - val_loss: 3.4967 - val_f1: 0.4722\n",
            "Epoch 468/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 3.8505 - f1: 0.9142 - val_loss: 3.4888 - val_f1: 0.4722\n",
            "Epoch 469/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 3.8421 - f1: 0.9036 - val_loss: 3.4832 - val_f1: 0.4722\n",
            "Epoch 470/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 3.8359 - f1: 0.9410 - val_loss: 3.4792 - val_f1: 0.4722\n",
            "Epoch 471/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 3.8277 - f1: 0.9533 - val_loss: 3.4729 - val_f1: 0.4722\n",
            "Epoch 472/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 3.8187 - f1: 0.9542 - val_loss: 3.4656 - val_f1: 0.4722\n",
            "Epoch 473/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 3.8116 - f1: 0.9386 - val_loss: 3.4585 - val_f1: 0.4722\n",
            "Epoch 474/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 3.8055 - f1: 0.9526 - val_loss: 3.4506 - val_f1: 0.4722\n",
            "Epoch 475/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 3.7996 - f1: 0.9540 - val_loss: 3.4446 - val_f1: 0.4722\n",
            "Epoch 476/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 3.7911 - f1: 0.9545 - val_loss: 3.4372 - val_f1: 0.4722\n",
            "Epoch 477/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 3.7824 - f1: 0.9353 - val_loss: 3.4309 - val_f1: 0.4722\n",
            "Epoch 478/1000\n",
            "108/108 [==============================] - 0s 58us/step - loss: 3.7756 - f1: 0.9212 - val_loss: 3.4252 - val_f1: 0.4722\n",
            "Epoch 479/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 3.7660 - f1: 0.9384 - val_loss: 3.4214 - val_f1: 0.4722\n",
            "Epoch 480/1000\n",
            "108/108 [==============================] - 0s 58us/step - loss: 3.7643 - f1: 0.9479 - val_loss: 3.4183 - val_f1: 0.4722\n",
            "Epoch 481/1000\n",
            "108/108 [==============================] - 0s 62us/step - loss: 3.7565 - f1: 0.9611 - val_loss: 3.4133 - val_f1: 0.4722\n",
            "Epoch 482/1000\n",
            "108/108 [==============================] - 0s 59us/step - loss: 3.7529 - f1: 0.9514 - val_loss: 3.4062 - val_f1: 0.4722\n",
            "Epoch 483/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 3.7436 - f1: 0.9641 - val_loss: 3.4032 - val_f1: 0.4722\n",
            "Epoch 484/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 3.7403 - f1: 0.9424 - val_loss: 3.3942 - val_f1: 0.4722\n",
            "Epoch 485/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 3.7299 - f1: 0.9647 - val_loss: 3.3876 - val_f1: 0.4722\n",
            "Epoch 486/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 3.7251 - f1: 0.9659 - val_loss: 3.3829 - val_f1: 0.4722\n",
            "Epoch 487/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 3.7174 - f1: 0.9646 - val_loss: 3.3749 - val_f1: 0.4722\n",
            "Epoch 488/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 3.7083 - f1: 0.9690 - val_loss: 3.3660 - val_f1: 0.4722\n",
            "Epoch 489/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 3.7031 - f1: 0.9646 - val_loss: 3.3600 - val_f1: 0.4500\n",
            "Epoch 490/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 3.6906 - f1: 0.9415 - val_loss: 3.3555 - val_f1: 0.4500\n",
            "Epoch 491/1000\n",
            "108/108 [==============================] - 0s 59us/step - loss: 3.6863 - f1: 0.9354 - val_loss: 3.3509 - val_f1: 0.4500\n",
            "Epoch 492/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 3.6822 - f1: 0.9261 - val_loss: 3.3487 - val_f1: 0.4500\n",
            "Epoch 493/1000\n",
            "108/108 [==============================] - 0s 58us/step - loss: 3.6732 - f1: 0.9447 - val_loss: 3.3429 - val_f1: 0.4500\n",
            "Epoch 494/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 3.6691 - f1: 0.9420 - val_loss: 3.3361 - val_f1: 0.4500\n",
            "Epoch 495/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 3.6578 - f1: 0.9386 - val_loss: 3.3327 - val_f1: 0.4500\n",
            "Epoch 496/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 3.6535 - f1: 0.9232 - val_loss: 3.3269 - val_f1: 0.4500\n",
            "Epoch 497/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 3.6473 - f1: 0.8880 - val_loss: 3.3186 - val_f1: 0.4500\n",
            "Epoch 498/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 3.6398 - f1: 0.9385 - val_loss: 3.3121 - val_f1: 0.4500\n",
            "Epoch 499/1000\n",
            "108/108 [==============================] - 0s 58us/step - loss: 3.6316 - f1: 0.9420 - val_loss: 3.3095 - val_f1: 0.4500\n",
            "Epoch 500/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 3.6237 - f1: 0.9244 - val_loss: 3.3040 - val_f1: 0.4500\n",
            "Epoch 501/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 3.6171 - f1: 0.9244 - val_loss: 3.2961 - val_f1: 0.4500\n",
            "Epoch 502/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 3.6079 - f1: 0.9296 - val_loss: 3.2852 - val_f1: 0.4500\n",
            "Epoch 503/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 3.6095 - f1: 0.9499 - val_loss: 3.2749 - val_f1: 0.4737\n",
            "Epoch 504/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 3.5950 - f1: 0.9678 - val_loss: 3.2691 - val_f1: 0.4737\n",
            "Epoch 505/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 3.5853 - f1: 0.9479 - val_loss: 3.2650 - val_f1: 0.4500\n",
            "Epoch 506/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 3.5792 - f1: 0.9638 - val_loss: 3.2595 - val_f1: 0.4500\n",
            "Epoch 507/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 3.5739 - f1: 0.9522 - val_loss: 3.2577 - val_f1: 0.4500\n",
            "Epoch 508/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 3.5653 - f1: 0.9510 - val_loss: 3.2546 - val_f1: 0.4500\n",
            "Epoch 509/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 3.5574 - f1: 0.8659 - val_loss: 3.2508 - val_f1: 0.4500\n",
            "Epoch 510/1000\n",
            "108/108 [==============================] - 0s 58us/step - loss: 3.5515 - f1: 0.9409 - val_loss: 3.2459 - val_f1: 0.4500\n",
            "Epoch 511/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 3.5469 - f1: 0.9182 - val_loss: 3.2403 - val_f1: 0.4500\n",
            "Epoch 512/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 3.5362 - f1: 0.9091 - val_loss: 3.2294 - val_f1: 0.4500\n",
            "Epoch 513/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 3.5334 - f1: 0.9555 - val_loss: 3.2177 - val_f1: 0.4500\n",
            "Epoch 514/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 3.5231 - f1: 0.9518 - val_loss: 3.2099 - val_f1: 0.4500\n",
            "Epoch 515/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 3.5168 - f1: 0.9760 - val_loss: 3.2036 - val_f1: 0.4500\n",
            "Epoch 516/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 3.5091 - f1: 0.9762 - val_loss: 3.1985 - val_f1: 0.4500\n",
            "Epoch 517/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 3.4943 - f1: 0.9766 - val_loss: 3.1995 - val_f1: 0.4500\n",
            "Epoch 518/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 3.4917 - f1: 0.9425 - val_loss: 3.2166 - val_f1: 0.4500\n",
            "Epoch 519/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 3.4933 - f1: 0.9302 - val_loss: 3.2308 - val_f1: 0.4286\n",
            "Epoch 520/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 3.4848 - f1: 0.8830 - val_loss: 3.2312 - val_f1: 0.4286\n",
            "Epoch 521/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 3.4809 - f1: 0.9304 - val_loss: 3.2282 - val_f1: 0.4286\n",
            "Epoch 522/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 3.4731 - f1: 0.9168 - val_loss: 3.2153 - val_f1: 0.4286\n",
            "Epoch 523/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 3.4591 - f1: 0.9079 - val_loss: 3.1925 - val_f1: 0.4286\n",
            "Epoch 524/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 3.4453 - f1: 0.8917 - val_loss: 3.1735 - val_f1: 0.4500\n",
            "Epoch 525/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 3.4368 - f1: 0.9451 - val_loss: 3.1478 - val_f1: 0.4500\n",
            "Epoch 526/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 3.4279 - f1: 0.9506 - val_loss: 3.1321 - val_f1: 0.4500\n",
            "Epoch 527/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 3.4243 - f1: 0.9518 - val_loss: 3.1209 - val_f1: 0.4737\n",
            "Epoch 528/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 3.4223 - f1: 0.9744 - val_loss: 3.1141 - val_f1: 0.4737\n",
            "Epoch 529/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 3.4179 - f1: 0.9762 - val_loss: 3.1076 - val_f1: 0.4737\n",
            "Epoch 530/1000\n",
            "108/108 [==============================] - 0s 58us/step - loss: 3.4108 - f1: 0.9773 - val_loss: 3.1021 - val_f1: 0.4737\n",
            "Epoch 531/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 3.3995 - f1: 0.9737 - val_loss: 3.0984 - val_f1: 0.4737\n",
            "Epoch 532/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 3.3903 - f1: 0.9773 - val_loss: 3.0952 - val_f1: 0.4500\n",
            "Epoch 533/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 3.3777 - f1: 0.9761 - val_loss: 3.0960 - val_f1: 0.4500\n",
            "Epoch 534/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 3.3717 - f1: 0.9761 - val_loss: 3.0992 - val_f1: 0.4500\n",
            "Epoch 535/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 3.3807 - f1: 0.8967 - val_loss: 3.1046 - val_f1: 0.4500\n",
            "Epoch 536/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 3.3559 - f1: 0.9238 - val_loss: 3.0887 - val_f1: 0.4500\n",
            "Epoch 537/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 3.3519 - f1: 0.9762 - val_loss: 3.0693 - val_f1: 0.4500\n",
            "Epoch 538/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 3.3453 - f1: 0.9470 - val_loss: 3.0578 - val_f1: 0.4500\n",
            "Epoch 539/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 3.3387 - f1: 0.9734 - val_loss: 3.0488 - val_f1: 0.4737\n",
            "Epoch 540/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 3.3318 - f1: 0.9747 - val_loss: 3.0425 - val_f1: 0.4737\n",
            "Epoch 541/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 3.3310 - f1: 0.9609 - val_loss: 3.0441 - val_f1: 0.4500\n",
            "Epoch 542/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 3.3112 - f1: 0.9761 - val_loss: 3.0417 - val_f1: 0.4500\n",
            "Epoch 543/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 3.3016 - f1: 0.9768 - val_loss: 3.0391 - val_f1: 0.4500\n",
            "Epoch 544/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 3.2980 - f1: 0.9405 - val_loss: 3.0420 - val_f1: 0.4500\n",
            "Epoch 545/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 3.2871 - f1: 0.9625 - val_loss: 3.0304 - val_f1: 0.4500\n",
            "Epoch 546/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 3.2811 - f1: 0.9643 - val_loss: 3.0250 - val_f1: 0.4500\n",
            "Epoch 547/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 3.2741 - f1: 0.9673 - val_loss: 3.0172 - val_f1: 0.4500\n",
            "Epoch 548/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 3.2647 - f1: 0.9786 - val_loss: 3.0115 - val_f1: 0.4500\n",
            "Epoch 549/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 3.2562 - f1: 0.9762 - val_loss: 3.0116 - val_f1: 0.4500\n",
            "Epoch 550/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 3.2534 - f1: 0.9643 - val_loss: 3.0123 - val_f1: 0.4500\n",
            "Epoch 551/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 3.2438 - f1: 0.9667 - val_loss: 3.0101 - val_f1: 0.4500\n",
            "Epoch 552/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 3.2367 - f1: 0.9364 - val_loss: 3.0029 - val_f1: 0.4500\n",
            "Epoch 553/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 3.2305 - f1: 0.9637 - val_loss: 2.9891 - val_f1: 0.4500\n",
            "Epoch 554/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 3.2224 - f1: 0.9628 - val_loss: 2.9834 - val_f1: 0.4500\n",
            "Epoch 555/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 3.2148 - f1: 0.9603 - val_loss: 2.9849 - val_f1: 0.4500\n",
            "Epoch 556/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 3.2082 - f1: 0.9361 - val_loss: 2.9828 - val_f1: 0.4500\n",
            "Epoch 557/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 3.2023 - f1: 0.9533 - val_loss: 2.9763 - val_f1: 0.4500\n",
            "Epoch 558/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 3.2011 - f1: 0.9522 - val_loss: 2.9902 - val_f1: 0.4286\n",
            "Epoch 559/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 3.1851 - f1: 0.9408 - val_loss: 2.9891 - val_f1: 0.4286\n",
            "Epoch 560/1000\n",
            "108/108 [==============================] - 0s 60us/step - loss: 3.1770 - f1: 0.9450 - val_loss: 2.9955 - val_f1: 0.4091\n",
            "Epoch 561/1000\n",
            "108/108 [==============================] - 0s 66us/step - loss: 3.1800 - f1: 0.9315 - val_loss: 2.9994 - val_f1: 0.4091\n",
            "Epoch 562/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 3.1670 - f1: 0.9116 - val_loss: 2.9950 - val_f1: 0.4091\n",
            "Epoch 563/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 3.1622 - f1: 0.9440 - val_loss: 2.9678 - val_f1: 0.4091\n",
            "Epoch 564/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 3.1478 - f1: 0.9400 - val_loss: 2.9523 - val_f1: 0.4286\n",
            "Epoch 565/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 3.1421 - f1: 0.9529 - val_loss: 2.9321 - val_f1: 0.4286\n",
            "Epoch 566/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 3.1332 - f1: 0.9498 - val_loss: 2.9283 - val_f1: 0.4286\n",
            "Epoch 567/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 3.1214 - f1: 0.9535 - val_loss: 2.9228 - val_f1: 0.4286\n",
            "Epoch 568/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 3.1165 - f1: 0.9419 - val_loss: 2.9180 - val_f1: 0.4286\n",
            "Epoch 569/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 3.1113 - f1: 0.9545 - val_loss: 2.9037 - val_f1: 0.4286\n",
            "Epoch 570/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 3.0983 - f1: 0.9516 - val_loss: 2.9048 - val_f1: 0.4286\n",
            "Epoch 571/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 3.0930 - f1: 0.9542 - val_loss: 2.9049 - val_f1: 0.4286\n",
            "Epoch 572/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 3.0880 - f1: 0.9309 - val_loss: 2.8976 - val_f1: 0.4286\n",
            "Epoch 573/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 3.0798 - f1: 0.9503 - val_loss: 2.8817 - val_f1: 0.4286\n",
            "Epoch 574/1000\n",
            "108/108 [==============================] - 0s 60us/step - loss: 3.0734 - f1: 0.9539 - val_loss: 2.8681 - val_f1: 0.4500\n",
            "Epoch 575/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 3.0667 - f1: 0.9773 - val_loss: 2.8563 - val_f1: 0.4500\n",
            "Epoch 576/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 3.0628 - f1: 0.9750 - val_loss: 2.8482 - val_f1: 0.4500\n",
            "Epoch 577/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 3.0502 - f1: 0.9772 - val_loss: 2.8524 - val_f1: 0.4500\n",
            "Epoch 578/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 3.0472 - f1: 0.9540 - val_loss: 2.8628 - val_f1: 0.4286\n",
            "Epoch 579/1000\n",
            "108/108 [==============================] - 0s 61us/step - loss: 3.0382 - f1: 0.9487 - val_loss: 2.8652 - val_f1: 0.4091\n",
            "Epoch 580/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 3.0339 - f1: 0.9398 - val_loss: 2.8628 - val_f1: 0.4091\n",
            "Epoch 581/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 3.0254 - f1: 0.9523 - val_loss: 2.8439 - val_f1: 0.4286\n",
            "Epoch 582/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 3.0176 - f1: 0.9404 - val_loss: 2.8280 - val_f1: 0.4500\n",
            "Epoch 583/1000\n",
            "108/108 [==============================] - 0s 60us/step - loss: 3.0082 - f1: 0.9773 - val_loss: 2.8098 - val_f1: 0.4500\n",
            "Epoch 584/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 3.0056 - f1: 0.9760 - val_loss: 2.7951 - val_f1: 0.4500\n",
            "Epoch 585/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 2.9994 - f1: 0.9783 - val_loss: 2.7856 - val_f1: 0.4500\n",
            "Epoch 586/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 2.9931 - f1: 0.9496 - val_loss: 2.7774 - val_f1: 0.4500\n",
            "Epoch 587/1000\n",
            "108/108 [==============================] - 0s 58us/step - loss: 2.9877 - f1: 0.9743 - val_loss: 2.7726 - val_f1: 0.4500\n",
            "Epoch 588/1000\n",
            "108/108 [==============================] - 0s 58us/step - loss: 2.9806 - f1: 0.9595 - val_loss: 2.7738 - val_f1: 0.4500\n",
            "Epoch 589/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 2.9717 - f1: 0.9775 - val_loss: 2.7763 - val_f1: 0.4500\n",
            "Epoch 590/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 2.9649 - f1: 0.9782 - val_loss: 2.7788 - val_f1: 0.4500\n",
            "Epoch 591/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 2.9573 - f1: 0.9756 - val_loss: 2.7753 - val_f1: 0.4500\n",
            "Epoch 592/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 2.9509 - f1: 0.9764 - val_loss: 2.7712 - val_f1: 0.4500\n",
            "Epoch 593/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 2.9438 - f1: 0.9737 - val_loss: 2.7679 - val_f1: 0.4286\n",
            "Epoch 594/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 2.9377 - f1: 0.9750 - val_loss: 2.7598 - val_f1: 0.4500\n",
            "Epoch 595/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 2.9320 - f1: 0.9583 - val_loss: 2.7605 - val_f1: 0.4091\n",
            "Epoch 596/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 2.9250 - f1: 0.9648 - val_loss: 2.7510 - val_f1: 0.4286\n",
            "Epoch 597/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 2.9171 - f1: 0.9755 - val_loss: 2.7440 - val_f1: 0.4286\n",
            "Epoch 598/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 2.9116 - f1: 0.9777 - val_loss: 2.7349 - val_f1: 0.4500\n",
            "Epoch 599/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 2.9052 - f1: 0.9737 - val_loss: 2.7309 - val_f1: 0.4500\n",
            "Epoch 600/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 2.8972 - f1: 0.9761 - val_loss: 2.7273 - val_f1: 0.4500\n",
            "Epoch 601/1000\n",
            "108/108 [==============================] - 0s 50us/step - loss: 2.8922 - f1: 0.9574 - val_loss: 2.7238 - val_f1: 0.4091\n",
            "Epoch 602/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 2.8842 - f1: 0.9747 - val_loss: 2.7138 - val_f1: 0.4500\n",
            "Epoch 603/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 2.8793 - f1: 0.9641 - val_loss: 2.7049 - val_f1: 0.4500\n",
            "Epoch 604/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 2.8716 - f1: 0.9500 - val_loss: 2.6906 - val_f1: 0.4500\n",
            "Epoch 605/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 2.8633 - f1: 0.9603 - val_loss: 2.6678 - val_f1: 0.4500\n",
            "Epoch 606/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 2.8618 - f1: 0.9556 - val_loss: 2.6499 - val_f1: 0.4500\n",
            "Epoch 607/1000\n",
            "108/108 [==============================] - 0s 61us/step - loss: 2.8605 - f1: 0.9792 - val_loss: 2.6386 - val_f1: 0.4500\n",
            "Epoch 608/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 2.8530 - f1: 0.9648 - val_loss: 2.6346 - val_f1: 0.4500\n",
            "Epoch 609/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 2.8452 - f1: 0.9574 - val_loss: 2.6307 - val_f1: 0.4500\n",
            "Epoch 610/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 2.8360 - f1: 0.9749 - val_loss: 2.6265 - val_f1: 0.4500\n",
            "Epoch 611/1000\n",
            "108/108 [==============================] - 0s 59us/step - loss: 2.8293 - f1: 0.9777 - val_loss: 2.6214 - val_f1: 0.4500\n",
            "Epoch 612/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 2.8221 - f1: 0.9749 - val_loss: 2.6245 - val_f1: 0.4500\n",
            "Epoch 613/1000\n",
            "108/108 [==============================] - 0s 50us/step - loss: 2.8077 - f1: 0.9767 - val_loss: 2.6368 - val_f1: 0.4500\n",
            "Epoch 614/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 2.7991 - f1: 0.9773 - val_loss: 2.6612 - val_f1: 0.4091\n",
            "Epoch 615/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 2.7957 - f1: 0.9522 - val_loss: 2.6842 - val_f1: 0.4091\n",
            "Epoch 616/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 2.7852 - f1: 0.9432 - val_loss: 2.6776 - val_f1: 0.4091\n",
            "Epoch 617/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 2.7752 - f1: 0.9231 - val_loss: 2.6638 - val_f1: 0.4091\n",
            "Epoch 618/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 2.7698 - f1: 0.9664 - val_loss: 2.6382 - val_f1: 0.4091\n",
            "Epoch 619/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 2.7607 - f1: 0.9651 - val_loss: 2.6319 - val_f1: 0.4091\n",
            "Epoch 620/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 2.7512 - f1: 0.9636 - val_loss: 2.6335 - val_f1: 0.4091\n",
            "Epoch 621/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 2.7444 - f1: 0.9484 - val_loss: 2.6284 - val_f1: 0.4091\n",
            "Epoch 622/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 2.7382 - f1: 0.9455 - val_loss: 2.6143 - val_f1: 0.4091\n",
            "Epoch 623/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 2.7319 - f1: 0.9664 - val_loss: 2.5996 - val_f1: 0.4091\n",
            "Epoch 624/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 2.7244 - f1: 0.9791 - val_loss: 2.5937 - val_f1: 0.4091\n",
            "Epoch 625/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 2.7184 - f1: 0.9792 - val_loss: 2.5898 - val_f1: 0.4091\n",
            "Epoch 626/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 2.7127 - f1: 0.9750 - val_loss: 2.5830 - val_f1: 0.4091\n",
            "Epoch 627/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 2.7063 - f1: 0.9563 - val_loss: 2.5917 - val_f1: 0.4091\n",
            "Epoch 628/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 2.6955 - f1: 0.9643 - val_loss: 2.5822 - val_f1: 0.4091\n",
            "Epoch 629/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 2.6912 - f1: 0.9623 - val_loss: 2.5684 - val_f1: 0.4091\n",
            "Epoch 630/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 2.6804 - f1: 0.9625 - val_loss: 2.5713 - val_f1: 0.4091\n",
            "Epoch 631/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 2.6762 - f1: 0.9660 - val_loss: 2.5676 - val_f1: 0.4091\n",
            "Epoch 632/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 2.6712 - f1: 0.9381 - val_loss: 2.5822 - val_f1: 0.4091\n",
            "Epoch 633/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 2.6627 - f1: 0.9636 - val_loss: 2.5640 - val_f1: 0.4091\n",
            "Epoch 634/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 2.6574 - f1: 0.9620 - val_loss: 2.5552 - val_f1: 0.4091\n",
            "Epoch 635/1000\n",
            "108/108 [==============================] - 0s 58us/step - loss: 2.6497 - f1: 0.9167 - val_loss: 2.5454 - val_f1: 0.4091\n",
            "Epoch 636/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 2.6453 - f1: 0.9688 - val_loss: 2.5139 - val_f1: 0.4091\n",
            "Epoch 637/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 2.6339 - f1: 0.9778 - val_loss: 2.5127 - val_f1: 0.4091\n",
            "Epoch 638/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 2.6273 - f1: 0.9756 - val_loss: 2.5345 - val_f1: 0.4091\n",
            "Epoch 639/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 2.6137 - f1: 0.9458 - val_loss: 2.5476 - val_f1: 0.4091\n",
            "Epoch 640/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 2.6079 - f1: 0.9678 - val_loss: 2.5505 - val_f1: 0.4091\n",
            "Epoch 641/1000\n",
            "108/108 [==============================] - 0s 58us/step - loss: 2.6014 - f1: 0.9522 - val_loss: 2.5484 - val_f1: 0.4091\n",
            "Epoch 642/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 2.5979 - f1: 0.9464 - val_loss: 2.5265 - val_f1: 0.4091\n",
            "Epoch 643/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 2.5898 - f1: 0.9542 - val_loss: 2.5495 - val_f1: 0.4091\n",
            "Epoch 644/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 2.5801 - f1: 0.9345 - val_loss: 2.5406 - val_f1: 0.4091\n",
            "Epoch 645/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 2.5695 - f1: 0.9336 - val_loss: 2.5136 - val_f1: 0.4091\n",
            "Epoch 646/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 2.5623 - f1: 0.9628 - val_loss: 2.4794 - val_f1: 0.4091\n",
            "Epoch 647/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 2.5569 - f1: 0.9750 - val_loss: 2.4602 - val_f1: 0.4091\n",
            "Epoch 648/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 2.5504 - f1: 0.9752 - val_loss: 2.4548 - val_f1: 0.4091\n",
            "Epoch 649/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 2.5437 - f1: 0.9750 - val_loss: 2.4515 - val_f1: 0.4091\n",
            "Epoch 650/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 2.5363 - f1: 0.9744 - val_loss: 2.4649 - val_f1: 0.4091\n",
            "Epoch 651/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 2.5269 - f1: 0.9643 - val_loss: 2.4931 - val_f1: 0.4091\n",
            "Epoch 652/1000\n",
            "108/108 [==============================] - 0s 50us/step - loss: 2.5188 - f1: 0.9538 - val_loss: 2.5209 - val_f1: 0.4091\n",
            "Epoch 653/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 2.5207 - f1: 0.9122 - val_loss: 2.5360 - val_f1: 0.4091\n",
            "Epoch 654/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 2.5105 - f1: 0.9514 - val_loss: 2.5047 - val_f1: 0.4091\n",
            "Epoch 655/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 2.5041 - f1: 0.9518 - val_loss: 2.4940 - val_f1: 0.4091\n",
            "Epoch 656/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 2.4908 - f1: 0.9348 - val_loss: 2.4520 - val_f1: 0.4091\n",
            "Epoch 657/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 2.4870 - f1: 0.9636 - val_loss: 2.4174 - val_f1: 0.4091\n",
            "Epoch 658/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 2.4752 - f1: 0.9472 - val_loss: 2.4174 - val_f1: 0.4091\n",
            "Epoch 659/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 2.4688 - f1: 0.9631 - val_loss: 2.4019 - val_f1: 0.4091\n",
            "Epoch 660/1000\n",
            "108/108 [==============================] - 0s 58us/step - loss: 2.4613 - f1: 0.9756 - val_loss: 2.3973 - val_f1: 0.4091\n",
            "Epoch 661/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 2.4565 - f1: 0.9375 - val_loss: 2.4092 - val_f1: 0.4091\n",
            "Epoch 662/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 2.4470 - f1: 0.9537 - val_loss: 2.3900 - val_f1: 0.4091\n",
            "Epoch 663/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 2.4396 - f1: 0.9762 - val_loss: 2.3741 - val_f1: 0.4091\n",
            "Epoch 664/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 2.4403 - f1: 0.9750 - val_loss: 2.3880 - val_f1: 0.4091\n",
            "Epoch 665/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 2.4264 - f1: 0.9792 - val_loss: 2.3863 - val_f1: 0.4091\n",
            "Epoch 666/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 2.4203 - f1: 0.9761 - val_loss: 2.3876 - val_f1: 0.4091\n",
            "Epoch 667/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 2.4158 - f1: 0.9641 - val_loss: 2.4086 - val_f1: 0.4091\n",
            "Epoch 668/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 2.4138 - f1: 0.9273 - val_loss: 2.4230 - val_f1: 0.4091\n",
            "Epoch 669/1000\n",
            "108/108 [==============================] - 0s 50us/step - loss: 2.4009 - f1: 0.9543 - val_loss: 2.3844 - val_f1: 0.4091\n",
            "Epoch 670/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 2.3938 - f1: 0.9737 - val_loss: 2.3795 - val_f1: 0.4091\n",
            "Epoch 671/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 2.3875 - f1: 0.9511 - val_loss: 2.3829 - val_f1: 0.4091\n",
            "Epoch 672/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 2.3784 - f1: 0.9664 - val_loss: 2.3963 - val_f1: 0.4091\n",
            "Epoch 673/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 2.3750 - f1: 0.9048 - val_loss: 2.4206 - val_f1: 0.4091\n",
            "Epoch 674/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 2.3707 - f1: 0.9545 - val_loss: 2.4078 - val_f1: 0.4091\n",
            "Epoch 675/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 2.3662 - f1: 0.9146 - val_loss: 2.4246 - val_f1: 0.4091\n",
            "Epoch 676/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 2.3568 - f1: 0.9312 - val_loss: 2.4079 - val_f1: 0.4091\n",
            "Epoch 677/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 2.3494 - f1: 0.9653 - val_loss: 2.3751 - val_f1: 0.4091\n",
            "Epoch 678/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 2.3440 - f1: 0.9643 - val_loss: 2.3625 - val_f1: 0.4091\n",
            "Epoch 679/1000\n",
            "108/108 [==============================] - 0s 50us/step - loss: 2.3400 - f1: 0.9722 - val_loss: 2.3425 - val_f1: 0.4091\n",
            "Epoch 680/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 2.3308 - f1: 0.9511 - val_loss: 2.3594 - val_f1: 0.4091\n",
            "Epoch 681/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 2.3255 - f1: 0.9605 - val_loss: 2.3619 - val_f1: 0.4091\n",
            "Epoch 682/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 2.3184 - f1: 0.9444 - val_loss: 2.3729 - val_f1: 0.4091\n",
            "Epoch 683/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 2.3107 - f1: 0.9500 - val_loss: 2.4039 - val_f1: 0.4091\n",
            "Epoch 684/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 2.3090 - f1: 0.9365 - val_loss: 2.4185 - val_f1: 0.4091\n",
            "Epoch 685/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 2.2985 - f1: 0.9542 - val_loss: 2.3948 - val_f1: 0.4091\n",
            "Epoch 686/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 2.2903 - f1: 0.9519 - val_loss: 2.3831 - val_f1: 0.4091\n",
            "Epoch 687/1000\n",
            "108/108 [==============================] - 0s 50us/step - loss: 2.2844 - f1: 0.9501 - val_loss: 2.3646 - val_f1: 0.4091\n",
            "Epoch 688/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 2.2783 - f1: 0.9560 - val_loss: 2.3642 - val_f1: 0.4091\n",
            "Epoch 689/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 2.2719 - f1: 0.9565 - val_loss: 2.3705 - val_f1: 0.4091\n",
            "Epoch 690/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 2.2635 - f1: 0.9539 - val_loss: 2.3886 - val_f1: 0.4091\n",
            "Epoch 691/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 2.2598 - f1: 0.9484 - val_loss: 2.3981 - val_f1: 0.4091\n",
            "Epoch 692/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 2.2554 - f1: 0.9328 - val_loss: 2.3952 - val_f1: 0.4091\n",
            "Epoch 693/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 2.2460 - f1: 0.9492 - val_loss: 2.3516 - val_f1: 0.4091\n",
            "Epoch 694/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 2.2424 - f1: 0.9230 - val_loss: 2.3252 - val_f1: 0.4091\n",
            "Epoch 695/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 2.2408 - f1: 0.9730 - val_loss: 2.3089 - val_f1: 0.4091\n",
            "Epoch 696/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 2.2364 - f1: 0.9360 - val_loss: 2.3339 - val_f1: 0.4091\n",
            "Epoch 697/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 2.2270 - f1: 0.9539 - val_loss: 2.3326 - val_f1: 0.4091\n",
            "Epoch 698/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 2.2212 - f1: 0.9277 - val_loss: 2.3218 - val_f1: 0.4091\n",
            "Epoch 699/1000\n",
            "108/108 [==============================] - 0s 59us/step - loss: 2.2158 - f1: 0.9256 - val_loss: 2.3055 - val_f1: 0.4091\n",
            "Epoch 700/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 2.2129 - f1: 0.9782 - val_loss: 2.2733 - val_f1: 0.4091\n",
            "Epoch 701/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 2.2085 - f1: 0.9881 - val_loss: 2.2641 - val_f1: 0.4091\n",
            "Epoch 702/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 2.2016 - f1: 0.9875 - val_loss: 2.2972 - val_f1: 0.4091\n",
            "Epoch 703/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 2.1971 - f1: 0.9153 - val_loss: 2.3441 - val_f1: 0.4091\n",
            "Epoch 704/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 2.1872 - f1: 0.9555 - val_loss: 2.3446 - val_f1: 0.4091\n",
            "Epoch 705/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 2.1816 - f1: 0.9512 - val_loss: 2.3899 - val_f1: 0.4091\n",
            "Epoch 706/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 2.1753 - f1: 0.9583 - val_loss: 2.4361 - val_f1: 0.4130\n",
            "Epoch 707/1000\n",
            "108/108 [==============================] - 0s 50us/step - loss: 2.1674 - f1: 0.9521 - val_loss: 2.4657 - val_f1: 0.4130\n",
            "Epoch 708/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 2.1612 - f1: 0.9324 - val_loss: 2.5179 - val_f1: 0.4130\n",
            "Epoch 709/1000\n",
            "108/108 [==============================] - 0s 62us/step - loss: 2.1630 - f1: 0.9380 - val_loss: 2.5493 - val_f1: 0.4130\n",
            "Epoch 710/1000\n",
            "108/108 [==============================] - 0s 59us/step - loss: 2.1574 - f1: 0.9434 - val_loss: 2.5412 - val_f1: 0.4130\n",
            "Epoch 711/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 2.1458 - f1: 0.9028 - val_loss: 2.4909 - val_f1: 0.4130\n",
            "Epoch 712/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 2.1319 - f1: 0.9441 - val_loss: 2.3975 - val_f1: 0.4130\n",
            "Epoch 713/1000\n",
            "108/108 [==============================] - 0s 50us/step - loss: 2.1179 - f1: 0.9596 - val_loss: 2.3528 - val_f1: 0.4091\n",
            "Epoch 714/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 2.1136 - f1: 0.9644 - val_loss: 2.3205 - val_f1: 0.4091\n",
            "Epoch 715/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 2.1048 - f1: 0.9472 - val_loss: 2.3079 - val_f1: 0.4091\n",
            "Epoch 716/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 2.0985 - f1: 0.9631 - val_loss: 2.2861 - val_f1: 0.4091\n",
            "Epoch 717/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 2.0896 - f1: 0.9664 - val_loss: 2.2873 - val_f1: 0.4091\n",
            "Epoch 718/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 2.0857 - f1: 0.9641 - val_loss: 2.3073 - val_f1: 0.4091\n",
            "Epoch 719/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 2.0780 - f1: 0.9437 - val_loss: 2.3092 - val_f1: 0.4091\n",
            "Epoch 720/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 2.0751 - f1: 0.9664 - val_loss: 2.3225 - val_f1: 0.4091\n",
            "Epoch 721/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 2.0644 - f1: 0.9648 - val_loss: 2.3102 - val_f1: 0.4091\n",
            "Epoch 722/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 2.0610 - f1: 0.9500 - val_loss: 2.3028 - val_f1: 0.4091\n",
            "Epoch 723/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 2.0556 - f1: 0.9603 - val_loss: 2.2771 - val_f1: 0.4091\n",
            "Epoch 724/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 2.0523 - f1: 0.9579 - val_loss: 2.2765 - val_f1: 0.4091\n",
            "Epoch 725/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 2.0468 - f1: 0.9678 - val_loss: 2.2836 - val_f1: 0.4091\n",
            "Epoch 726/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 2.0416 - f1: 0.9641 - val_loss: 2.3396 - val_f1: 0.4091\n",
            "Epoch 727/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 2.0369 - f1: 0.9353 - val_loss: 2.3741 - val_f1: 0.4130\n",
            "Epoch 728/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 2.0297 - f1: 0.9335 - val_loss: 2.3767 - val_f1: 0.4130\n",
            "Epoch 729/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 2.0275 - f1: 0.9534 - val_loss: 2.3676 - val_f1: 0.4130\n",
            "Epoch 730/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 2.0196 - f1: 0.9309 - val_loss: 2.3635 - val_f1: 0.4130\n",
            "Epoch 731/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 2.0174 - f1: 0.9575 - val_loss: 2.3237 - val_f1: 0.4091\n",
            "Epoch 732/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 2.0092 - f1: 0.9653 - val_loss: 2.3235 - val_f1: 0.4130\n",
            "Epoch 733/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 2.0039 - f1: 0.9424 - val_loss: 2.3343 - val_f1: 0.4130\n",
            "Epoch 734/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 1.9993 - f1: 0.9311 - val_loss: 2.3194 - val_f1: 0.4130\n",
            "Epoch 735/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 1.9921 - f1: 0.9653 - val_loss: 2.2790 - val_f1: 0.4091\n",
            "Epoch 736/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 1.9895 - f1: 0.9639 - val_loss: 2.2854 - val_f1: 0.4091\n",
            "Epoch 737/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.9833 - f1: 0.9356 - val_loss: 2.2931 - val_f1: 0.4091\n",
            "Epoch 738/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.9783 - f1: 0.9668 - val_loss: 2.2740 - val_f1: 0.4091\n",
            "Epoch 739/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 1.9752 - f1: 0.9415 - val_loss: 2.2858 - val_f1: 0.4091\n",
            "Epoch 740/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.9742 - f1: 0.9625 - val_loss: 2.2563 - val_f1: 0.4091\n",
            "Epoch 741/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 1.9683 - f1: 0.9444 - val_loss: 2.2950 - val_f1: 0.4130\n",
            "Epoch 742/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 1.9599 - f1: 0.9674 - val_loss: 2.3127 - val_f1: 0.4130\n",
            "Epoch 743/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 1.9555 - f1: 0.9644 - val_loss: 2.3262 - val_f1: 0.4130\n",
            "Epoch 744/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.9536 - f1: 0.9435 - val_loss: 2.3271 - val_f1: 0.4130\n",
            "Epoch 745/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 1.9522 - f1: 0.9487 - val_loss: 2.4239 - val_f1: 0.4130\n",
            "Epoch 746/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 1.9469 - f1: 0.9194 - val_loss: 2.4663 - val_f1: 0.4130\n",
            "Epoch 747/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 1.9457 - f1: 0.9424 - val_loss: 2.3994 - val_f1: 0.4130\n",
            "Epoch 748/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.9317 - f1: 0.9260 - val_loss: 2.3587 - val_f1: 0.4130\n",
            "Epoch 749/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 1.9347 - f1: 0.9493 - val_loss: 2.2535 - val_f1: 0.4091\n",
            "Epoch 750/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.9284 - f1: 0.9639 - val_loss: 2.2313 - val_f1: 0.4091\n",
            "Epoch 751/1000\n",
            "108/108 [==============================] - 0s 50us/step - loss: 1.9289 - f1: 0.9537 - val_loss: 2.2393 - val_f1: 0.4091\n",
            "Epoch 752/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 1.9279 - f1: 0.9368 - val_loss: 2.2892 - val_f1: 0.4130\n",
            "Epoch 753/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 1.9144 - f1: 0.9630 - val_loss: 2.2923 - val_f1: 0.4130\n",
            "Epoch 754/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 1.9121 - f1: 0.9648 - val_loss: 2.3324 - val_f1: 0.4130\n",
            "Epoch 755/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 1.9025 - f1: 0.9549 - val_loss: 2.3571 - val_f1: 0.4130\n",
            "Epoch 756/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 1.8999 - f1: 0.9524 - val_loss: 2.3773 - val_f1: 0.4130\n",
            "Epoch 757/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 1.8982 - f1: 0.9424 - val_loss: 2.4658 - val_f1: 0.3958\n",
            "Epoch 758/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 1.8978 - f1: 0.9055 - val_loss: 2.4634 - val_f1: 0.3958\n",
            "Epoch 759/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.8889 - f1: 0.9016 - val_loss: 2.3802 - val_f1: 0.4130\n",
            "Epoch 760/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.8784 - f1: 0.9331 - val_loss: 2.2392 - val_f1: 0.4130\n",
            "Epoch 761/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 1.8938 - f1: 0.9622 - val_loss: 2.1687 - val_f1: 0.4286\n",
            "Epoch 762/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 1.9108 - f1: 0.9721 - val_loss: 2.1686 - val_f1: 0.4286\n",
            "Epoch 763/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.8949 - f1: 0.9648 - val_loss: 2.2532 - val_f1: 0.4130\n",
            "Epoch 764/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 1.8877 - f1: 0.9438 - val_loss: 2.3763 - val_f1: 0.4130\n",
            "Epoch 765/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 1.8727 - f1: 0.9528 - val_loss: 2.3955 - val_f1: 0.3958\n",
            "Epoch 766/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.8702 - f1: 0.9416 - val_loss: 2.4370 - val_f1: 0.3958\n",
            "Epoch 767/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 1.8702 - f1: 0.9137 - val_loss: 2.4288 - val_f1: 0.3958\n",
            "Epoch 768/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 1.8681 - f1: 0.9441 - val_loss: 2.3831 - val_f1: 0.3958\n",
            "Epoch 769/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 1.8624 - f1: 0.9421 - val_loss: 2.3710 - val_f1: 0.4130\n",
            "Epoch 770/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 1.8605 - f1: 0.9253 - val_loss: 2.3460 - val_f1: 0.4130\n",
            "Epoch 771/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 1.8568 - f1: 0.9545 - val_loss: 2.3567 - val_f1: 0.4130\n",
            "Epoch 772/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.8541 - f1: 0.9399 - val_loss: 2.3813 - val_f1: 0.3958\n",
            "Epoch 773/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.8529 - f1: 0.9144 - val_loss: 2.3776 - val_f1: 0.3958\n",
            "Epoch 774/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.8500 - f1: 0.9396 - val_loss: 2.3607 - val_f1: 0.3958\n",
            "Epoch 775/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 1.8473 - f1: 0.9131 - val_loss: 2.3434 - val_f1: 0.4130\n",
            "Epoch 776/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 1.8460 - f1: 0.9522 - val_loss: 2.3173 - val_f1: 0.4130\n",
            "Epoch 777/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 1.8448 - f1: 0.9511 - val_loss: 2.3270 - val_f1: 0.4130\n",
            "Epoch 778/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 1.8443 - f1: 0.9542 - val_loss: 2.3465 - val_f1: 0.4130\n",
            "Epoch 779/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 1.8400 - f1: 0.9266 - val_loss: 2.3498 - val_f1: 0.3958\n",
            "Epoch 780/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 1.8383 - f1: 0.9340 - val_loss: 2.3098 - val_f1: 0.4130\n",
            "Epoch 781/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 1.8371 - f1: 0.9509 - val_loss: 2.2723 - val_f1: 0.4130\n",
            "Epoch 782/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 1.8391 - f1: 0.9611 - val_loss: 2.2713 - val_f1: 0.4130\n",
            "Epoch 783/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 1.8350 - f1: 0.9563 - val_loss: 2.3081 - val_f1: 0.4130\n",
            "Epoch 784/1000\n",
            "108/108 [==============================] - 0s 50us/step - loss: 1.8325 - f1: 0.9347 - val_loss: 2.3841 - val_f1: 0.3958\n",
            "Epoch 785/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 1.8283 - f1: 0.9385 - val_loss: 2.3879 - val_f1: 0.3958\n",
            "Epoch 786/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 1.8299 - f1: 0.9165 - val_loss: 2.3875 - val_f1: 0.3958\n",
            "Epoch 787/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 1.8225 - f1: 0.9395 - val_loss: 2.3278 - val_f1: 0.3958\n",
            "Epoch 788/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 1.8216 - f1: 0.9540 - val_loss: 2.2901 - val_f1: 0.4130\n",
            "Epoch 789/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 1.8205 - f1: 0.9554 - val_loss: 2.2820 - val_f1: 0.4130\n",
            "Epoch 790/1000\n",
            "108/108 [==============================] - 0s 60us/step - loss: 1.8197 - f1: 0.9466 - val_loss: 2.2903 - val_f1: 0.4130\n",
            "Epoch 791/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 1.8160 - f1: 0.9283 - val_loss: 2.3159 - val_f1: 0.3958\n",
            "Epoch 792/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.8199 - f1: 0.9236 - val_loss: 2.3543 - val_f1: 0.3958\n",
            "Epoch 793/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 1.8105 - f1: 0.9239 - val_loss: 2.3011 - val_f1: 0.4130\n",
            "Epoch 794/1000\n",
            "108/108 [==============================] - 0s 49us/step - loss: 1.8147 - f1: 0.9509 - val_loss: 2.2491 - val_f1: 0.4130\n",
            "Epoch 795/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 1.8073 - f1: 0.9535 - val_loss: 2.2903 - val_f1: 0.4130\n",
            "Epoch 796/1000\n",
            "108/108 [==============================] - 0s 58us/step - loss: 1.8100 - f1: 0.8830 - val_loss: 2.3439 - val_f1: 0.3958\n",
            "Epoch 797/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 1.7997 - f1: 0.9202 - val_loss: 2.2861 - val_f1: 0.3958\n",
            "Epoch 798/1000\n",
            "108/108 [==============================] - 0s 49us/step - loss: 1.8017 - f1: 0.9549 - val_loss: 2.2538 - val_f1: 0.4130\n",
            "Epoch 799/1000\n",
            "108/108 [==============================] - 0s 50us/step - loss: 1.7992 - f1: 0.9363 - val_loss: 2.2620 - val_f1: 0.4130\n",
            "Epoch 800/1000\n",
            "108/108 [==============================] - 0s 50us/step - loss: 1.7969 - f1: 0.9486 - val_loss: 2.2641 - val_f1: 0.4130\n",
            "Epoch 801/1000\n",
            "108/108 [==============================] - 0s 48us/step - loss: 1.7918 - f1: 0.9534 - val_loss: 2.3386 - val_f1: 0.3958\n",
            "Epoch 802/1000\n",
            "108/108 [==============================] - 0s 50us/step - loss: 1.7944 - f1: 0.9080 - val_loss: 2.3709 - val_f1: 0.3958\n",
            "Epoch 803/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 1.7868 - f1: 0.9269 - val_loss: 2.3101 - val_f1: 0.3958\n",
            "Epoch 804/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.7841 - f1: 0.9324 - val_loss: 2.2352 - val_f1: 0.4130\n",
            "Epoch 805/1000\n",
            "108/108 [==============================] - 0s 50us/step - loss: 1.7885 - f1: 0.9611 - val_loss: 2.2047 - val_f1: 0.4130\n",
            "Epoch 806/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 1.7877 - f1: 0.9583 - val_loss: 2.2441 - val_f1: 0.4130\n",
            "Epoch 807/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 1.7836 - f1: 0.9505 - val_loss: 2.3666 - val_f1: 0.3800\n",
            "Epoch 808/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 1.7884 - f1: 0.8857 - val_loss: 2.4339 - val_f1: 0.3800\n",
            "Epoch 809/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 1.7875 - f1: 0.8510 - val_loss: 2.3716 - val_f1: 0.3800\n",
            "Epoch 810/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 1.7737 - f1: 0.9120 - val_loss: 2.2563 - val_f1: 0.3958\n",
            "Epoch 811/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 1.7885 - f1: 0.9237 - val_loss: 2.1726 - val_f1: 0.4130\n",
            "Epoch 812/1000\n",
            "108/108 [==============================] - 0s 50us/step - loss: 1.7782 - f1: 0.9688 - val_loss: 2.2532 - val_f1: 0.3958\n",
            "Epoch 813/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.7769 - f1: 0.9366 - val_loss: 2.3408 - val_f1: 0.3958\n",
            "Epoch 814/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.7699 - f1: 0.9180 - val_loss: 2.3538 - val_f1: 0.3800\n",
            "Epoch 815/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 1.7732 - f1: 0.9159 - val_loss: 2.3135 - val_f1: 0.3958\n",
            "Epoch 816/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.7680 - f1: 0.9445 - val_loss: 2.2719 - val_f1: 0.3958\n",
            "Epoch 817/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 1.7640 - f1: 0.9410 - val_loss: 2.2969 - val_f1: 0.3958\n",
            "Epoch 818/1000\n",
            "108/108 [==============================] - 0s 49us/step - loss: 1.7645 - f1: 0.9166 - val_loss: 2.3805 - val_f1: 0.3800\n",
            "Epoch 819/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 1.7668 - f1: 0.9079 - val_loss: 2.3382 - val_f1: 0.3800\n",
            "Epoch 820/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 1.7604 - f1: 0.9234 - val_loss: 2.2651 - val_f1: 0.3958\n",
            "Epoch 821/1000\n",
            "108/108 [==============================] - 0s 50us/step - loss: 1.7576 - f1: 0.9205 - val_loss: 2.2045 - val_f1: 0.4130\n",
            "Epoch 822/1000\n",
            "108/108 [==============================] - 0s 48us/step - loss: 1.7583 - f1: 0.9468 - val_loss: 2.1940 - val_f1: 0.4130\n",
            "Epoch 823/1000\n",
            "108/108 [==============================] - 0s 48us/step - loss: 1.7590 - f1: 0.9530 - val_loss: 2.2548 - val_f1: 0.3958\n",
            "Epoch 824/1000\n",
            "108/108 [==============================] - 0s 48us/step - loss: 1.7538 - f1: 0.9103 - val_loss: 2.2693 - val_f1: 0.3958\n",
            "Epoch 825/1000\n",
            "108/108 [==============================] - 0s 47us/step - loss: 1.7511 - f1: 0.9224 - val_loss: 2.2477 - val_f1: 0.3958\n",
            "Epoch 826/1000\n",
            "108/108 [==============================] - 0s 48us/step - loss: 1.7487 - f1: 0.9073 - val_loss: 2.2467 - val_f1: 0.3958\n",
            "Epoch 827/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 1.7458 - f1: 0.9405 - val_loss: 2.2626 - val_f1: 0.3958\n",
            "Epoch 828/1000\n",
            "108/108 [==============================] - 0s 49us/step - loss: 1.7463 - f1: 0.9248 - val_loss: 2.2813 - val_f1: 0.3958\n",
            "Epoch 829/1000\n",
            "108/108 [==============================] - 0s 49us/step - loss: 1.7441 - f1: 0.9209 - val_loss: 2.2368 - val_f1: 0.3958\n",
            "Epoch 830/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 1.7403 - f1: 0.9281 - val_loss: 2.1755 - val_f1: 0.4130\n",
            "Epoch 831/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 1.7419 - f1: 0.9520 - val_loss: 2.1746 - val_f1: 0.4130\n",
            "Epoch 832/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.7367 - f1: 0.9516 - val_loss: 2.2142 - val_f1: 0.3958\n",
            "Epoch 833/1000\n",
            "108/108 [==============================] - 0s 48us/step - loss: 1.7353 - f1: 0.9193 - val_loss: 2.2801 - val_f1: 0.3958\n",
            "Epoch 834/1000\n",
            "108/108 [==============================] - 0s 49us/step - loss: 1.7350 - f1: 0.9181 - val_loss: 2.2924 - val_f1: 0.3800\n",
            "Epoch 835/1000\n",
            "108/108 [==============================] - 0s 48us/step - loss: 1.7313 - f1: 0.9295 - val_loss: 2.2380 - val_f1: 0.3958\n",
            "Epoch 836/1000\n",
            "108/108 [==============================] - 0s 48us/step - loss: 1.7263 - f1: 0.9207 - val_loss: 2.1903 - val_f1: 0.3958\n",
            "Epoch 837/1000\n",
            "108/108 [==============================] - 0s 49us/step - loss: 1.7353 - f1: 0.9525 - val_loss: 2.1254 - val_f1: 0.4130\n",
            "Epoch 838/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.7273 - f1: 0.9399 - val_loss: 2.1745 - val_f1: 0.3958\n",
            "Epoch 839/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 1.7258 - f1: 0.8952 - val_loss: 2.2530 - val_f1: 0.3958\n",
            "Epoch 840/1000\n",
            "108/108 [==============================] - 0s 50us/step - loss: 1.7208 - f1: 0.9425 - val_loss: 2.2298 - val_f1: 0.3958\n",
            "Epoch 841/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 1.7234 - f1: 0.9250 - val_loss: 2.2374 - val_f1: 0.3958\n",
            "Epoch 842/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 1.7186 - f1: 0.9156 - val_loss: 2.1712 - val_f1: 0.3958\n",
            "Epoch 843/1000\n",
            "108/108 [==============================] - 0s 50us/step - loss: 1.7158 - f1: 0.9506 - val_loss: 2.1546 - val_f1: 0.4130\n",
            "Epoch 844/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 1.7181 - f1: 0.9527 - val_loss: 2.2037 - val_f1: 0.3958\n",
            "Epoch 845/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 1.7126 - f1: 0.9167 - val_loss: 2.2122 - val_f1: 0.3958\n",
            "Epoch 846/1000\n",
            "108/108 [==============================] - 0s 48us/step - loss: 1.7081 - f1: 0.8932 - val_loss: 2.1742 - val_f1: 0.3958\n",
            "Epoch 847/1000\n",
            "108/108 [==============================] - 0s 49us/step - loss: 1.7070 - f1: 0.9403 - val_loss: 2.1001 - val_f1: 0.4130\n",
            "Epoch 848/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 1.7131 - f1: 0.9659 - val_loss: 2.0881 - val_f1: 0.4130\n",
            "Epoch 849/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 1.7096 - f1: 0.9528 - val_loss: 2.1566 - val_f1: 0.3958\n",
            "Epoch 850/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 1.7047 - f1: 0.9438 - val_loss: 2.2526 - val_f1: 0.3800\n",
            "Epoch 851/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 1.7035 - f1: 0.8597 - val_loss: 2.3115 - val_f1: 0.3800\n",
            "Epoch 852/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 1.7086 - f1: 0.8740 - val_loss: 2.2841 - val_f1: 0.3800\n",
            "Epoch 853/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 1.7005 - f1: 0.8951 - val_loss: 2.2728 - val_f1: 0.3800\n",
            "Epoch 854/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 1.6975 - f1: 0.8714 - val_loss: 2.2331 - val_f1: 0.3800\n",
            "Epoch 855/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 1.6936 - f1: 0.9417 - val_loss: 2.1935 - val_f1: 0.3958\n",
            "Epoch 856/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 1.6912 - f1: 0.9375 - val_loss: 2.1946 - val_f1: 0.3958\n",
            "Epoch 857/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 1.6867 - f1: 0.9191 - val_loss: 2.2412 - val_f1: 0.3800\n",
            "Epoch 858/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 1.6881 - f1: 0.8981 - val_loss: 2.2574 - val_f1: 0.3800\n",
            "Epoch 859/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 1.6863 - f1: 0.8819 - val_loss: 2.2403 - val_f1: 0.3800\n",
            "Epoch 860/1000\n",
            "108/108 [==============================] - 0s 58us/step - loss: 1.6834 - f1: 0.8720 - val_loss: 2.2040 - val_f1: 0.3958\n",
            "Epoch 861/1000\n",
            "108/108 [==============================] - 0s 61us/step - loss: 1.6764 - f1: 0.9251 - val_loss: 2.1228 - val_f1: 0.3958\n",
            "Epoch 862/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 1.6803 - f1: 0.9337 - val_loss: 2.1016 - val_f1: 0.3958\n",
            "Epoch 863/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 1.6754 - f1: 0.9499 - val_loss: 2.1175 - val_f1: 0.3958\n",
            "Epoch 864/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.6732 - f1: 0.9476 - val_loss: 2.1584 - val_f1: 0.3958\n",
            "Epoch 865/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 1.6716 - f1: 0.9223 - val_loss: 2.1970 - val_f1: 0.3958\n",
            "Epoch 866/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 1.6710 - f1: 0.8789 - val_loss: 2.1769 - val_f1: 0.3958\n",
            "Epoch 867/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 1.6635 - f1: 0.9121 - val_loss: 2.1041 - val_f1: 0.3958\n",
            "Epoch 868/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 1.6661 - f1: 0.9637 - val_loss: 2.0068 - val_f1: 0.4130\n",
            "Epoch 869/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.6844 - f1: 0.9362 - val_loss: 1.9879 - val_f1: 0.4130\n",
            "Epoch 870/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 1.6739 - f1: 0.9630 - val_loss: 2.0476 - val_f1: 0.4130\n",
            "Epoch 871/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.6638 - f1: 0.9347 - val_loss: 2.1768 - val_f1: 0.3958\n",
            "Epoch 872/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.6590 - f1: 0.9223 - val_loss: 2.2077 - val_f1: 0.3800\n",
            "Epoch 873/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.6618 - f1: 0.8880 - val_loss: 2.2577 - val_f1: 0.3800\n",
            "Epoch 874/1000\n",
            "108/108 [==============================] - 0s 50us/step - loss: 1.6633 - f1: 0.8975 - val_loss: 2.1973 - val_f1: 0.3800\n",
            "Epoch 875/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.6557 - f1: 0.9227 - val_loss: 2.1241 - val_f1: 0.3958\n",
            "Epoch 876/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 1.6520 - f1: 0.9067 - val_loss: 2.0984 - val_f1: 0.3958\n",
            "Epoch 877/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 1.6525 - f1: 0.9518 - val_loss: 2.0393 - val_f1: 0.4130\n",
            "Epoch 878/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.6516 - f1: 0.9631 - val_loss: 2.0422 - val_f1: 0.4130\n",
            "Epoch 879/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.6491 - f1: 0.9534 - val_loss: 2.0806 - val_f1: 0.3958\n",
            "Epoch 880/1000\n",
            "108/108 [==============================] - 0s 50us/step - loss: 1.6417 - f1: 0.9505 - val_loss: 2.1344 - val_f1: 0.3958\n",
            "Epoch 881/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.6428 - f1: 0.9432 - val_loss: 2.1645 - val_f1: 0.3800\n",
            "Epoch 882/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 1.6540 - f1: 0.8825 - val_loss: 2.2364 - val_f1: 0.3800\n",
            "Epoch 883/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.6462 - f1: 0.8744 - val_loss: 2.1586 - val_f1: 0.3800\n",
            "Epoch 884/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.6340 - f1: 0.9197 - val_loss: 2.0634 - val_f1: 0.3958\n",
            "Epoch 885/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 1.6359 - f1: 0.9520 - val_loss: 2.0203 - val_f1: 0.4130\n",
            "Epoch 886/1000\n",
            "108/108 [==============================] - 0s 50us/step - loss: 1.6351 - f1: 0.9390 - val_loss: 2.0212 - val_f1: 0.4130\n",
            "Epoch 887/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 1.6327 - f1: 0.9545 - val_loss: 2.0360 - val_f1: 0.3958\n",
            "Epoch 888/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.6288 - f1: 0.9499 - val_loss: 2.0740 - val_f1: 0.3958\n",
            "Epoch 889/1000\n",
            "108/108 [==============================] - 0s 49us/step - loss: 1.6310 - f1: 0.8823 - val_loss: 2.1082 - val_f1: 0.3958\n",
            "Epoch 890/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 1.6220 - f1: 0.9176 - val_loss: 2.1049 - val_f1: 0.3958\n",
            "Epoch 891/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 1.6240 - f1: 0.9388 - val_loss: 2.0775 - val_f1: 0.3958\n",
            "Epoch 892/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 1.6186 - f1: 0.9211 - val_loss: 2.1119 - val_f1: 0.3958\n",
            "Epoch 893/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 1.6234 - f1: 0.8851 - val_loss: 2.1112 - val_f1: 0.3958\n",
            "Epoch 894/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 1.6232 - f1: 0.9505 - val_loss: 2.0007 - val_f1: 0.4130\n",
            "Epoch 895/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 1.6173 - f1: 0.9373 - val_loss: 1.9956 - val_f1: 0.4130\n",
            "Epoch 896/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 1.6146 - f1: 0.9481 - val_loss: 2.0060 - val_f1: 0.3958\n",
            "Epoch 897/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 1.6133 - f1: 0.9523 - val_loss: 2.0771 - val_f1: 0.3958\n",
            "Epoch 898/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 1.6089 - f1: 0.8883 - val_loss: 2.1393 - val_f1: 0.3800\n",
            "Epoch 899/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 1.6195 - f1: 0.8569 - val_loss: 2.1657 - val_f1: 0.3800\n",
            "Epoch 900/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 1.6042 - f1: 0.8858 - val_loss: 2.0586 - val_f1: 0.3958\n",
            "Epoch 901/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 1.5970 - f1: 0.9500 - val_loss: 1.9828 - val_f1: 0.4130\n",
            "Epoch 902/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 1.6162 - f1: 0.9590 - val_loss: 1.9110 - val_f1: 0.4130\n",
            "Epoch 903/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 1.6098 - f1: 0.9588 - val_loss: 1.9524 - val_f1: 0.4130\n",
            "Epoch 904/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.5957 - f1: 0.9643 - val_loss: 2.0284 - val_f1: 0.3958\n",
            "Epoch 905/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 1.5901 - f1: 0.9261 - val_loss: 2.1182 - val_f1: 0.3800\n",
            "Epoch 906/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 1.5979 - f1: 0.8729 - val_loss: 2.1287 - val_f1: 0.3800\n",
            "Epoch 907/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 1.5936 - f1: 0.8981 - val_loss: 2.0617 - val_f1: 0.3958\n",
            "Epoch 908/1000\n",
            "108/108 [==============================] - 0s 50us/step - loss: 1.5871 - f1: 0.8870 - val_loss: 2.0535 - val_f1: 0.3958\n",
            "Epoch 909/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.5863 - f1: 0.9112 - val_loss: 2.0244 - val_f1: 0.3958\n",
            "Epoch 910/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 1.5884 - f1: 0.9043 - val_loss: 2.0605 - val_f1: 0.3958\n",
            "Epoch 911/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 1.5903 - f1: 0.8673 - val_loss: 2.0703 - val_f1: 0.3800\n",
            "Epoch 912/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 1.5758 - f1: 0.9158 - val_loss: 1.9692 - val_f1: 0.3958\n",
            "Epoch 913/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.5786 - f1: 0.9586 - val_loss: 1.9266 - val_f1: 0.4130\n",
            "Epoch 914/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.5874 - f1: 0.9631 - val_loss: 1.9211 - val_f1: 0.4130\n",
            "Epoch 915/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 1.5765 - f1: 0.9479 - val_loss: 1.9857 - val_f1: 0.3958\n",
            "Epoch 916/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 1.5825 - f1: 0.9130 - val_loss: 2.0559 - val_f1: 0.3800\n",
            "Epoch 917/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.5720 - f1: 0.8596 - val_loss: 1.9807 - val_f1: 0.3958\n",
            "Epoch 918/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 1.5690 - f1: 0.9484 - val_loss: 1.9310 - val_f1: 0.4130\n",
            "Epoch 919/1000\n",
            "108/108 [==============================] - 0s 50us/step - loss: 1.5697 - f1: 0.9329 - val_loss: 1.9247 - val_f1: 0.4130\n",
            "Epoch 920/1000\n",
            "108/108 [==============================] - 0s 49us/step - loss: 1.5699 - f1: 0.9153 - val_loss: 1.9491 - val_f1: 0.3958\n",
            "Epoch 921/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.5632 - f1: 0.9141 - val_loss: 1.9705 - val_f1: 0.3958\n",
            "Epoch 922/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.5602 - f1: 0.9228 - val_loss: 2.0023 - val_f1: 0.3958\n",
            "Epoch 923/1000\n",
            "108/108 [==============================] - 0s 50us/step - loss: 1.5603 - f1: 0.9123 - val_loss: 2.0844 - val_f1: 0.3800\n",
            "Epoch 924/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.5670 - f1: 0.8977 - val_loss: 2.0874 - val_f1: 0.3800\n",
            "Epoch 925/1000\n",
            "108/108 [==============================] - 0s 50us/step - loss: 1.5614 - f1: 0.8664 - val_loss: 2.0459 - val_f1: 0.3800\n",
            "Epoch 926/1000\n",
            "108/108 [==============================] - 0s 49us/step - loss: 1.5600 - f1: 0.9295 - val_loss: 1.8882 - val_f1: 0.4130\n",
            "Epoch 927/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 1.5568 - f1: 0.9464 - val_loss: 1.8349 - val_f1: 0.4091\n",
            "Epoch 928/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.5681 - f1: 0.9610 - val_loss: 1.8172 - val_f1: 0.4091\n",
            "Epoch 929/1000\n",
            "108/108 [==============================] - 0s 50us/step - loss: 1.5602 - f1: 0.9323 - val_loss: 1.8979 - val_f1: 0.4130\n",
            "Epoch 930/1000\n",
            "108/108 [==============================] - 0s 50us/step - loss: 1.5514 - f1: 0.9514 - val_loss: 2.0794 - val_f1: 0.3800\n",
            "Epoch 931/1000\n",
            "108/108 [==============================] - 0s 49us/step - loss: 1.5533 - f1: 0.8768 - val_loss: 2.1411 - val_f1: 0.3800\n",
            "Epoch 932/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 1.5604 - f1: 0.8624 - val_loss: 2.0744 - val_f1: 0.3800\n",
            "Epoch 933/1000\n",
            "108/108 [==============================] - 0s 49us/step - loss: 1.5420 - f1: 0.8778 - val_loss: 1.9601 - val_f1: 0.3958\n",
            "Epoch 934/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 1.5409 - f1: 0.9036 - val_loss: 1.8830 - val_f1: 0.4130\n",
            "Epoch 935/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 1.5401 - f1: 0.9512 - val_loss: 1.8863 - val_f1: 0.4130\n",
            "Epoch 936/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 1.5377 - f1: 0.9299 - val_loss: 1.9114 - val_f1: 0.3958\n",
            "Epoch 937/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 1.5386 - f1: 0.9302 - val_loss: 1.9762 - val_f1: 0.3958\n",
            "Epoch 938/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 1.5439 - f1: 0.8738 - val_loss: 2.0291 - val_f1: 0.3800\n",
            "Epoch 939/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.5297 - f1: 0.8791 - val_loss: 1.9841 - val_f1: 0.3958\n",
            "Epoch 940/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.5249 - f1: 0.9090 - val_loss: 1.9479 - val_f1: 0.3958\n",
            "Epoch 941/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.5235 - f1: 0.9252 - val_loss: 1.9430 - val_f1: 0.3958\n",
            "Epoch 942/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 1.5212 - f1: 0.8807 - val_loss: 1.9483 - val_f1: 0.3958\n",
            "Epoch 943/1000\n",
            "108/108 [==============================] - 0s 49us/step - loss: 1.5257 - f1: 0.9490 - val_loss: 1.9208 - val_f1: 0.3958\n",
            "Epoch 944/1000\n",
            "108/108 [==============================] - 0s 49us/step - loss: 1.5114 - f1: 0.9349 - val_loss: 1.9994 - val_f1: 0.3800\n",
            "Epoch 945/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.5196 - f1: 0.8788 - val_loss: 2.0442 - val_f1: 0.3800\n",
            "Epoch 946/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 1.5172 - f1: 0.8830 - val_loss: 1.9636 - val_f1: 0.3958\n",
            "Epoch 947/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 1.5072 - f1: 0.8539 - val_loss: 1.8935 - val_f1: 0.3958\n",
            "Epoch 948/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 1.5126 - f1: 0.9511 - val_loss: 1.8391 - val_f1: 0.4130\n",
            "Epoch 949/1000\n",
            "108/108 [==============================] - 0s 49us/step - loss: 1.5101 - f1: 0.9518 - val_loss: 1.8784 - val_f1: 0.3958\n",
            "Epoch 950/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 1.5003 - f1: 0.9294 - val_loss: 1.9696 - val_f1: 0.3800\n",
            "Epoch 951/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 1.5060 - f1: 0.8467 - val_loss: 2.0348 - val_f1: 0.3800\n",
            "Epoch 952/1000\n",
            "108/108 [==============================] - 0s 50us/step - loss: 1.5076 - f1: 0.8875 - val_loss: 1.9664 - val_f1: 0.3800\n",
            "Epoch 953/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 1.5001 - f1: 0.8987 - val_loss: 1.8921 - val_f1: 0.3958\n",
            "Epoch 954/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 1.4969 - f1: 0.9426 - val_loss: 1.8745 - val_f1: 0.3958\n",
            "Epoch 955/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.4943 - f1: 0.9529 - val_loss: 1.8751 - val_f1: 0.3958\n",
            "Epoch 956/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.4923 - f1: 0.9484 - val_loss: 1.8747 - val_f1: 0.3958\n",
            "Epoch 957/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 1.4900 - f1: 0.9048 - val_loss: 1.9305 - val_f1: 0.3958\n",
            "Epoch 958/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 1.4898 - f1: 0.8831 - val_loss: 2.0131 - val_f1: 0.3800\n",
            "Epoch 959/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 1.4952 - f1: 0.8627 - val_loss: 2.0072 - val_f1: 0.3800\n",
            "Epoch 960/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 1.4843 - f1: 0.8478 - val_loss: 1.9092 - val_f1: 0.3958\n",
            "Epoch 961/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 1.4735 - f1: 0.8992 - val_loss: 1.8018 - val_f1: 0.4130\n",
            "Epoch 962/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.5157 - f1: 0.9623 - val_loss: 1.7054 - val_f1: 0.4286\n",
            "Epoch 963/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 1.5098 - f1: 0.9661 - val_loss: 1.7223 - val_f1: 0.4091\n",
            "Epoch 964/1000\n",
            "108/108 [==============================] - 0s 60us/step - loss: 1.4941 - f1: 0.9669 - val_loss: 1.8150 - val_f1: 0.3958\n",
            "Epoch 965/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 1.4820 - f1: 0.9217 - val_loss: 1.9219 - val_f1: 0.3800\n",
            "Epoch 966/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 1.4751 - f1: 0.8485 - val_loss: 1.9250 - val_f1: 0.3800\n",
            "Epoch 967/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.4698 - f1: 0.8617 - val_loss: 1.8487 - val_f1: 0.3958\n",
            "Epoch 968/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 1.4702 - f1: 0.9297 - val_loss: 1.7553 - val_f1: 0.4130\n",
            "Epoch 969/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 1.4833 - f1: 0.9653 - val_loss: 1.7219 - val_f1: 0.4091\n",
            "Epoch 970/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 1.4791 - f1: 0.9431 - val_loss: 1.8252 - val_f1: 0.3958\n",
            "Epoch 971/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 1.4642 - f1: 0.9179 - val_loss: 1.9142 - val_f1: 0.3800\n",
            "Epoch 972/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.4683 - f1: 0.8860 - val_loss: 1.9540 - val_f1: 0.3800\n",
            "Epoch 973/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 1.4667 - f1: 0.8612 - val_loss: 1.9442 - val_f1: 0.3800\n",
            "Epoch 974/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.4646 - f1: 0.8880 - val_loss: 1.8788 - val_f1: 0.3958\n",
            "Epoch 975/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 1.4553 - f1: 0.8667 - val_loss: 1.8551 - val_f1: 0.3958\n",
            "Epoch 976/1000\n",
            "108/108 [==============================] - 0s 50us/step - loss: 1.4554 - f1: 0.9191 - val_loss: 1.8194 - val_f1: 0.3958\n",
            "Epoch 977/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 1.4519 - f1: 0.9518 - val_loss: 1.8300 - val_f1: 0.3958\n",
            "Epoch 978/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 1.4633 - f1: 0.8687 - val_loss: 1.8797 - val_f1: 0.3800\n",
            "Epoch 979/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 1.4566 - f1: 0.9089 - val_loss: 1.8081 - val_f1: 0.3958\n",
            "Epoch 980/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 1.4468 - f1: 0.9072 - val_loss: 1.8311 - val_f1: 0.3958\n",
            "Epoch 981/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 1.4443 - f1: 0.9068 - val_loss: 1.8473 - val_f1: 0.3958\n",
            "Epoch 982/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 1.4436 - f1: 0.8751 - val_loss: 1.8437 - val_f1: 0.3958\n",
            "Epoch 983/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 1.4418 - f1: 0.8905 - val_loss: 1.8186 - val_f1: 0.3958\n",
            "Epoch 984/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 1.4393 - f1: 0.9499 - val_loss: 1.7543 - val_f1: 0.4130\n",
            "Epoch 985/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 1.4414 - f1: 0.9561 - val_loss: 1.7810 - val_f1: 0.3958\n",
            "Epoch 986/1000\n",
            "108/108 [==============================] - 0s 51us/step - loss: 1.4362 - f1: 0.9498 - val_loss: 1.7868 - val_f1: 0.3958\n",
            "Epoch 987/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.4342 - f1: 0.9268 - val_loss: 1.7895 - val_f1: 0.3958\n",
            "Epoch 988/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 1.4329 - f1: 0.9444 - val_loss: 1.7508 - val_f1: 0.3958\n",
            "Epoch 989/1000\n",
            "108/108 [==============================] - 0s 53us/step - loss: 1.4350 - f1: 0.9478 - val_loss: 1.7476 - val_f1: 0.3958\n",
            "Epoch 990/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.4293 - f1: 0.9492 - val_loss: 1.7813 - val_f1: 0.3958\n",
            "Epoch 991/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 1.4287 - f1: 0.9142 - val_loss: 1.8286 - val_f1: 0.3958\n",
            "Epoch 992/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.4260 - f1: 0.9068 - val_loss: 1.8191 - val_f1: 0.3958\n",
            "Epoch 993/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 1.4321 - f1: 0.8955 - val_loss: 1.8478 - val_f1: 0.3800\n",
            "Epoch 994/1000\n",
            "108/108 [==============================] - 0s 56us/step - loss: 1.4249 - f1: 0.8989 - val_loss: 1.8349 - val_f1: 0.3958\n",
            "Epoch 995/1000\n",
            "108/108 [==============================] - 0s 50us/step - loss: 1.4209 - f1: 0.8641 - val_loss: 1.8254 - val_f1: 0.3958\n",
            "Epoch 996/1000\n",
            "108/108 [==============================] - 0s 49us/step - loss: 1.4193 - f1: 0.8994 - val_loss: 1.7889 - val_f1: 0.3958\n",
            "Epoch 997/1000\n",
            "108/108 [==============================] - 0s 52us/step - loss: 1.4137 - f1: 0.9279 - val_loss: 1.7344 - val_f1: 0.3958\n",
            "Epoch 998/1000\n",
            "108/108 [==============================] - 0s 54us/step - loss: 1.4133 - f1: 0.9312 - val_loss: 1.7154 - val_f1: 0.4130\n",
            "Epoch 999/1000\n",
            "108/108 [==============================] - 0s 55us/step - loss: 1.4135 - f1: 0.9500 - val_loss: 1.7173 - val_f1: 0.4130\n",
            "Epoch 1000/1000\n",
            "108/108 [==============================] - 0s 57us/step - loss: 1.4124 - f1: 0.9493 - val_loss: 1.7411 - val_f1: 0.3958\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f08f072ff60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eo7QO2ntN93P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "outputId": "11731ae6-6a53-45ca-b41a-892f32fcbd90"
      },
      "source": [
        "y_pred=model.predict(X_test_trm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-4f1a4c1d01ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_trm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m         \u001b[0;31m# Case 2: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1441\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1442\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             raise ValueError(\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 3 array(s), but instead got the following list of 1 arrays: [array([[-1.09879547e+02,  1.77169800e+02, -1.60525055e+02,\n        -3.45145447e+02],\n       [-1.25683235e+02,  1.98511337e+02, -9.29407806e+01,\n        -2.82471802e+02],\n       [-8.41316376e+01,  4.5..."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yHyCxse5BeL"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGqT50UsOMSq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "06e5b0e5-f370-4523-fad4-6dfebff972a5"
      },
      "source": [
        "loss, accuracy, f1_score, precision, recall = model.evaluate(Xtest, ytest_one_hot, verbose=0)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-fab916cee2f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytest_one_hot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'Xtest' is not defined"
          ]
        }
      ]
    }
  ]
}